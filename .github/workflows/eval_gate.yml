name: Agent Evaluation Gate

# Trigger on PRs that touch agent code
on:
  pull_request:
    paths:
      - 'agents/**'
      - 'agent_factory/orchestrators/**'
      - 'phoenix_integration/**'
      - 'evals/**'
      - 'prompts/**'
      - 'requirements.txt'
  
  # Also allow manual trigger
  workflow_dispatch:
    inputs:
      limit:
        description: 'Number of cases to evaluate (leave empty for all)'
        required: false
        default: ''

jobs:
  eval-gate:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install arize-phoenix[evals] openai psycopg2-binary
      
      - name: Start Phoenix server
        run: |
          # Start Phoenix in background
          phoenix serve --port 6006 &
          
          # Wait for it to be ready
          sleep 5
          
          # Verify it's running
          curl -s http://localhost:6006 > /dev/null || echo "Phoenix may not be running"
      
      - name: Run golden dataset evals
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          # Determine limit from input or use default for PRs
          LIMIT="${{ github.event.inputs.limit }}"
          if [ -z "$LIMIT" ]; then
            LIMIT="50"  # Default to 50 cases for CI speed
          fi

          python phoenix_integration/evals/run_eval.py \
            --dataset datasets/golden_from_neon.jsonl \
            --output evals/eval_results.json \
            --limit $LIMIT
      
      - name: Upload eval results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-results
          path: evals/eval_results.json
      
      - name: Post results to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read eval results
            let results;
            try {
              results = JSON.parse(fs.readFileSync('evals/eval_results.json', 'utf8'));
            } catch (e) {
              console.log('Could not read results file');
              return;
            }
            
            const summary = results.summary || {};
            const byEval = summary.by_eval || {};
            
            // Build markdown table
            let table = '| Eval | Pass Rate | Threshold | Status |\n';
            table += '|------|-----------|-----------|--------|\n';
            
            for (const [name, stats] of Object.entries(byEval)) {
              const status = stats.passed ? '‚úÖ PASS' : '‚ùå FAIL';
              const blocking = stats.blocking ? ' ‚ö†Ô∏è' : '';
              table += `| ${name} | ${(stats.pass_rate * 100).toFixed(1)}% | ${(stats.threshold * 100).toFixed(0)}% | ${status}${blocking} |\n`;
            }
            
            const overallStatus = summary.gate_passed ? '‚úÖ **GATE PASSED**' : '‚ùå **GATE FAILED**';
            
            const body = `## Agent Evaluation Results
            
            ${overallStatus}
            
            ${table}
            
            <details>
            <summary>üìä Full results</summary>
            
            \`\`\`json
            ${JSON.stringify(summary, null, 2)}
            \`\`\`
            
            </details>
            
            ---
            *Run at ${new Date().toISOString()}*
            `;
            
            // Post comment
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
      
      - name: Check gate status
        run: |
          # Parse the results and exit with appropriate code
          python -c "
          import json
          with open('evals/eval_results.json') as f:
              results = json.load(f)
          
          summary = results.get('summary', {})
          gate_passed = summary.get('gate_passed', False)
          
          print('=' * 60)
          print('GATE STATUS')
          print('=' * 60)
          
          if gate_passed:
              print('‚úÖ All blocking evals passed')
              exit(0)
          else:
              print('‚ùå One or more blocking evals failed')
              print()
              for name, stats in summary.get('by_eval', {}).items():
                  if stats.get('blocking') and not stats.get('passed'):
                      print(f'  FAILED: {name}')
                      print(f'    Pass rate: {stats[\"pass_rate\"]*100:.1f}%')
                      print(f'    Threshold: {stats[\"threshold\"]*100:.0f}%')
              exit(1)
          "

  # Optional: Nightly full eval
  nightly-eval:
    if: github.event.schedule
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install arize-phoenix[evals] openai psycopg2-binary
      
      - name: Run full eval (no limit)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          phoenix serve &
          sleep 5
          python phoenix_integration/evals/run_eval.py \
            --dataset datasets/golden_from_neon.jsonl \
            --output evals/nightly_results.json
      
      - name: Upload nightly results
        uses: actions/upload-artifact@v4
        with:
          name: nightly-eval-results
          path: evals/nightly_results.json
          retention-days: 30
