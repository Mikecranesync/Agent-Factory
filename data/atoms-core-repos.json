{
  "metadata": {
    "source": "Knowledge Extraction from CORE Repositories",
    "extraction_date": "2025-12-21",
    "task_id": "task-86.7",
    "version": "2.0",
    "total_atoms": 52,
    "status": "Phase 2.2 Complete - 52 atoms extracted and validated",
    "next_phase": "Generate embeddings and continue Phase 2.3",
    "source_documents": [
      "docs/architecture/AGENT_FACTORY_PATTERNS.md (3,600 words)",
      "docs/architecture/BACKLOG_MCP_PATTERNS.md (3,200 words)",
      "docs/architecture/PAI_CONFIG_PATTERNS.md (3,000 words)",
      "docs/patterns/CROSS_REPO_INTEGRATION.md (2,400 words)",
      "docs/architecture/archon_architecture_analysis.md (4,800 words)",
      "docs/architecture/00_architecture_platform.md (5,200 words)",
      "agent_factory/scaffold/claude_executor.py (620 lines)",
      "agent_factory/scaffold/pr_creator.py (530 lines)",
      "agent_factory/scaffold/worktree_manager.py (450 lines)",
      "agent_factory/scaffold/safety_monitor.py (230 lines)",
      "agent_factory/scaffold/context_assembler.py (280 lines)",
      "agent_factory/scaffold/task_router.py (300 lines)",
      "agent_factory/rivet_pro/confidence_scorer.py (550 lines)",
      "agent_factory/rivet_pro/vps_kb_client.py (460 lines)",
      "agent_factory/llm/cache.py (180 lines)",
      "agent_factory/llm/streaming.py (160 lines)",
      "docs/patterns/SME_AGENT_PATTERN.md (365 lines)",
      "docs/patterns/CROSS_REPO_INTEGRATION.md (200 lines)"
    ],
    "categories": {
      "agent-factory-patterns": 12,
      "agent-factory-best-practices": 5,
      "backlog-mcp-patterns": 3,
      "pai-config-patterns": 2,
      "cross-repo-patterns": 4,
      "archon-patterns": 5,
      "archon-best-practices": 2,
      "scaffold-patterns": 7,
      "rivet-pro-patterns": 3,
      "llm-patterns": 2,
      "best-practices": 5
    },
    "quality_metrics": {
      "avg_content_length": 920,
      "all_have_code_examples": true,
      "all_have_keywords": true,
      "all_have_prereqs": true,
      "ieee_lom_compliant": true
    }
  },
  "atoms": [
    {
      "atom_id": "pattern:agent-factory-llm-router",
      "type": "pattern",
      "title": "LLM Router Cost Optimization",
      "summary": "Multi-provider LLM routing with capability-based model selection reduces costs 73% ($750/mo → $198/mo) for 50+ autonomous agents",
      "content": "**Problem**: Autonomous agents used expensive models (GPT-4o) for simple tasks like classification, causing unnecessarily high costs.\n\n**Solution**: Route requests to the cheapest capable model based on task complexity using ModelCapability enum (SIMPLE, MODERATE, COMPLEX, CODING, RESEARCH).\n\n**Architecture**: LLMRouter selects models from registry based on capability requirements. Supports 12 models across 4 providers (OpenAI, Anthropic, Google, Ollama) with automatic failover chain and exponential backoff.\n\n**Key Features**:\n- Multi-provider support via LiteLLM\n- Fallback chain (max 3 models)\n- Automatic cost tracking\n- LRU cache with TTL (30-40% hit rate)\n- Streaming support\n\n**Impact**: 73% cost reduction in production with 50+ agents, from $750/month to $198/month.\n\n**Files**: agent_factory/llm/router.py (493 lines), agent_factory/llm/config.py (347 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "llm",
        "routing",
        "cost-optimization",
        "failover",
        "multi-provider"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:llm-api-basics",
        "concept:capability-based-routing"
      ],
      "code_example": "```python\nfrom agent_factory.llm.router import create_router\nfrom agent_factory.llm.types import ModelCapability\n\n# Create router with caching and fallback\nrouter = create_router(\n    max_retries=3,\n    enable_fallback=True,\n    enable_cache=True\n)\n\n# Route by capability (automatic model selection)\nresponse = router.route_by_capability(\n    messages=[{\"role\": \"user\", \"content\": \"Classify this email\"}],\n    capability=ModelCapability.SIMPLE  # Uses gpt-3.5-turbo\n)\n\nprint(f\"Model: {response.model}\")  # gpt-3.5-turbo\nprint(f\"Cost: ${response.usage.total_cost_usd:.6f}\")\n```"
    },
    {
      "atom_id": "pattern:agent-factory-database-failover",
      "type": "pattern",
      "title": "Multi-Provider Database Failover",
      "summary": "PostgreSQL failover across 3 providers (Supabase, Railway, Neon) achieves 99.9% uptime vs 99.5% single-provider",
      "content": "**Problem**: Single PostgreSQL provider (Supabase) had outages, breaking agent memory and knowledge base access.\n\n**Solution**: Support 3 PostgreSQL providers with automatic failover, health checks, and connection pooling.\n\n**Architecture**: DatabaseManager maintains connection pools for all providers, performs health checks (60s cache), and automatically fails over when primary is unavailable.\n\n**Providers**:\n- Supabase (Free tier, pgvector, PostgREST API)\n- Railway ($5/month, 1GB, better uptime)\n- Neon (Serverless, auto-scale)\n\n**Failover Strategy**: Try primary → health check fails → try next in sequence. Environment variable DATABASE_FAILOVER_ORDER defines sequence.\n\n**Performance**: Single provider 99.5% uptime (4 hours downtime/month), multi-provider 99.9% uptime (40 minutes downtime/month). Failover adds +200ms latency (acceptable for async agents).\n\n**Files**: agent_factory/core/database_manager.py (452 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "database",
        "failover",
        "postgresql",
        "high-availability",
        "multi-provider"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:database-connection-pooling",
        "concept:health-checks"
      ],
      "code_example": "```python\nfrom agent_factory.core.database_manager import DatabaseManager\n\n# Auto-selects provider from DATABASE_PROVIDER env var\ndb = DatabaseManager()\n\n# Execute query with automatic failover\nresult = db.execute_query(\"SELECT version()\")\nprint(result)  # [('PostgreSQL 16.1 ...',)]\n\n# Check provider health\nhealth = db.health_check_all()\nprint(health)  # {'supabase': True, 'railway': True, 'neon': False}\n\n# Force specific provider\ndb.set_provider('railway')\nrows = db.execute_query(\n    \"SELECT * FROM knowledge_atoms LIMIT 10\",\n    fetch_mode=\"all\"\n)\n```"
    },
    {
      "atom_id": "pattern:agent-factory-settings-service",
      "type": "pattern",
      "title": "Database-Backed Settings Service",
      "summary": "Zero-downtime configuration changes via database-backed settings with environment variable fallback and 5-minute cache TTL",
      "content": "**Problem**: Changing agent behavior required code changes and service restarts, causing downtime and slow iteration.\n\n**Solution**: Store configuration in Supabase table with environment variable fallback. Settings cached in-memory (5-minute TTL) for performance.\n\n**Architecture**: Settings Service queries database → caches result → returns value. Fallback: Database → Environment Variable → Default Value.\n\n**Database Schema**: agent_factory_settings table with columns: category (text), key (text), value (text), description (text), updated_at (timestamptz).\n\n**Type Conversion**: Supports get_bool(), get_int(), get_float() with validation and warnings on parse failures.\n\n**Runtime Changes**: Update Supabase table via UI → call settings.reload() → new values picked up without service restart.\n\n**Pattern Origin**: Adapted from Archon's CredentialService (13.4k⭐ production system).\n\n**Files**: agent_factory/core/settings_service.py (319 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "configuration",
        "settings",
        "runtime-config",
        "database",
        "zero-downtime"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:database-crud",
        "concept:caching"
      ],
      "code_example": "```python\nfrom agent_factory.core.settings_service import settings\n\n# Get string setting (with category)\nmodel = settings.get(\"DEFAULT_MODEL\", category=\"llm\")\n# Returns: \"gpt-4o-mini\" (from DB or env)\n\n# Get typed settings\nbatch_size = settings.get_int(\"BATCH_SIZE\", default=50, category=\"memory\")\nuse_hybrid = settings.get_bool(\"USE_HYBRID_SEARCH\", category=\"memory\")\ntemperature = settings.get_float(\"DEFAULT_TEMPERATURE\", default=0.7, category=\"llm\")\n\n# Set value programmatically (updates database)\nsettings.set(\"DEBUG_MODE\", \"true\", category=\"general\")\n\n# Reload from database (picks up runtime changes)\nsettings.reload()\n```"
    },
    {
      "atom_id": "pattern:agent-factory-sme-template",
      "type": "pattern",
      "title": "SME Agent Template Pattern",
      "summary": "Abstract base class for domain expert agents reduces implementation from 200 lines to 50 lines (75% code reduction)",
      "content": "**Problem**: Each domain expert agent (Motor Control, PLC Programming, Networking) had 200+ lines of duplicate boilerplate.\n\n**Solution**: SMEAgentTemplate abstract base class enforces standard structure. Developers implement 4 methods: analyze_query(), search_kb(), generate_answer(), score_confidence().\n\n**Architecture**: Template method pattern with fixed workflow: Query → Analysis → KB Search → Answer Generation → Confidence Scoring → Escalation Decision.\n\n**Standard Workflow**:\n1. analyze_query() extracts domain, entities, keywords, complexity\n2. search_kb() performs semantic search with reranking\n3. generate_answer() builds context and generates via LLM\n4. score_confidence() combines document relevance, answer length, keyword overlap\n5. Escalate if confidence < threshold\n\n**Benefits**: 75% less code (200→50 lines), consistent error handling, automatic escalation, standardized metadata, rapid development (30 min vs 2 hours).\n\n**Files**: agent_factory/templates/sme_agent_template.py (375 lines), docs/patterns/SME_AGENT_PATTERN.md (365 lines), examples/sme_agent_example.py (261 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "template",
        "sme-agent",
        "domain-expert",
        "abstraction",
        "code-reuse"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:template-method-pattern",
        "concept:rag-search"
      ],
      "code_example": "```python\nfrom agent_factory.templates import SMEAgentTemplate\nfrom agent_factory.templates.sme_agent_template import QueryAnalysis, SMEAnswer\n\nclass MotorControlSME(SMEAgentTemplate):\n    def __init__(self):\n        super().__init__(\n            name=\"Motor Control SME\",\n            domain=\"motor_control\",\n            min_confidence=0.7,\n            max_docs=10\n        )\n\n    def analyze_query(self, query: str) -> QueryAnalysis:\n        # Extract entities, keywords, classify type\n        return QueryAnalysis(...)\n\n    def search_kb(self, analysis: QueryAnalysis) -> List[Dict]:\n        # Semantic search + reranking\n        return reranked_docs\n\n    def generate_answer(self, query: str, docs: List[Dict]) -> str:\n        # Generate answer from docs\n        return answer\n\n    def score_confidence(self, query: str, answer: str, docs: List[Dict]) -> float:\n        # Score 0.0-1.0\n        return confidence\n\n# Usage\nsme = MotorControlSME()\nresult = sme.answer(\"Why is my motor overheating?\")\nprint(f\"Confidence: {result.confidence:.2f}\")\nif result.escalate:\n    print(\"Escalating to human expert\")\n```"
    },
    {
      "atom_id": "pattern:agent-factory-rag-reranking",
      "type": "pattern",
      "title": "RAG Cross-Encoder Reranking",
      "summary": "Cross-encoder reranking improves search relevance from 65% to 85% (+20% absolute improvement) using ms-marco-MiniLM model",
      "content": "**Problem**: Pure keyword/vector search retrieved irrelevant documents, leading to hallucinated answers.\n\n**Solution**: Cross-encoder reranking scores query-document pairs for better semantic relevance after initial retrieval.\n\n**Architecture**: Initial retrieval (pgvector + full-text) → 20 candidate docs → Cross-encoder scores each (query, doc) pair → Rerank by score → Return top-k.\n\n**Why Cross-Encoders Work Better**:\n- Bi-encoder (vector search): Encodes query and documents separately, fast but less accurate\n- Cross-encoder (reranking): Encodes query+document together with attention mechanism, slower but much more accurate\n\n**Model**: cross-encoder/ms-marco-MiniLM-L-6-v2 (90MB, ~50ms for 20 docs on CPU, ~10ms on GPU).\n\n**Performance**: Initial retrieval 65% relevant, after reranking 85% relevant (20% absolute improvement). Total latency: 100ms (search) + 50ms (reranking) = 150ms (acceptable for async agents).\n\n**Impact**: Better answers → higher confidence scores → fewer escalations → less human intervention.\n\n**Files**: agent_factory/rivet_pro/rag/reranker.py (200 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "rag",
        "reranking",
        "cross-encoder",
        "semantic-search",
        "quality"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:vector-search",
        "concept:cross-encoders"
      ],
      "code_example": "```python\nfrom agent_factory.rivet_pro.rag import rerank_search_results, Reranker, RerankConfig\n\n# Simple usage (default config)\nreranked_docs = rerank_search_results(\n    query=\"Motor overheating troubleshooting\",\n    docs=initial_search_results,\n    top_k=8\n)\n\n# Advanced usage (custom config)\nconfig = RerankConfig(\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    top_k=10,\n    batch_size=32,\n    max_length=512\n)\nreranker = Reranker(config)\nreranked = reranker.rerank(query, docs, top_k=8)\n\n# Results\nfor doc in reranked:\n    print(f\"{doc.title}: {doc.similarity:.3f}\")\n# Motor Overheating Causes: 0.892\n# VFD Troubleshooting: 0.784\n# Thermal Protection: 0.721\n```"
    },
    {
      "atom_id": "pattern:agent-factory-observability",
      "type": "pattern",
      "title": "Observability Stack for AI Agents",
      "summary": "Distributed tracing + metrics + structured logging provides production visibility for 50+ autonomous agents",
      "content": "**Problem**: 50+ autonomous agents running 24/7 with no visibility into failures, costs, or performance.\n\n**Solution**: Implement distributed tracing (spans), metrics collection (histograms, counters, gauges), and structured logging.\n\n**Architecture**: Tracer.start_span() creates root span → Nested spans for LLM calls, RAG search, database queries → Each span has attributes (model, tokens, cost, duration) → Export to backend (Jaeger, Grafana).\n\n**Key Features**:\n1. Distributed Tracing: Track requests across multiple agents, visualize dependencies\n2. Metrics Collection: Operation duration histograms, error rate counters, cost gauges\n3. Structured Logging: JSON logs with metadata (query, confidence, sources, latency_ms)\n4. Cost Attribution: Track LLM costs per agent for budgeting\n\n**Usage**: Wrap operations in spans, set attributes for context, export to monitoring backend.\n\n**Files**: agent_factory/observability/tracer.py (300 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "observability",
        "tracing",
        "metrics",
        "logging",
        "monitoring"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:distributed-tracing",
        "concept:structured-logging"
      ],
      "code_example": "```python\nfrom agent_factory.observability.tracer import get_tracer\nimport logging\n\ntracer = get_tracer()\nlogger = logging.getLogger(\"sme.motor_control\")\n\n# Start root span\nwith tracer.start_span(\"agent.answer\") as span:\n    span.set_attribute(\"agent_name\", \"MotorControlSME\")\n    span.set_attribute(\"query\", query)\n\n    # Nested span for LLM call\n    with tracer.start_span(\"llm.complete\", parent=span) as llm_span:\n        llm_span.set_attribute(\"model\", \"gpt-4o-mini\")\n        response = router.complete(messages, config)\n        llm_span.set_attribute(\"tokens\", response.usage.total_tokens)\n        llm_span.set_attribute(\"cost\", response.usage.total_cost_usd)\n\n    span.set_attribute(\"confidence\", confidence)\n    span.set_attribute(\"escalate\", escalate)\n\n# Structured logging\nlogger.info(\"Query processed\", extra={\n    \"query\": query,\n    \"confidence\": 0.85,\n    \"sources\": [\"atom-001\", \"atom-002\"],\n    \"latency_ms\": 1200\n})\n```"
    },
    {
      "atom_id": "pattern:agent-factory-git-worktree",
      "type": "pattern",
      "title": "Git Worktree Multi-Agent Isolation",
      "summary": "Isolated git worktrees prevent file conflicts when multiple agents work on same repository simultaneously",
      "content": "**Problem**: Multiple agents working in same directory caused file conflicts and lost work.\n\n**Solution**: Each agent gets isolated worktree on separate branch. Main repository stays clean while parallel development happens in sibling directories.\n\n**Architecture**:\n- Main: C:\\Agent-Factory\\ (main branch, protected)\n- Worktree 1: C:\\agent-factory-feature-routing\\ (feature-routing branch, Orchestrator Agent)\n- Worktree 2: C:\\agent-factory-feature-cache\\ (feature-cache branch, Cache Agent)\n- Worktree 3: C:\\agent-factory-feature-reranker\\ (feature-reranker branch, RAG Agent)\n\n**Benefits**: No file conflicts (separate directories), parallel development (multiple agents), easy cleanup (remove worktree after PR merged).\n\n**Pre-commit Hook**: Blocks commits to main directory, enforces worktree usage.\n\n**Workflow**: Create worktree → Agent works in isolation → Create PR → Merge → Remove worktree.\n\n**Files**: docs/patterns/GIT_WORKTREE_GUIDE.md",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "git",
        "worktree",
        "multi-agent",
        "concurrency",
        "isolation"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:git-basics",
        "concept:git-branches"
      ],
      "code_example": "```bash\n# Manual worktree creation\ngit worktree add ../agent-factory-feature-routing -b feature-routing\ncd ../agent-factory-feature-routing\n\n# Via CLI tool\nagentcli worktree-create feature-routing\ncd ../agent-factory-feature-routing\n\n# Work in isolation\n# Edit files, commit, push\n\n# Create PR\ngh pr create --title \"Add routing feature\" --body \"...\"\n\n# After merge, remove worktree\ncd ../agent-factory\ngit worktree remove ../agent-factory-feature-routing\n```"
    },
    {
      "atom_id": "pattern:agent-factory-constitutional",
      "type": "pattern",
      "title": "Constitutional Programming Pattern",
      "summary": "Specs drive code generation - blueprint with acceptance criteria ensures implementation matches design exactly",
      "content": "**Problem**: Building complex systems required extensive upfront design, then manual implementation that often deviated from spec.\n\n**Solution**: Specs drive code generation. Blueprint includes acceptance criteria, PRD generates implementation, AI validates against criteria.\n\n**Architecture**: High-level spec (Markdown) → AI expands into detailed PRD → PRD drives implementation → Generate code from spec → Validate against acceptance criteria → Iterate until all criteria pass → Working system matches spec exactly.\n\n**Example**: PHASE1_SPEC.md acceptance criteria (\"orchestrator.route() selects correct agent based on keywords\", \"Fallback to general agent if no match\") → AI generates AgentOrchestrator.route() implementation that matches criteria exactly.\n\n**Benefits**: Perfect spec alignment, no spec drift, testable requirements, AI-generated code matches design.\n\n**Files**: docs/patterns/CONSTITUTIONAL_PROGRAMMING.md",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "constitutional-programming",
        "spec-driven",
        "acceptance-criteria",
        "ai-code-generation"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:acceptance-criteria",
        "concept:prd"
      ],
      "code_example": "```markdown\n# Spec (docs/PHASE1_SPEC.md)\n## Acceptance Criteria\n- [ ] orchestrator.route() selects correct agent based on keywords\n- [ ] Fallback to general agent if no match\n- [ ] Callbacks fire on orchestrator events\n\n# Generated Implementation\nclass AgentOrchestrator:\n    def route(self, query: str) -> str:\n        # Implementation matches acceptance criteria exactly\n        for agent_name, (agent, keywords) in self.agents.items():\n            if any(kw in query.lower() for kw in keywords):\n                return agent.invoke(query)\n\n        # Fallback to general agent (criteria: fallback)\n        return self.general_agent.invoke(query)\n```"
    },
    {
      "atom_id": "best-practice:llm-cost-tracking",
      "type": "best-practice",
      "title": "LLM Cost Tracking Best Practices",
      "summary": "Track LLM costs per request with automatic aggregation, export to dashboards, and set budget alerts",
      "content": "**Practice**: Always track LLM costs at request level and aggregate for monitoring.\n\n**Implementation**:\n1. Use LLMRouter's automatic cost tracking (every response includes usage.total_cost_usd)\n2. Aggregate with get_global_tracker().aggregate_stats()\n3. Export metrics to monitoring dashboard (Grafana)\n4. Set budget alerts (e.g., > $50/day)\n5. Break down costs per agent for attribution\n\n**Why It Matters**: Autonomous agents can rack up costs quickly. Daily monitoring prevents surprise bills and enables cost optimization.\n\n**Real Example**: Agent-Factory tracked $750/month → identified expensive model usage → implemented routing → reduced to $198/month.\n\n**Tools**: agent_factory/llm/tracker.py, Grafana dashboards, budget alerts",
      "atom_type": "best-practice",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "cost-tracking",
        "llm",
        "monitoring",
        "budgets"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:llm-usage-tracking"
      ],
      "code_example": "```python\nfrom agent_factory.llm.router import create_router\nfrom agent_factory.llm.tracker import get_global_tracker\n\n# All requests automatically tracked\nrouter = create_router()\nresponse = router.complete(messages, config)\nprint(f\"This request: ${response.usage.total_cost_usd:.6f}\")\n\n# Aggregate daily\ntracker = get_global_tracker()\nstats = tracker.aggregate_stats()\nprint(f\"Total today: ${stats['total_cost_usd']:.2f}\")\nprint(f\"By model: {stats['costs_by_model']}\")\nprint(f\"By agent: {stats['costs_by_agent']}\")\n\n# Set alerts\nif stats['total_cost_usd'] > 50:\n    send_alert(\"Daily LLM budget exceeded!\")\n```"
    },
    {
      "atom_id": "pattern:backlog-mcp-server",
      "type": "pattern",
      "title": "MCP Server Architecture",
      "summary": "Model Context Protocol enables Claude Code to discover and use task management tools automatically with zero setup",
      "content": "**Problem**: AI agents need structured access to task management, but JSON APIs require manual integration.\n\n**Solution**: Implement MCP (Model Context Protocol) server that Claude Code recognizes automatically via .mcp.json config.\n\n**Architecture**: Claude Code CLI → MCP Client (built-in) → IPC Connection (stdio) → Backlog.md MCP Server (npm global package) → Storage (backlog/tasks/*.md) → TASK.md (auto-generated view).\n\n**Protocol**: 18 MCP tools (task_create, task_edit, task_list, task_view, task_archive, etc.) + 3 resources (workflow guides).\n\n**Integration Benefits**:\n1. Zero Setup: Claude Code discovers MCP servers automatically, no API keys needed\n2. Type Safety: MCP protocol enforces schemas, compile-time errors for invalid calls\n3. Bidirectional Sync: MCP changes → TASK.md updates, human edits .md files → MCP reflects\n\n**Configuration**: .mcp.json in project root with server command, args, env vars.\n\n**Files**: Package published as backlog.md@1.28.0 on npm",
      "atom_type": "pattern",
      "vendor": "backlog-md",
      "equipment_type": "task-management",
      "source_document": "docs/architecture/BACKLOG_MCP_PATTERNS.md",
      "keywords": [
        "mcp",
        "model-context-protocol",
        "claude-code",
        "task-management"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:mcp-protocol",
        "concept:ipc"
      ],
      "code_example": "```json\n// .mcp.json (project-level)\n{\n  \"mcpServers\": {\n    \"backlog\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"backlog.md@1.28.0\"],\n      \"env\": {\n        \"BACKLOG_DIR\": \"./backlog\"\n      }\n    }\n  }\n}\n\n// Usage in Claude Code\nbacklog.task_create({\n  title: \"BUILD: LRU cache with TTL\",\n  description: \"Implement OrderedDict-based cache...\",\n  priority: \"high\",\n  labels: [\"llm\", \"performance\"],\n  acceptanceCriteria: [\n    \"Cache stores responses with SHA256 key\",\n    \"TTL-based expiration (1 hour default)\"\n  ]\n})\n// Returns: {task_id: \"task-56\", status: \"created\"}\n```"
    },
    {
      "atom_id": "pattern:backlog-structured-markdown",
      "type": "pattern",
      "title": "Structured Markdown Pattern",
      "summary": "YAML frontmatter + Markdown content enables dual access - structured data for AI, natural language for humans",
      "content": "**Problem**: Need format readable by both AI (structured data) and humans (natural language).\n\n**Solution**: YAML frontmatter for metadata + Markdown for content. Single file serves both machines and humans.\n\n**File Structure**: Frontmatter (task_id, title, status, priority, labels, dependencies, milestone) + Markdown sections (Description, Acceptance Criteria, Implementation Plan, Notes, Related).\n\n**YAML Fields**:\n- Required: task_id, title, status, created_at, updated_at\n- Optional: description, priority, labels, assignee, dependencies, milestone, acceptance_criteria\n\n**Sync Behavior**: MCP edit → frontmatter updated + markdown section appended → TASK.md regenerated. Markdown edit → MCP reflects changes on next query.\n\n**Benefits**: Version control friendly (git diff works), human editable (any text editor), AI queryable (MCP tools), single source of truth.\n\n**Files**: backlog/tasks/*.md (one file per task)",
      "atom_type": "pattern",
      "vendor": "backlog-md",
      "equipment_type": "task-management",
      "source_document": "docs/architecture/BACKLOG_MCP_PATTERNS.md",
      "keywords": [
        "yaml",
        "markdown",
        "frontmatter",
        "dual-access",
        "structured-data"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:yaml",
        "concept:markdown"
      ],
      "code_example": "```markdown\n---\ntask_id: task-56\ntitle: \"BUILD: LRU cache with TTL\"\nstatus: In Progress\npriority: high\nlabels:\n  - llm\n  - performance\nassignee:\n  - claude\ncreated_at: 2025-12-21T10:30:00Z\nupdated_at: 2025-12-21T14:45:00Z\nmilestone: \"Week 2: LLM Enhancements\"\n---\n\n# BUILD: LRU cache with TTL\n\n## Description\nImplement OrderedDict-based LRU cache to reduce API costs.\n\n## Acceptance Criteria\n- [ ] Cache stores responses with SHA256 key\n- [ ] TTL-based expiration (default 1 hour)\n- [ ] LRU eviction when max_size reached\n\n## Notes\n- Pattern from Redis TTL + Python OrderedDict\n- Expected 30-40% cost reduction\n```"
    },
    {
      "atom_id": "pattern:backlog-state-machine",
      "type": "pattern",
      "title": "Task State Machine Pattern",
      "summary": "Simple 3-state machine (To Do → In Progress → Done) with explicit transitions prevents lifecycle ambiguity",
      "content": "**Problem**: Task lifecycle needs clear transitions to prevent ambiguity (is \"testing\" in progress or done?).\n\n**Solution**: Simple 3-state machine with explicit transitions: To Do (pending) → In Progress (active) → Done (completed) → Archived (historical).\n\n**Transitions**:\n- To Do → In Progress: backlog.task_edit({id, status: \"In Progress\"})\n- In Progress → Done: backlog.task_edit({id, status: \"Done\"})\n- Done → Archived: backlog.task_archive({id})\n\n**State Invariants**:\n1. Exactly ONE task \"In Progress\" at a time (enforced by workflow)\n2. Tasks can't skip states (warning logged if attempted)\n\n**Workflow**: Get current in-progress task → complete it → start next task. Prevents task thrashing and ensures focus.\n\n**Files**: Enforced in backlog.md MCP server task_edit handler",
      "atom_type": "pattern",
      "vendor": "backlog-md",
      "equipment_type": "task-management",
      "source_document": "docs/architecture/BACKLOG_MCP_PATTERNS.md",
      "keywords": [
        "state-machine",
        "task-lifecycle",
        "transitions",
        "workflow"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:state-machines"
      ],
      "code_example": "```javascript\n// Get current task\nlet current = backlog.task_list({status: \"In Progress\"})\n\n// Complete current before starting next\nif (current.length > 0) {\n  backlog.task_edit({id: current[0].task_id, status: \"Done\"})\n}\n\n// Start next task\nbacklog.task_edit({id: \"task-57\", status: \"In Progress\"})\n\n// INVALID: Skip state\nbacklog.task_edit({id: \"task-56\", status: \"Done\"})  // Warning logged\n\n// VALID: Follow transitions\nbacklog.task_edit({id: \"task-56\", status: \"In Progress\"})\nbacklog.task_edit({id: \"task-56\", status: \"Done\"})\n```"
    },
    {
      "atom_id": "pattern:pai-config-hooks",
      "type": "pattern",
      "title": "Hook/Event System for AI Lifecycle",
      "summary": "TypeScript hook system enables automation on AI lifecycle events (session start, tool use, task complete) without hardcoding logic",
      "content": "**Problem**: Need to react to AI lifecycle events without hardcoding logic into Claude Code.\n\n**Solution**: TypeScript hook system with standardized event payloads configured via settings.json.\n\n**Architecture**: Claude Code Event → Hook Registry (settings.json) → Load Hook Module (TypeScript) → Execute Hook Handler (async) → Continue Claude Code Execution.\n\n**Hook Types**:\n- onSessionStart: Initialize PAI session, load preferences, set env vars\n- onToolUse: Capture all tool usage for analytics, track LLM costs\n- onTaskComplete: Save checkpoint, update Backlog.md, send voice notification\n- onPromptSubmit: Load core context\n\n**Event Payloads**: HookContext interface with session_id, user_id, timestamp, event-specific payload.\n\n**Benefits**: Decoupled logic (hooks are separate modules), async execution (don't block main flow), composable (multiple hooks per event).\n\n**Files**: hooks/*.ts (one file per hook), settings.json (hook registry)",
      "atom_type": "pattern",
      "vendor": "pai-config-windows",
      "equipment_type": "ai-infrastructure",
      "source_document": "docs/architecture/PAI_CONFIG_PATTERNS.md",
      "keywords": [
        "hooks",
        "events",
        "lifecycle",
        "automation",
        "typescript"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:event-driven-architecture",
        "concept:hooks"
      ],
      "code_example": "```typescript\n// settings.json\n{\n  \"hooks\": {\n    \"onSessionStart\": \"hooks/initialize-pai-session.ts\",\n    \"onToolUse\": \"hooks/capture-all-events.ts\",\n    \"onTaskComplete\": \"hooks/on-task-complete.ts\"\n  }\n}\n\n// hooks/on-task-complete.ts\nimport { TaskCompleteEvent } from './types'\n\nexport async function onTaskComplete(event: TaskCompleteEvent): Promise<void> {\n  // Save checkpoint\n  await saveCheckpoint({\n    task_id: event.task_id,\n    phase: 'completed',\n    progress: 1.0,\n    state: event.final_state\n  })\n\n  // Update Backlog.md\n  await updateTask(event.task_id, {\n    status: 'Done',\n    completed_at: Date.now()\n  })\n\n  // Notify user\n  await notifyVoice(`Task ${event.task_id} completed successfully`)\n}\n```"
    },
    {
      "atom_id": "pattern:pai-config-context-sync",
      "type": "pattern",
      "title": "Checkpoint-Based Context Synchronization",
      "summary": "CHECKPOINT.md protocol enables AI sessions to restore state after interruptions (crash, restart) with zero context loss",
      "content": "**Problem**: AI sessions lose context when interrupted (power loss, crash, manual restart).\n\n**Solution**: Checkpoint-based state restoration with CHECKPOINT.md protocol. Save state periodically (every 5 minutes) to markdown file with JSON state.\n\n**Architecture**: AI Session → Periodic Checkpoint (save state to CHECKPOINT.md) → Session Interrupted → New Session Starts → Detect CHECKPOINT.md → Restore State → AI Resumes Work.\n\n**Checkpoint Structure**: YAML frontmatter (checkpoint_id, session_id, phase, progress, task_id) + Markdown (What's Done, What's Next, Context) + JSON state block.\n\n**Implementation**: saveCheckpoint() writes CHECKPOINT.md + .checkpoint.json, restoreCheckpoint() reads and parses on session start.\n\n**Benefits**: Fault tolerance (no lost context), resumable sessions (pick up where left off), human readable (CHECKPOINT.md is markdown).\n\n**Files**: CHECKPOINT.md (root of project), .checkpoint.json (programmatic access)",
      "atom_type": "pattern",
      "vendor": "pai-config-windows",
      "equipment_type": "ai-infrastructure",
      "source_document": "docs/architecture/PAI_CONFIG_PATTERNS.md",
      "keywords": [
        "checkpoint",
        "context",
        "state-restoration",
        "fault-tolerance",
        "resumable"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:state-management",
        "concept:serialization"
      ],
      "code_example": "```typescript\ninterface Checkpoint {\n  checkpoint_id: string\n  session_id: string\n  phase: string  // \"planning\", \"implementation\", \"testing\", \"done\"\n  progress: number  // 0.0-1.0\n  task_id: string\n  created_at: string\n  context: string  // Markdown description\n  state: any  // JSON state\n}\n\nasync function saveCheckpoint(checkpoint: Checkpoint): Promise<void> {\n  const content = buildCheckpointMarkdown(checkpoint)\n  await fs.writeFile('CHECKPOINT.md', content)\n  await fs.writeFile('.checkpoint.json', JSON.stringify(checkpoint, null, 2))\n  console.log(`[PAI] Checkpoint saved: ${checkpoint.phase} (${checkpoint.progress * 100}%)`)\n}\n\nasync function restoreCheckpoint(): Promise<Checkpoint | null> {\n  if (!await fs.exists('CHECKPOINT.md')) return null\n  const content = await fs.readFile('CHECKPOINT.md', 'utf-8')\n  return parseCheckpointMarkdown(content)\n}\n```"
    },
    {
      "atom_id": "pattern:cross-repo-config-management",
      "type": "pattern",
      "title": "Cross-Repository Configuration Management",
      "summary": "Environment variables + structured config files pattern shared across Agent-Factory (database), Backlog.md (YAML), pai-config (JSON)",
      "content": "**Problem**: All 3 repos need runtime configuration without code changes.\n\n**Solutions**: Each repo uses appropriate config mechanism with environment variable fallback.\n\n**Agent-Factory**: Database-backed (agent_factory_settings table) with env fallback. settings.get(\"KEY\", category) → DB → ENV → default.\n\n**Backlog.md**: YAML config (backlog/config.yaml) with structured milestones, paths. Loaded on server start.\n\n**pai-config-windows**: JSON config (settings.json) with hooks, checkpoint settings. TypeScript strongly typed.\n\n**Shared Principle**: Environment variables as fallback + structured config files for complex settings. Single source of truth per setting.\n\n**Anti-Pattern**: Duplicate configuration across repos. Always use one source (usually pai-config) that propagates via env vars.\n\n**Files**: agent_factory_settings table (Supabase), backlog/config.yaml, settings.json",
      "atom_type": "pattern",
      "vendor": "cross-repo",
      "equipment_type": "integration",
      "source_document": "docs/patterns/CROSS_REPO_INTEGRATION.md",
      "keywords": [
        "configuration",
        "env-vars",
        "cross-repo",
        "single-source-of-truth"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:environment-variables",
        "concept:config-files"
      ],
      "code_example": "```python\n# Agent-Factory (database-backed)\nfrom agent_factory.core.settings_service import settings\nmodel = settings.get(\"DEFAULT_MODEL\", category=\"llm\")\n# Lookup: DB → ENV → default\n\n# Backlog.md (YAML config)\n# backlog/config.yaml\nbacklog_dir: \"./backlog\"\nmilestones:\n  - name: \"Week 2\"\n    description: \"LLM enhancements\"\n\n# pai-config (JSON config)\n// settings.json\n{\n  \"hooks\": {\"onToolUse\": \"hooks/capture-all-events.ts\"},\n  \"context\": {\"checkpointInterval\": 300}\n}\n```"
    },
    {
      "atom_id": "pattern:cross-repo-event-driven",
      "type": "pattern",
      "title": "Event-Driven Architecture Pattern",
      "summary": "Hook/callback registration with typed event payloads pattern used across all 3 CORE repos",
      "content": "**Problem**: Need to react to lifecycle events (session start, task complete, error) across different systems.\n\n**Solutions**: Each repo implements event-driven pattern with language-appropriate mechanism.\n\n**Agent-Factory**: Python callbacks with decorators. @on_agent_complete decorator registers handler.\n\n**Backlog.md**: MCP protocol events. task.on('status_change', handler) subscribes to state changes.\n\n**pai-config-windows**: TypeScript hook system. settings.json maps events to hook files.\n\n**Shared Principle**: Hook/callback registration with typed event payloads. Decoupled architecture where events trigger actions without tight coupling.\n\n**Integration**: Agent-Factory fires event → pai-config hook captures → Backlog.md MCP updates → TASK.md syncs.\n\n**Files**: agent_factory/core/callbacks.py, backlog MCP event emitters, hooks/*.ts",
      "atom_type": "pattern",
      "vendor": "cross-repo",
      "equipment_type": "integration",
      "source_document": "docs/patterns/CROSS_REPO_INTEGRATION.md",
      "keywords": [
        "event-driven",
        "callbacks",
        "hooks",
        "decoupled",
        "cross-repo"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:event-driven-architecture",
        "concept:observer-pattern"
      ],
      "code_example": "```python\n# Agent-Factory (callbacks)\nfrom agent_factory.core.callbacks import on_agent_complete\n\n@on_agent_complete\ndef handle_completion(agent_name, result):\n    print(f\"{agent_name} completed: {result}\")\n\n# Backlog.md (MCP protocol events)\ntask.on('status_change', (task_id, old_status, new_status) => {\n  console.log(`Task ${task_id}: ${old_status} → ${new_status}`)\n})\n\n# pai-config (hook system)\nexport async function onTaskComplete(context: HookContext) {\n  await saveCheckpoint(context)\n  await notifyUser(context.task_id)\n}\n```"
    },
    {
      "atom_id": "best-practice:database-failover-testing",
      "type": "best-practice",
      "title": "Database Failover Testing",
      "summary": "Test failover by temporarily disabling primary provider, verify automatic switchover, monitor latency impact",
      "content": "**Practice**: Regularly test database failover to ensure reliability.\n\n**Testing Steps**:\n1. Check baseline health: db.health_check_all() → all providers healthy\n2. Temporarily disable primary (firewall rule or pause service)\n3. Execute query: db.execute_query(\"SELECT version()\") → should automatically failover\n4. Verify logs show failover occurred\n5. Measure latency impact (+200ms expected)\n6. Re-enable primary, verify it becomes active again\n\n**Frequency**: Test quarterly, after infrastructure changes, before high-traffic events.\n\n**Monitoring**: Set up alerts for failover events (unusual, indicates primary issues).\n\n**Why It Matters**: Failover only works if tested. Untested failover = false sense of security.\n\n**Tools**: agent_factory/core/database_manager.py, monitoring dashboards",
      "atom_type": "best-practice",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "failover",
        "testing",
        "reliability",
        "database"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:database-failover",
        "pattern:agent-factory-database-failover"
      ],
      "code_example": "```python\nfrom agent_factory.core.database_manager import DatabaseManager\nimport time\n\ndb = DatabaseManager()\n\n# 1. Baseline health\nprint(\"Baseline:\", db.health_check_all())\n# {'supabase': True, 'railway': True, 'neon': True}\n\n# 2. Disable primary (manually via dashboard)\nprint(\"\\nDisabling Supabase... (do this manually)\")\ntime.sleep(5)\n\n# 3. Execute query (should failover)\nstart = time.time()\nresult = db.execute_query(\"SELECT version()\")\nlatency = (time.time() - start) * 1000\n\nprint(f\"Query succeeded: {result}\")\nprint(f\"Latency: {latency:.0f}ms\")  # Expect +200ms\n\n# 4. Check logs for failover\nprint(\"Check logs for: [WARN] Primary database unhealthy, failing over\")\n\n# 5. Re-enable primary\nprint(\"\\nRe-enable Supabase... (do this manually)\")\n```"
    },
    {
      "atom_id": "best-practice:sme-confidence-thresholds",
      "type": "best-practice",
      "title": "SME Agent Confidence Threshold Tuning",
      "summary": "Set domain-specific confidence thresholds (0.6-0.8) based on escalation cost vs accuracy tradeoff",
      "content": "**Practice**: Tune confidence thresholds per SME agent domain based on escalation cost.\n\n**Guidelines**:\n- **High Stakes** (safety, medical): 0.8+ threshold → More escalations, fewer errors\n- **Medium Stakes** (troubleshooting, how-to): 0.7 threshold → Balanced escalation\n- **Low Stakes** (general info, FAQs): 0.6 threshold → Fewer escalations, acceptable error rate\n\n**Tuning Process**:\n1. Start with 0.7 (default)\n2. Monitor escalation rate and error rate for 1 week\n3. If too many escalations (>30%): Lower threshold to 0.65\n4. If too many errors (user complaints): Raise threshold to 0.75\n5. Iterate until balanced\n\n**Metrics**: Track escalation_rate (escalations / total queries) and error_rate (user-reported wrong answers / total queries).\n\n**Why It Matters**: Wrong threshold wastes human time (too many escalations) or produces bad answers (too few escalations).\n\n**Tools**: agent_factory/templates/sme_agent_template.py (min_confidence parameter)",
      "atom_type": "best-practice",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "sme-agent",
        "confidence",
        "threshold",
        "tuning"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "pattern:agent-factory-sme-template"
      ],
      "code_example": "```python\nfrom agent_factory.templates import SMEAgentTemplate\n\n# High stakes domain (safety)\nclass SafetySME(SMEAgentTemplate):\n    def __init__(self):\n        super().__init__(\n            name=\"Safety SME\",\n            domain=\"safety\",\n            min_confidence=0.8,  # High threshold\n            max_docs=10\n        )\n\n# Medium stakes domain (troubleshooting)\nclass MotorSME(SMEAgentTemplate):\n    def __init__(self):\n        super().__init__(\n            name=\"Motor SME\",\n            domain=\"motor_control\",\n            min_confidence=0.7,  # Balanced\n            max_docs=10\n        )\n\n# Low stakes domain (general info)\nclass GeneralSME(SMEAgentTemplate):\n    def __init__(self):\n        super().__init__(\n            name=\"General SME\",\n            domain=\"general\",\n            min_confidence=0.6,  # Lower threshold\n            max_docs=10\n        )\n\n# Monitor metrics\nresult = sme.answer(query)\nlog_metrics({\n    \"confidence\": result.confidence,\n    \"escalated\": result.escalate,\n    \"domain\": sme.domain\n})\n```"
    },
    {
      "atom_id": "best-practice:task-one-at-a-time",
      "type": "best-practice",
      "title": "One Task In Progress Policy",
      "summary": "Maintain exactly ONE task in In Progress state to ensure focus and completion",
      "content": "**Practice**: Always complete current task before starting next. Exactly ONE task \"In Progress\" at any time.\n\n**Why It Matters**: Task thrashing (starting multiple tasks without finishing) leads to nothing getting done. Serial execution ensures completion.\n\n**Workflow**:\n1. Check current task: backlog.task_list({status: \"In Progress\"})\n2. If exists: Complete it before starting new task\n3. Mark new task In Progress\n4. Work until done\n5. Mark Done\n6. Repeat\n\n**Exception**: Blocked tasks. If current task is blocked, mark it \"To Do\" with blocker note, then start next.\n\n**Enforcement**: Workflow reminder in Backlog.md, not hard-enforced (allows flexibility for edge cases).\n\n**Benefits**: Clear focus, guaranteed progress, easy status reporting (\"What's current?\" has one answer).\n\n**Tools**: Backlog.md task_list, task_edit MCP tools",
      "atom_type": "best-practice",
      "vendor": "backlog-md",
      "equipment_type": "task-management",
      "source_document": "docs/architecture/BACKLOG_MCP_PATTERNS.md",
      "keywords": [
        "task-management",
        "workflow",
        "focus",
        "serial-execution"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "pattern:backlog-state-machine"
      ],
      "code_example": "```javascript\n// Check current task\nlet current = backlog.task_list({status: \"In Progress\"})\n\nif (current.length > 0) {\n  console.log(`Current task: ${current[0].title}`)\n\n  // Complete it first\n  backlog.task_edit({\n    id: current[0].task_id,\n    status: \"Done\",\n    notesAppend: [\"Completed successfully\"]\n  })\n}\n\n// Now start next task\nbacklog.task_edit({\n  id: \"task-57\",\n  status: \"In Progress\"\n})\n\n// Verify only one in progress\nlet check = backlog.task_list({status: \"In Progress\"})\nassert(check.length === 1, \"Should have exactly 1 in progress\")\n```"
    },
    {
      "atom_id": "best-practice:checkpoint-frequency",
      "type": "best-practice",
      "title": "Checkpoint Frequency Tuning",
      "summary": "Save checkpoints every 5 minutes for long tasks, every major phase completion for structured work",
      "content": "**Practice**: Tune checkpoint frequency based on task duration and complexity.\n\n**Guidelines**:\n- **Long-running tasks** (>30 min): Every 5 minutes (time-based)\n- **Structured tasks**: Every phase completion (event-based)\n- **Short tasks** (<10 min): Only on completion\n\n**Implementation**: Set checkpointInterval in pai-config settings.json. Time-based triggers via setInterval, event-based triggers via hooks.\n\n**Cost Tradeoff**: More checkpoints = more disk I/O but less lost work. Fewer checkpoints = faster but riskier.\n\n**Why It Matters**: Checkpoint too frequently → slow performance, unnecessary disk writes. Checkpoint too rarely → risk losing significant work on crash.\n\n**Monitoring**: Track checkpoint file sizes, disk usage, restoration frequency.\n\n**Tools**: pai-config checkpoint system, settings.json",
      "atom_type": "best-practice",
      "vendor": "pai-config-windows",
      "equipment_type": "ai-infrastructure",
      "source_document": "docs/architecture/PAI_CONFIG_PATTERNS.md",
      "keywords": [
        "checkpoint",
        "frequency",
        "tuning",
        "performance"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "pattern:pai-config-context-sync"
      ],
      "code_example": "```json\n// settings.json\n{\n  \"context\": {\n    \"checkpointInterval\": 300,  // 5 minutes in seconds\n    \"checkpointOnPhaseComplete\": true,\n    \"maxCheckpointSize\": 10485760  // 10MB limit\n  }\n}\n\n// Time-based checkpoint (every 5 min)\nsetInterval(async () => {\n  await saveCheckpoint(getCurrentState())\n}, 5 * 60 * 1000)\n\n// Event-based checkpoint (on phase complete)\nexport async function onPhaseComplete(event: PhaseCompleteEvent): Promise<void> {\n  await saveCheckpoint({\n    phase: event.next_phase,\n    progress: event.progress,\n    state: event.state\n  })\n}\n```"
    },
    {
      "atom_id": "pattern:llm-router-usage",
      "type": "pattern",
      "title": "LLM Router Implementation Example",
      "summary": "Complete example of using LLMRouter with capability-based routing, fallback, caching, and cost tracking",
      "content": "**Use Case**: Route LLM requests to cheapest capable model with automatic failover and cost tracking.\n\n**Implementation**: Import create_router(), configure with enable_cache and enable_fallback, use route_by_capability() for automatic model selection or complete() for explicit model.\n\n**Features Demonstrated**:\n- Capability-based routing (SIMPLE → gpt-3.5-turbo, COMPLEX → gpt-4o)\n- Fallback chain configuration\n- Automatic cost tracking\n- Cache configuration\n- Streaming support\n\n**Cost Savings**: This pattern reduced production costs from $750/month to $198/month (73% reduction) for 50+ agents.\n\n**Files**: agent_factory/llm/router.py, agent_factory/llm/types.py",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/AGENT_FACTORY_PATTERNS.md",
      "keywords": [
        "llm",
        "router",
        "implementation",
        "code-example"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "pattern:agent-factory-llm-router"
      ],
      "code_example": "```python\nfrom agent_factory.llm.router import create_router\nfrom agent_factory.llm.types import LLMConfig, LLMProvider, ModelCapability\nfrom agent_factory.llm.tracker import get_global_tracker\n\n# Create router with all features enabled\nrouter = create_router(\n    max_retries=3,\n    enable_fallback=True,  # Auto-failover on errors\n    enable_cache=True      # LRU cache with TTL\n)\n\n# Example 1: Capability-based routing (automatic model selection)\nresponse = router.route_by_capability(\n    messages=[{\"role\": \"user\", \"content\": \"Classify this email as spam/not spam\"}],\n    capability=ModelCapability.SIMPLE  # Uses cheapest: gpt-3.5-turbo\n)\nprint(f\"Model: {response.model}\")  # gpt-3.5-turbo\nprint(f\"Cost: ${response.usage.total_cost_usd:.6f}\")  # $0.000150\nprint(f\"Answer: {response.content}\")\n\n# Example 2: Explicit model with fallback chain\nconfig = LLMConfig(\n    provider=LLMProvider.OPENAI,\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    fallback_models=[\"gpt-3.5-turbo\", \"claude-3-haiku-20240307\"]  # Try these if primary fails\n)\n\nresponse = router.complete(\n    messages=[{\"role\": \"user\", \"content\": \"Write a Python function to parse JSON\"}],\n    config=config\n)\nprint(f\"Answer: {response.content}\")\n\n# Example 3: Streaming\nfor chunk in router.complete_stream(messages, config):\n    print(chunk.text, end=\"\", flush=True)\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage.total_cost_usd:.6f}\")\n        break\n\n# Example 4: Aggregate cost tracking\ntracker = get_global_tracker()\nstats = tracker.aggregate_stats()\nprint(f\"Total cost today: ${stats['total_cost_usd']:.2f}\")\nprint(f\"By model: {stats['costs_by_model']}\")\nprint(f\"Request count: {stats['request_count']}\")\nprint(f\"Cache hit rate: {stats['cache_hit_rate']:.1%}\")\n```"
    },
    {
      "atom_id": "pattern:archon-hybrid-search",
      "type": "pattern",
      "title": "PostgreSQL Hybrid Search Pattern",
      "summary": "Combine vector similarity + full-text search in single RPC function for better relevance than pure vector search",
      "content": "**Problem**: Pure vector search misses keyword-specific queries, pure full-text search misses semantic meaning.\n\n**Solution**: Hybrid search combining pgvector cosine similarity + PostgreSQL full-text search (ts_rank) in single database RPC function.\n\n**Architecture**: Execute vector search (ivfflat index) → Execute full-text search (GIN index) → FULL OUTER JOIN results → Deduplicate by id → Score by GREATEST(vector_sim, text_rank) → Return top-k.\n\n**PostgreSQL RPC Function**: hybrid_search_archon_crawled_pages(query_embedding vector(1536), query_text TEXT, match_count INT, filter JSONB, source_filter TEXT).\n\n**Benefits**:\n- Better relevance than single method\n- Database-level optimization (faster than app-level merging)\n- Match type tracking (vector, text, both)\n- Leverages PostgreSQL's built-in text search\n\n**Indexes Required**: IVFFlat index on embedding columns, GIN index on to_tsvector(content).\n\n**Pattern Origin**: Archon production system (13.4k⭐) serving RAG queries at scale.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (SQL functions, lines 361-423)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "rag-search",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "hybrid-search",
        "pgvector",
        "full-text-search",
        "postgresql",
        "rag"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:vector-search",
        "concept:full-text-search",
        "concept:postgresql-rpc"
      ],
      "code_example": "```sql\n-- PostgreSQL RPC function\nCREATE OR REPLACE FUNCTION hybrid_search_archon_crawled_pages(\n    query_embedding vector(1536),\n    query_text TEXT,\n    match_count INT DEFAULT 5\n)\nRETURNS TABLE (id UUID, url TEXT, content TEXT, similarity FLOAT, match_type TEXT)\nLANGUAGE plpgsql AS $$\nBEGIN\n    RETURN QUERY\n    WITH vector_search AS (\n        SELECT p.id, p.url, p.content,\n               1 - (p.embedding_1536 <=> query_embedding) AS similarity,\n               'vector' AS match_type\n        FROM archon_crawled_pages p\n        ORDER BY p.embedding_1536 <=> query_embedding\n        LIMIT match_count\n    ),\n    text_search AS (\n        SELECT p.id, p.url, p.content,\n               ts_rank(to_tsvector('english', p.content), plainto_tsquery('english', query_text)) AS similarity,\n               'text' AS match_type\n        FROM archon_crawled_pages p\n        WHERE to_tsvector('english', p.content) @@ plainto_tsquery('english', query_text)\n        ORDER BY similarity DESC\n        LIMIT match_count\n    )\n    SELECT DISTINCT ON (v.id)\n        COALESCE(v.id, t.id) AS id,\n        COALESCE(v.url, t.url) AS url,\n        COALESCE(v.content, t.content) AS content,\n        GREATEST(COALESCE(v.similarity, 0), COALESCE(t.similarity, 0)) AS similarity,\n        CASE WHEN v.id IS NOT NULL AND t.id IS NOT NULL THEN 'both' ELSE COALESCE(v.match_type, t.match_type) END AS match_type\n    FROM vector_search v\n    FULL OUTER JOIN text_search t ON v.id = t.id\n    ORDER BY v.id, similarity DESC\n    LIMIT match_count;\nEND;\n$$;\n```"
    },
    {
      "atom_id": "pattern:archon-multidimensional-embeddings",
      "type": "pattern",
      "title": "Multi-Dimensional Embedding Support",
      "summary": "Store embeddings in multiple dimensions (768, 1024, 1536, 3072) to future-proof for model changes and support multiple providers",
      "content": "**Problem**: Different embedding models use different dimensions. Changing models requires regenerating all embeddings.\n\n**Solution**: Store embeddings in multiple columns (embedding_768, embedding_1536, etc.) with metadata tracking which model was used.\n\n**Schema Pattern**: archon_crawled_pages table with separate vector columns per dimension + embedding_model + embedding_dimension tracking.\n\n**Index Strategy**: Create separate IVFFlat indexes per dimension (idx_crawled_pages_embedding_768, idx_crawled_pages_embedding_1536). Improves query performance vs single HNSW index.\n\n**Dynamic Column Selection**: Query uses embedding_dimension to select correct column dynamically. Application code determines which column to use based on current embedding model.\n\n**Benefits**:\n- No re-indexing when switching models\n- Support multiple providers simultaneously\n- A/B test different embedding models\n- Gradual migration path\n\n**Provider Examples**: OpenAI text-embedding-3-small (1536), Voyage-2 (1024), Nomic-embed-text (768), OpenAI text-embedding-3-large (3072).\n\n**Files**: docs/architecture/archon_architecture_analysis.md (schema lines 247-287)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "rag-search",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "embeddings",
        "multi-dimensional",
        "pgvector",
        "future-proof",
        "schema-design"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:vector-embeddings",
        "concept:pgvector"
      ],
      "code_example": "```sql\n-- Multi-dimensional embedding support\nCREATE TABLE archon_crawled_pages (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    url TEXT NOT NULL,\n    content TEXT NOT NULL,\n\n    -- Multi-dimensional embeddings\n    embedding_768 vector(768),\n    embedding_1024 vector(1024),\n    embedding_1536 vector(1536),\n    embedding_3072 vector(3072),\n\n    -- Model tracking\n    embedding_model TEXT,  -- 'text-embedding-3-small'\n    embedding_dimension INT,  -- 1536\n\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Separate indexes per dimension\nCREATE INDEX idx_embedding_768 ON archon_crawled_pages USING ivfflat (embedding_768 vector_cosine_ops);\nCREATE INDEX idx_embedding_1536 ON archon_crawled_pages USING ivfflat (embedding_1536 vector_cosine_ops);\n\n-- Dynamic column selection in Python\ncolumn = f\"embedding_{dimension}\"\nquery = f\"\"\"\n    SELECT id, url, content,\n           1 - ({column} <=> $1) AS similarity\n    FROM archon_crawled_pages\n    ORDER BY {column} <=> $1\n    LIMIT $2\n\"\"\"\nresults = await db.execute(query, query_embedding, match_count)\n```"
    },
    {
      "atom_id": "pattern:archon-batch-processing",
      "type": "pattern",
      "title": "Batch Processing with Progress Reporting",
      "summary": "Process large datasets (1000+ chunks) in batches with progress callbacks, retry logic, and graceful degradation",
      "content": "**Problem**: Ingesting 1000+ document chunks at once causes memory issues, timeouts, and poor user experience (no visibility).\n\n**Solution**: Batch processing (50 items default) with progress callbacks, exponential backoff retry, and individual insert fallback.\n\n**Pattern**: Split data into batches → Process each batch → Report progress → On failure, retry batch 3 times → Final attempt: individual inserts → Continue with next batch.\n\n**Progress Reporting**: Callback function receives (stage, progress_pct, message). Stages: discovery, crawling, processing, storage, completed.\n\n**Retry Strategy**: Batch insert fails → Wait 1s, retry → Wait 2s, retry → Wait 4s, retry → Individual inserts (skip failures).\n\n**Graceful Degradation**: Individual insert failures logged but don't block entire batch. User sees which URLs failed.\n\n**Configuration**: Batch size from settings (database-backed), default 50. Adjustable per operation.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (batch processing lines 516-552)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "data-processing",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "batch-processing",
        "progress-reporting",
        "retry-logic",
        "graceful-degradation"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:batch-processing",
        "concept:exponential-backoff"
      ],
      "code_example": "```python\nasync def add_documents_to_supabase(contents, batch_size=50, progress_callback=None):\n    # Load batch size from settings\n    batch_size = await settings.get_int(\"DOCUMENT_STORAGE_BATCH_SIZE\", 50)\n\n    total_batches = (len(contents) + batch_size - 1) // batch_size\n\n    for batch_num, i in enumerate(range(0, len(contents), batch_size), 1):\n        # Get batch\n        batch = contents[i:i+batch_size]\n\n        # Generate embeddings\n        embeddings = await create_embeddings_batch(batch)\n\n        # Insert with retry logic\n        for retry in range(3):\n            try:\n                client.table(\"archon_crawled_pages\").insert(batch_data).execute()\n                break\n            except Exception as e:\n                if retry == 2:\n                    # Final attempt: individual inserts\n                    for record in batch_data:\n                        try:\n                            client.table(\"archon_crawled_pages\").insert(record).execute()\n                        except:\n                            logger.error(f\"Failed to insert {record['url']}\")\n                else:\n                    await asyncio.sleep(2 ** retry)  # Exponential backoff\n\n        # Report progress\n        progress = int((batch_num / total_batches) * 100)\n        if progress_callback:\n            await progress_callback(\"processing\", progress, f\"Batch {batch_num}/{total_batches}\")\n```"
    },
    {
      "atom_id": "pattern:archon-service-layer",
      "type": "pattern",
      "title": "Service Layer Abstraction",
      "summary": "Separate business logic from HTTP concerns via service layer - improves testability, reusability, and maintainability",
      "content": "**Problem**: Business logic mixed with FastAPI routes makes code hard to test and reuse across different interfaces (API, MCP, CLI).\n\n**Solution**: API Route → Service → Database architecture. Services contain business logic, routes handle HTTP concerns.\n\n**Pattern**: FastAPI route receives request → Validate input → Call service method → Service executes business logic → Service queries database → Route formats response.\n\n**Benefits**:\n- Services testable without FastAPI\n- Same service used by API routes, MCP tools, CLI commands\n- Clear separation of concerns\n- Easier to add new interfaces (gRPC, GraphQL)\n\n**Service Examples**: RAGService (search strategies, reranking), CrawlingService (web scraping), EmbeddingService (batch embedding generation), CredentialService (settings management).\n\n**Testing**: Services can be unit tested with mocked database. Routes integration tested with real services.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (service layer lines 86-147)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "architecture",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "service-layer",
        "separation-of-concerns",
        "testability",
        "architecture"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:layered-architecture",
        "concept:dependency-injection"
      ],
      "code_example": "```python\n# api_routes/knowledge_api.py\n@router.post(\"/api/rag/query\")\nasync def rag_query(request: RAGQueryRequest, user = Depends(verify_token)):\n    # Validate input (HTTP concern)\n    # Call service layer (business logic)\n    success, result = await rag_service.perform_rag_query(\n        query=request.query,\n        source=request.source,\n        match_count=request.match_count\n    )\n    # Format response (HTTP concern)\n    return result\n\n# services/search/rag_service.py\nclass RAGService:\n    async def perform_rag_query(self, query, source, match_count):\n        # Business logic (independent of HTTP)\n        # 1. Create embedding\n        embedding = await create_embedding(query)\n\n        # 2. Search with hybrid strategy\n        results = await self.hybrid_strategy.search_documents(...)\n\n        # 3. Rerank if enabled\n        if self.reranking_strategy:\n            results = await self.reranking_strategy.rerank_results(...)\n\n        # 4. Return data (not HTTP response)\n        return True, {\"results\": results, \"search_mode\": \"hybrid\"}\n\n# tests/test_rag_service.py\nasync def test_rag_query():\n    # Test service directly (no FastAPI)\n    service = RAGService()\n    success, result = await service.perform_rag_query(\"test query\", None, 5)\n    assert success\n    assert len(result['results']) <= 5\n```"
    },
    {
      "atom_id": "pattern:archon-mcp-http",
      "type": "pattern",
      "title": "HTTP-Based MCP Server Architecture",
      "summary": "Run MCP server as separate microservice communicating via HTTP instead of direct imports for true isolation",
      "content": "**Problem**: MCP servers importing from main application create tight coupling, shared dependencies, and deployment complexity.\n\n**Solution**: Run MCP server as independent microservice on separate port, communicate via HTTP, no shared imports.\n\n**Architecture**: Claude Code → MCP Client (stdio) → MCP Server (port 8051) → HTTP calls → FastAPI Server (port 8181) → Database.\n\n**Benefits**:\n- True microservices isolation\n- Independent restart/upgrade\n- Clear service boundaries\n- Easier testing and deployment\n- Different scaling policies (MCP server can scale separately)\n\n**Trade-off**: +10-20ms latency per request (HTTP overhead) vs direct calls. Acceptable for async operations.\n\n**Pattern**: Each MCP tool makes HTTP request to FastAPI endpoint. Example: rag_search_knowledge_base() calls POST /api/rag/query.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (MCP server lines 148-198)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "microservices",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "mcp",
        "microservices",
        "http",
        "isolation",
        "architecture"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:microservices",
        "concept:mcp-protocol"
      ],
      "code_example": "```python\n# src/mcp_server/features/rag/rag_tools.py\nimport httpx\nimport json\n\nAPI_URL = \"http://localhost:8181\"\n\n@mcp.tool()\nasync def rag_search_knowledge_base(ctx, query, source_id=None, match_count=5):\n    \"\"\"Search knowledge base using RAG\"\"\"\n\n    # HTTP call to FastAPI server (not direct import)\n    async with httpx.AsyncClient() as client:\n        response = await client.post(\n            f\"{API_URL}/api/rag/query\",\n            json={\n                \"query\": query,\n                \"source\": source_id,\n                \"match_count\": match_count\n            },\n            timeout=30.0\n        )\n\n        if response.status_code != 200:\n            return json.dumps({\"error\": response.text})\n\n        return json.dumps(response.json())\n\n# Benefits:\n# 1. MCP server can restart without affecting FastAPI\n# 2. No shared dependencies (different package.json)\n# 3. Can deploy to different servers\n# 4. Easier to add new MCP tools (just HTTP calls)\n```"
    },
    {
      "atom_id": "pattern:archon-contextual-embeddings",
      "type": "pattern",
      "title": "Contextual Embeddings for Better Retrieval",
      "summary": "LLM rewrites document chunks with surrounding context before embedding to improve semantic search quality",
      "content": "**Problem**: Embedding individual chunks loses document context, leading to poor semantic search (\"it\" refers to what?).\n\n**Solution**: Before embedding chunk, use LLM to rewrite with document context. \"The system failed\" → \"The HVAC system failed due to low refrigerant\".\n\n**Process**: For each chunk → Get surrounding chunks (±2) → LLM prompt: \"Rewrite this chunk to be standalone, incorporating context\" → Embed rewritten chunk → Store both original + rewritten.\n\n**Cost**: ~$0.01 per 1000 chunks (using gpt-4o-mini). One-time cost during ingestion.\n\n**Quality Improvement**: 15-20% better retrieval accuracy in production. Especially helps with pronouns, abbreviations, implicit references.\n\n**Configuration**: Optional via settings (USE_CONTEXTUAL_EMBEDDINGS = true/false). Disabled by default due to cost.\n\n**When To Use**: High-value knowledge bases (technical docs, legal docs) where accuracy > cost.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (contextual embeddings lines 494-496, 529-531)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "rag-search",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "contextual-embeddings",
        "rag",
        "quality",
        "semantic-search"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:embeddings",
        "concept:semantic-search"
      ],
      "code_example": "```python\nasync def generate_contextual_embedding(chunk: Dict, surrounding_chunks: List[Dict]) -> Dict:\n    \"\"\"Rewrite chunk with context before embedding\"\"\"\n\n    # Build context from surrounding chunks\n    context = \"\\n\".join([c['content'] for c in surrounding_chunks])\n\n    prompt = f\"\"\"\n    Document context:\n    {context}\n\n    Original chunk:\n    {chunk['content']}\n\n    Rewrite this chunk to be standalone and self-explanatory,\n    incorporating relevant context from the surrounding text.\n    Expand abbreviations, resolve pronouns, add necessary details.\n    \"\"\"\n\n    # Use cheap model for rewriting\n    response = await llm_router.route(\n        prompt,\n        task_type=\"simple\",  # gpt-4o-mini\n        max_cost=0.001\n    )\n\n    # Embed rewritten version\n    rewritten = response['response']\n    embedding = await create_embedding(rewritten)\n\n    return {\n        **chunk,\n        'content_original': chunk['content'],\n        'content_rewritten': rewritten,\n        'embedding': embedding\n    }\n\n# Example:\n# Original: \"It failed due to insufficient refrigerant\"\n# Rewritten: \"The HVAC system failed due to insufficient refrigerant levels in the compressor\"\n# Better retrieval for query: \"HVAC refrigerant issues\"\n```"
    },
    {
      "atom_id": "pattern:archon-progress-tracking",
      "type": "pattern",
      "title": "Multi-Stage Progress Reporting Pattern",
      "summary": "Long-running operations report detailed progress (stage, percentage, message) for user visibility and cancellation support",
      "content": "**Problem**: Long operations (web crawling, embedding generation) provide no feedback, users don't know if it's working or stuck.\n\n**Solution**: Multi-stage progress reporting with callbacks. Each stage reports (stage_name, progress_pct, status_message).\n\n**Stages**: discovery (10%), crawling (10-50%), processing (50-90%), storage (90-100%), completed (100%).\n\n**Callback Pattern**: async function progress_callback(stage: str, progress: int, message: str). Called at each milestone.\n\n**Cancellation Support**: progress_callback can raise CancelledError to abort operation gracefully. Check before expensive operations.\n\n**Frontend Integration**: WebSocket or polling receives progress updates → displays progress bar → shows current stage.\n\n**Benefits**: User visibility, estimated time remaining, ability to cancel, debugging (see where it's stuck).\n\n**Files**: docs/architecture/archon_architecture_analysis.md (progress pattern lines 759-784)",
      "atom_type": "pattern",
      "vendor": "archon",
      "equipment_type": "user-experience",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "progress-reporting",
        "long-running-operations",
        "cancellation",
        "user-experience"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:async-callbacks",
        "concept:websockets"
      ],
      "code_example": "```python\nasync def crawl_website(url, progress_callback):\n    \"\"\"Multi-stage web crawling with progress\"\"\"\n\n    # Stage 1: Discovery (0-10%)\n    await progress_callback(\"discovery\", 10, \"Finding pages...\")\n    pages = await discover_pages(url)\n\n    # Stage 2: Crawling (10-50%)\n    for i, page in enumerate(pages):\n        progress = 10 + int((i / len(pages)) * 40)\n        await progress_callback(\"crawling\", progress, f\"Page {i+1}/{len(pages)}\")\n        await crawl_page(page)\n\n    # Stage 3: Processing (50-90%)\n    await progress_callback(\"processing\", 60, \"Generating embeddings...\")\n    await generate_embeddings()\n\n    # Stage 4: Storage (90-100%)\n    await progress_callback(\"storage\", 90, \"Saving to database...\")\n    await save_to_db()\n\n    # Complete\n    await progress_callback(\"completed\", 100, \"Crawl complete\")\n\n# Cancellation support\ndef cancellation_check():\n    if is_cancelled:\n        raise asyncio.CancelledError(\"Operation cancelled by user\")\n\nasync def process_batch(batch, cancellation_check):\n    for item in batch:\n        cancellation_check()  # Check before each item\n        await process_item(item)\n```"
    },
    {
      "atom_id": "pattern:platform-llm-routing",
      "type": "pattern",
      "title": "Multi-LLM Cost Optimization Routing",
      "summary": "Route requests through Llama3 (local) → Perplexity (cheap) → Claude (expensive) chain to minimize costs while maintaining quality",
      "content": "**Problem**: Using expensive LLMs (Claude Sonnet) for all tasks wastes money on simple queries.\n\n**Solution**: Three-tier LLM routing based on task type with automatic fallback. Local (free) → Perplexity ($0.001/1K) → Claude ($0.015/1K).\n\n**Task Classification**:\n- Simple queries, classification → Llama3 (local Ollama)\n- Research, fact-checking → Perplexity Sonar (online)\n- Complex reasoning, code generation → Claude Sonnet\n\n**Fallback Chain**: Try cheapest capable model → On failure, try next expensive → Continue until success or all fail.\n\n**Cost Tracking**: Track cost per request, aggregate by user, enforce budget limits.\n\n**Max Cost Parameter**: Optional max_cost parameter filters models. Useful for budget constraints.\n\n**Expected Savings**: 60-70% cost reduction vs using Claude for everything.\n\n**Files**: docs/architecture/00_architecture_platform.md (LLM router lines 466-564)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/00_architecture_platform.md",
      "keywords": [
        "llm-routing",
        "cost-optimization",
        "multi-provider",
        "fallback"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:llm-api",
        "concept:cost-optimization"
      ],
      "code_example": "```python\nclass LLMRouter:\n    def __init__(self):\n        self.models = {\n            \"local\": {\n                \"model\": \"ollama/llama3\",\n                \"cost_per_1k\": 0.0,\n                \"use_for\": [\"simple_queries\", \"classification\"]\n            },\n            \"perplexity\": {\n                \"model\": \"perplexity/llama-3.1-sonar-small-128k-online\",\n                \"cost_per_1k\": 0.001,\n                \"use_for\": [\"research\", \"fact_checking\"]\n            },\n            \"claude\": {\n                \"model\": \"anthropic/claude-sonnet-4-5\",\n                \"cost_per_1k\": 0.015,\n                \"use_for\": [\"complex_reasoning\", \"code_generation\"]\n            }\n        }\n        self.fallback_chain = [\"local\", \"perplexity\", \"claude\"]\n\n    async def route(self, prompt: str, task_type: str, max_cost: Optional[float] = None):\n        # Select best model for task\n        preferred = self._select_model(task_type, max_cost)\n\n        # Try preferred model\n        try:\n            response = await completion(\n                model=self.models[preferred][\"model\"],\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n            return {\"response\": response.content, \"model\": preferred, \"cost\": self._calculate_cost(response, preferred)}\n        except Exception as e:\n            # Fallback to next model\n            return await self._fallback(prompt, preferred)\n```"
    },
    {
      "atom_id": "pattern:platform-brain-fart-checker",
      "type": "pattern",
      "title": "AI-Powered Idea Validator (Brain Fart Checker)",
      "summary": "Multi-agent evaluation system with hard kill criteria validates startup ideas before building",
      "content": "**Problem**: Building products nobody wants wastes months. Need objective idea validation.\n\n**Solution**: Multi-agent system evaluates ideas across 4 dimensions with hard kill criteria. Returns BUILD, ITERATE, or KILL verdict.\n\n**Agents**:\n1. Market Research Agent (Perplexity): TAM, trends, pain points, pricing\n2. Competitive Analysis Agent (Perplexity): Direct competitors, feature comparison\n3. Technical Feasibility Agent (Claude): Build complexity, tech stack, timeline\n4. Revenue Projection Agent (Claude): Realistic 6-month MRR estimate\n\n**Kill Criteria** (automatic KILL if fails any):\n- Novelty score < 60 (too generic)\n- Estimated 6-month MRR < $2K (not viable)\n- Competitor count > 20 (too saturated)\n\n**Output**: Structured IdeaEvaluation (novelty_score, market_size_score, competition_score, execution_difficulty, estimated_mrr, verdict, reasoning, next_steps).\n\n**Use Case**: Validate ideas before investing time/money.\n\n**Files**: docs/architecture/00_architecture_platform.md (brain fart checker lines 668-776)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "ai-orchestration",
      "source_document": "docs/architecture/00_architecture_platform.md",
      "keywords": [
        "idea-validation",
        "multi-agent",
        "startup",
        "brain-fart-checker"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:multi-agent-systems",
        "concept:structured-output"
      ],
      "code_example": "```python\nfrom pydantic import BaseModel, Field\n\nclass IdeaEvaluation(BaseModel):\n    novelty_score: float = Field(ge=0, le=100)\n    market_size_score: float = Field(ge=0, le=100)\n    competition_score: float = Field(ge=0, le=100)\n    execution_difficulty: float = Field(ge=0, le=100)\n    estimated_mrr: float = Field(ge=0)\n    competitor_count: int = Field(ge=0)\n    verdict: str  # BUILD, ITERATE, KILL\n    reasoning: str\n    next_steps: List[str]\n\nclass BrainFartChecker:\n    KILL_CRITERIA = {\n        \"novelty_score\": 60,\n        \"estimated_mrr\": 2000,\n        \"competitor_count\": 20\n    }\n\n    async def evaluate(self, idea: str) -> IdeaEvaluation:\n        # Agent 1: Market research\n        market_data = await self._research_market(idea)\n\n        # Agent 2: Competitive analysis\n        competitors = await self._analyze_competitors(idea)\n\n        # Agent 3: Technical feasibility\n        feasibility = await self._assess_feasibility(idea)\n\n        # Agent 4: Revenue projection\n        revenue = await self._project_revenue(idea, market_data)\n\n        # Synthesize\n        evaluation = await self._synthesize(idea, market_data, competitors, feasibility, revenue)\n\n        # Apply kill criteria\n        if evaluation.novelty_score < self.KILL_CRITERIA[\"novelty_score\"]:\n            evaluation.verdict = \"KILL\"\n            evaluation.reasoning = f\"Novelty too low ({evaluation.novelty_score}). Market is saturated.\"\n\n        return evaluation\n```"
    },
    {
      "atom_id": "pattern:platform-rate-limiting",
      "type": "pattern",
      "title": "Redis-Based Sliding Window Rate Limiting",
      "summary": "Tier-based rate limits (Free: 10/min, Pro: 100/min, Enterprise: 1000/min) using Redis sorted sets for accurate sliding window",
      "content": "**Problem**: Need to enforce API rate limits per user tier without blocking legitimate traffic.\n\n**Solution**: Redis sorted set with timestamps as scores implements sliding window rate limit. More accurate than fixed window.\n\n**Algorithm**:\n1. Remove requests older than window (1 minute ago)\n2. Count requests in current window\n3. If count >= limit, reject (HTTP 429)\n4. If count < limit, add current request to set\n5. Set expiry on key (60 seconds)\n\n**Tier Limits**:\n- Free: 10 requests/minute\n- Pro: 100 requests/minute\n- Enterprise: 1000 requests/minute\n\n**Advantages over Fixed Window**: No burst at window boundaries, accurate per-second rate limiting.\n\n**Response Headers**: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, Retry-After (on 429).\n\n**Files**: docs/architecture/00_architecture_platform.md (rate limiting lines 371-438)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "api-gateway",
      "source_document": "docs/architecture/00_architecture_platform.md",
      "keywords": [
        "rate-limiting",
        "redis",
        "sliding-window",
        "api-gateway"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:rate-limiting",
        "concept:redis"
      ],
      "code_example": "```python\nfrom redis import asyncio as aioredis\nfrom datetime import datetime, timedelta\n\nclass RateLimiter:\n    def __init__(self, redis: aioredis.Redis):\n        self.redis = redis\n        self.limits = {\n            \"free\": 10,\n            \"pro\": 100,\n            \"enterprise\": 1000\n        }\n\n    async def check_limit(self, user_id: str, tier: str) -> bool:\n        \"\"\"Sliding window rate limit\"\"\"\n        now = datetime.now()\n        window_start = now - timedelta(minutes=1)\n\n        # Redis sorted set: score = timestamp\n        key = f\"rate_limit:{user_id}\"\n\n        # Remove old requests\n        await self.redis.zremrangebyscore(\n            key,\n            0,\n            window_start.timestamp()\n        )\n\n        # Count requests in window\n        count = await self.redis.zcard(key)\n\n        if count >= self.limits[tier]:\n            return False  # Rate limit exceeded\n\n        # Add current request\n        await self.redis.zadd(\n            key,\n            {str(now.timestamp()): now.timestamp()}\n        )\n\n        # Set expiry\n        await self.redis.expire(key, 60)\n\n        return True  # Request allowed\n```"
    },
    {
      "atom_id": "pattern:platform-connection-pooling",
      "type": "pattern",
      "title": "Async PostgreSQL Connection Pooling",
      "summary": "asyncpg connection pool (min=10, max=100) with SSL for production and RLS context setting per request",
      "content": "**Problem**: Creating new database connections per request is slow (50-100ms) and doesn't scale.\n\n**Solution**: asyncpg connection pool maintains warm connections (min=10, max=100) with automatic scaling.\n\n**Configuration**:\n- min_size: 10 (keep warm for fast response)\n- max_size: 100 (max concurrent queries)\n- command_timeout: 60 seconds (prevent hung queries)\n- SSL: required in production\n\n**RLS Context**: Set app.current_user_id before each query for Row-Level Security enforcement.\n\n**Benefits**: <5ms connection acquisition vs 50-100ms new connection, automatic connection recycling, health checks.\n\n**Monitoring**: Track pool utilization (should stay <70%), connection wait time, query duration.\n\n**Files**: docs/architecture/00_architecture_platform.md (connection pooling lines 797-843)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "database",
      "source_document": "docs/architecture/00_architecture_platform.md",
      "keywords": [
        "connection-pooling",
        "postgresql",
        "asyncpg",
        "performance"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:connection-pooling",
        "concept:async-io"
      ],
      "code_example": "```python\nfrom asyncpg import create_pool\nimport os\n\nclass DatabasePool:\n    def __init__(self):\n        self.pool = None\n\n    async def connect(self):\n        \"\"\"Initialize connection pool\"\"\"\n        self.pool = await create_pool(\n            host=os.getenv(\"DB_HOST\"),\n            port=5432,\n            database=os.getenv(\"DB_NAME\"),\n            user=os.getenv(\"DB_USER\"),\n            password=os.getenv(\"DB_PASSWORD\"),\n\n            # Pool configuration\n            min_size=10,  # Keep warm\n            max_size=100,  # Max concurrent\n            command_timeout=60,  # Prevent hung queries\n\n            # SSL for production\n            ssl=\"require\" if os.getenv(\"ENV\") == \"production\" else None\n        )\n\n    async def execute(self, query: str, *args):\n        \"\"\"Execute query with connection from pool\"\"\"\n        async with self.pool.acquire() as conn:\n            return await conn.fetch(query, *args)\n\n    async def set_user_context(self, user_id: str):\n        \"\"\"Set RLS context for multi-tenancy\"\"\"\n        async with self.pool.acquire() as conn:\n            await conn.execute(\n                \"SET app.current_user_id = $1\",\n                user_id\n            )\n```"
    },
    {
      "atom_id": "pattern:platform-cache-strategy",
      "type": "pattern",
      "title": "Redis Caching Strategy with Namespaced TTLs",
      "summary": "Namespace-based caching (agent_spec: 1h, llm_response: 30min, marketplace: 10min, user_quota: 1min) with pattern-based invalidation",
      "content": "**Problem**: Expensive database queries repeated frequently waste resources and slow response time.\n\n**Solution**: Redis caching with namespace-based TTLs. Different data types have different staleness tolerances.\n\n**TTL Strategy**:\n- agent_spec: 3600s (1 hour) - rarely changes\n- llm_response: 1800s (30 min) - moderate staleness OK\n- marketplace_templates: 600s (10 min) - fresh for browsing\n- user_quota: 60s (1 min) - must be accurate\n\n**Key Format**: {namespace}:{key} (e.g., \"agent_spec:abc123\").\n\n**Invalidation**: Pattern-based invalidation via Redis KEYS command. Update agent → invalidate \"agent_spec:*\".\n\n**Cache Key Generation**: MD5 hash of arguments for deterministic keys.\n\n**Performance**: 80%+ cache hit rate in production, <1ms Redis latency.\n\n**Files**: docs/architecture/00_architecture_platform.md (caching lines 847-932)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "caching",
      "source_document": "docs/architecture/00_architecture_platform.md",
      "keywords": [
        "redis",
        "caching",
        "ttl",
        "performance",
        "invalidation"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:caching",
        "concept:redis"
      ],
      "code_example": "```python\nfrom redis import asyncio as aioredis\nimport json\nimport hashlib\n\nclass CacheManager:\n    def __init__(self, redis: aioredis.Redis):\n        self.redis = redis\n        self.ttls = {\n            \"agent_spec\": 3600,\n            \"llm_response\": 1800,\n            \"marketplace_templates\": 600,\n            \"user_quota\": 60\n        }\n\n    async def get(self, key: str, namespace: str = \"default\"):\n        full_key = f\"{namespace}:{key}\"\n        value = await self.redis.get(full_key)\n        return json.loads(value) if value else None\n\n    async def set(self, key: str, value, namespace: str = \"default\", ttl: int = None):\n        full_key = f\"{namespace}:{key}\"\n        ttl = ttl or self.ttls.get(namespace, 300)\n        await self.redis.setex(full_key, ttl, json.dumps(value))\n\n    async def invalidate(self, pattern: str):\n        \"\"\"Invalidate cache by pattern\"\"\"\n        keys = await self.redis.keys(pattern)\n        if keys:\n            await self.redis.delete(*keys)\n\n    def cache_key(self, *args) -> str:\n        \"\"\"Generate cache key from arguments\"\"\"\n        content = \":\".join(str(arg) for arg in args)\n        return hashlib.md5(content.encode()).hexdigest()\n\n# Usage\ncache_key = cache.cache_key(\"agent_spec\", agent_id)\ncached = await cache.get(cache_key, namespace=\"agent_spec\")\nif not cached:\n    spec = await db.execute(\"SELECT * FROM agents WHERE id = $1\", agent_id)\n    await cache.set(cache_key, spec, namespace=\"agent_spec\")\n```"
    },
    {
      "atom_id": "best-practice:archon-page-chunk-relationship",
      "type": "best-practice",
      "title": "Page + Chunk Relationship Pattern",
      "summary": "Separate tables for full pages (archon_page_metadata) and chunks (archon_crawled_pages) linked by page_id for flexible retrieval",
      "content": "**Practice**: Store full page content separately from chunks to enable both granular search and full-page retrieval.\n\n**Schema Design**:\n- archon_page_metadata: Full page content, section title, word count, URL (unique)\n- archon_crawled_pages: Chunks with embeddings, chunk_number, page_id FK\n\n**Why Split**: Search operates on chunks (better relevance), display shows full pages (better UX). Link via page_id.\n\n**Retrieval Modes**:\n- \"chunks\": Return individual chunks with scores\n- \"pages\": Group chunks by page_id, return full pages\n\n**Benefits**: Granular search without losing context, efficient storage (full page stored once), flexible retrieval.\n\n**Example**: User searches \"motor troubleshooting\" → Retrieve top-3 chunks → Group by page_id → Show full pages for context.\n\n**Files**: docs/architecture/archon_architecture_analysis.md (page metadata schema lines 289-304, pattern lines 851-853)",
      "atom_type": "best-practice",
      "vendor": "archon",
      "equipment_type": "rag-search",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "schema-design",
        "page-chunks",
        "retrieval",
        "rag"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:database-schema",
        "concept:foreign-keys"
      ],
      "code_example": "```sql\n-- Full pages\nCREATE TABLE archon_page_metadata (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    url TEXT NOT NULL UNIQUE,\n    section_title TEXT,\n    word_count INT DEFAULT 0,\n    full_content TEXT,\n    source_id UUID REFERENCES archon_sources(id),\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Chunks for search\nCREATE TABLE archon_crawled_pages (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    url TEXT NOT NULL,\n    chunk_number INT DEFAULT 0,\n    content TEXT NOT NULL,\n    embedding_1536 vector(1536),\n    page_id UUID REFERENCES archon_page_metadata(id),  -- Link to full page\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Query pattern\nasync def search_and_group_by_pages(query, match_count=5):\n    # Search chunks\n    chunks = await hybrid_search(query, match_count * 3)  # Fetch more for grouping\n\n    # Group by page_id\n    pages = {}\n    for chunk in chunks:\n        if chunk['page_id'] not in pages:\n            page = await db.execute(\"SELECT * FROM archon_page_metadata WHERE id = $1\", chunk['page_id'])\n            pages[chunk['page_id']] = page\n\n    return list(pages.values())[:match_count]\n```"
    },
    {
      "atom_id": "best-practice:archon-metadata-jsonb",
      "type": "best-practice",
      "title": "Flexible Schema Evolution with JSONB",
      "summary": "Use PostgreSQL JSONB columns for metadata to avoid migrations when adding fields - queryable with GIN indexes",
      "content": "**Practice**: Store flexible metadata in JSONB columns instead of adding columns for every new field.\n\n**Benefits**:\n- No migrations for new fields\n- Flexible schema (different sources have different metadata)\n- Queryable with GIN indexes\n- JSON-native queries (WHERE metadata->>'key' = 'value')\n\n**When To Use**: Metadata that varies by record, optional fields, experimental features.\n\n**When NOT To Use**: Required fields, foreign keys, frequently queried fields (use real columns for better performance).\n\n**Index Strategy**: GIN index on JSONB column for full-text search and key existence checks.\n\n**Example**: Source metadata (crawl_status, total_pages, strategy) varies by source type (single_page vs recursive).\n\n**Files**: docs/architecture/archon_architecture_analysis.md (JSONB pattern lines 856-858)",
      "atom_type": "best-practice",
      "vendor": "archon",
      "equipment_type": "database",
      "source_document": "docs/architecture/archon_architecture_analysis.md",
      "keywords": [
        "jsonb",
        "schema-evolution",
        "postgresql",
        "flexibility"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:postgresql",
        "concept:json"
      ],
      "code_example": "```sql\n-- Schema with JSONB metadata\nCREATE TABLE archon_sources (\n    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n    url TEXT NOT NULL,\n    domain TEXT,\n    crawl_status TEXT,  -- Required field (real column)\n    metadata JSONB DEFAULT '{}'::jsonb,  -- Flexible metadata\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- GIN index for JSONB queries\nCREATE INDEX idx_sources_metadata_gin ON archon_sources USING gin (metadata);\n\n-- Query JSONB metadata\nSELECT * FROM archon_sources\nWHERE metadata->>'strategy' = 'recursive'\n  AND metadata->>'depth' = '3';\n\n-- Add new metadata (no migration needed)\nUPDATE archon_sources\nSET metadata = jsonb_set(metadata, '{last_crawl_date}', '\"2025-12-21\"')\nWHERE id = 'abc123';\n\n-- Python usage\nmetadata = {\n    \"strategy\": \"recursive\",\n    \"depth\": 3,\n    \"custom_headers\": {\"User-Agent\": \"Mozilla/5.0\"},\n    \"exclude_patterns\": [\"*.pdf\", \"*.zip\"]\n}\nawait db.execute(\n    \"INSERT INTO archon_sources (url, metadata) VALUES ($1, $2)\",\n    url, json.dumps(metadata)\n)\n```"
    },
    {
      "atom_id": "pattern:scaffold-claude-executor",
      "type": "pattern",
      "title": "Headless Claude Code CLI Execution",
      "summary": "Execute tasks via claude-code --non-interactive with context assembly, output parsing, cost tracking, and test result detection",
      "content": "**Problem**: Need to execute autonomous coding tasks via Claude Code CLI without human interaction.\n\n**Solution**: ClaudeExecutor orchestrates headless Claude Code execution with full context assembly, result parsing, and safety monitoring.\n\n**Workflow**:\n1. Assemble context (CLAUDE.md, task spec, file tree, git commits)\n2. Invoke claude-code --non-interactive --prompt '<context>'\n3. Parse output for success indicators (commits, tests passed)\n4. Extract files changed, cost estimate, duration\n5. Return structured ExecutionResult\n\n**Success Indicators**:\n- Exit code 0\n- Commit created (git log detects SHA)\n- Tests passed (pytest output detected)\n- Success keywords (\"completed successfully\", \"implementation complete\")\n\n**Cost Estimation**: Parse output for \"Cost: $X\" pattern, fallback to $0.01 per 1000 chars.\n\n**Timeout Handling**: 3600s (1 hour) default timeout with graceful failure on TimeoutExpired.\n\n**Files**: agent_factory/scaffold/claude_executor.py (620 lines)",
      "atom_type": "pattern",
      "vendor": "scaffold",
      "equipment_type": "autonomous-execution",
      "source_document": "agent_factory/scaffold/claude_executor.py",
      "keywords": [
        "claude-code",
        "headless",
        "autonomous",
        "task-execution"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:subprocess",
        "concept:context-assembly"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.claude_executor import ClaudeExecutor\nfrom pathlib import Path\n\nexecutor = ClaudeExecutor(\n    repo_root=Path.cwd(),\n    claude_cmd=\"claude-code\",\n    timeout_sec=3600\n)\n\n# Execute task\ntask = {\n    \"id\": \"task-42\",\n    \"title\": \"Add caching to LLM router\",\n    \"description\": \"Implement LRU cache with TTL\",\n    \"acceptance_criteria\": [\n        \"Cache class implemented\",\n        \"Tests pass\",\n        \"Documentation updated\"\n    ]\n}\n\nresult = executor.execute_task(task, \"/path/to/worktree\")\n\nif result.success:\n    print(f\"✓ Task completed: {len(result.commits)} commits\")\n    print(f\"  Files changed: {result.files_changed}\")\n    print(f\"  Tests passed: {result.tests_passed}\")\n    print(f\"  Cost: ${result.cost_usd:.2f}\")\nelse:\n    print(f\"✗ Task failed: {result.error}\")\n```"
    },
    {
      "atom_id": "pattern:scaffold-pr-creator",
      "type": "pattern",
      "title": "Autonomous Draft PR Creation",
      "summary": "Automatically commit changes, push branch, create draft PRs via GitHub CLI with structured body (task ID, acceptance criteria, auto-generated metadata)",
      "content": "**Problem**: Manual PR creation is tedious and inconsistent for autonomous agents.\n\n**Solution**: PRCreator automates end-to-end PR workflow with structured commit messages and PR bodies.\n\n**Workflow**:\n1. Validate worktree has changes (git status --porcelain)\n2. Commit all changes with detailed message (task title, description, acceptance criteria)\n3. Push branch to remote (git push -u origin autonomous/task-id)\n4. Create draft PR via gh pr create --draft\n5. Extract PR URL and number from gh output\n\n**Commit Message Format**:\n```\n{task.title}\n\n{task.description}\n\nAcceptance Criteria:\n- [ ] {criterion 1}\n- [ ] {criterion 2}\n\n🤖 Generated with Claude Code\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\n```\n\n**PR Title**: Inferred from labels (fix:, feat:, docs:, test:, refactor:).\n\n**Error Handling**: Fail gracefully if no changes, push fails, or gh CLI unavailable.\n\n**Files**: agent_factory/scaffold/pr_creator.py (530 lines)",
      "atom_type": "pattern",
      "vendor": "scaffold",
      "equipment_type": "autonomous-pr",
      "source_document": "agent_factory/scaffold/pr_creator.py",
      "keywords": [
        "github",
        "pull-request",
        "automation",
        "gh-cli"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:git-workflow",
        "concept:github-cli"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.pr_creator import PRCreator\nfrom agent_factory.scaffold.models import TaskContext\nfrom pathlib import Path\n\npr_creator = PRCreator(\n    repo_root=Path.cwd(),\n    gh_cmd=\"gh\",\n    remote=\"origin\"\n)\n\ntask = TaskContext(\n    task_id=\"task-123\",\n    title=\"Fix authentication bug\",\n    description=\"Session timeout causing premature logout\",\n    acceptance_criteria=[\n        \"Bug identified and fixed\",\n        \"Tests passing\",\n        \"No regressions\"\n    ],\n    priority=\"high\",\n    labels=[\"bug\", \"auth\"]\n)\n\nresult = pr_creator.create_pr(task, \"/path/to/worktree\")\n\nif result.success:\n    print(f\"✓ PR created: {result.pr_url}\")\n    print(f\"  PR #: {result.pr_number}\")\n    print(f\"  Branch: {result.branch}\")\n    print(f\"  Commits: {result.commits_pushed}\")\nelse:\n    print(f\"✗ PR creation failed: {result.error}\")\n```"
    },
    {
      "atom_id": "pattern:scaffold-worktree-manager",
      "type": "pattern",
      "title": "Isolated Git Worktree Management",
      "summary": "Create isolated worktrees at ../agent-factory-{task-id} with metadata tracking, duplicate prevention, and max concurrent limit (default: 3)",
      "content": "**Problem**: Multiple parallel tasks need isolated environments without interfering with each other.\n\n**Solution**: WorktreeManager creates isolated git worktrees for each task with metadata tracking and safety limits.\n\n**Features**:\n- Creates worktrees at ../agent-factory-{task-id} on branch autonomous/{task-id}\n- Tracks metadata in .scaffold/worktrees.json (task_id, path, branch, created_at, status, pr_url)\n- Prevents duplicate worktrees for same task\n- Enforces max_concurrent limit (default: 3)\n- Provides cleanup with force option\n\n**Metadata Schema**:\n```python\n{\n    \"task_id\": \"task-42\",\n    \"worktree_path\": \"/path/to/agent-factory-task-42\",\n    \"branch_name\": \"autonomous/task-42\",\n    \"created_at\": \"2025-12-21T10:00:00Z\",\n    \"creator\": \"scaffold-orchestrator\",\n    \"status\": \"active\",  # active, stale, merged, abandoned\n    \"pr_url\": \"https://github.com/org/repo/pull/123\"\n}\n```\n\n**Safety**: Checks git worktree list before creating (belt-and-suspenders), validates task_id format.\n\n**Files**: agent_factory/scaffold/worktree_manager.py (450 lines)",
      "atom_type": "pattern",
      "vendor": "scaffold",
      "equipment_type": "git-workflow",
      "source_document": "agent_factory/scaffold/worktree_manager.py",
      "keywords": [
        "git-worktree",
        "isolation",
        "parallel-execution",
        "metadata"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:git-worktree",
        "concept:metadata-tracking"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.worktree_manager import WorktreeManager\nfrom pathlib import Path\n\nmanager = WorktreeManager(\n    repo_root=Path.cwd(),\n    max_concurrent=3\n)\n\n# Create worktree\ntry:\n    worktree_path = manager.create_worktree(\"task-42\", creator=\"orchestrator\")\n    print(f\"Created: {worktree_path}\")\nexcept WorktreeExistsError:\n    print(\"Worktree already exists for this task\")\nexcept WorktreeLimitError:\n    print(\"Max concurrent worktrees reached (3)\")\n\n# List active worktrees\nactive = [wt for wt in manager.list_worktrees() if wt.status == \"active\"]\nprint(f\"Active worktrees: {len(active)}\")\n\n# Update status after PR created\nmanager.update_worktree_status(\n    \"task-42\",\n    status=\"merged\",\n    pr_url=\"https://github.com/org/repo/pull/123\"\n)\n\n# Cleanup when done\nmanager.cleanup_worktree(\"task-42\", force=True, delete_branch=True)\n```"
    },
    {
      "atom_id": "pattern:scaffold-safety-monitor",
      "type": "pattern",
      "title": "Session Safety Limits (Cost, Time, Failures)",
      "summary": "Enforces hard limits on API costs ($5 max), execution time (4 hours max), and consecutive failures (3 max) to prevent runaway costs",
      "content": "**Problem**: Autonomous agents can run forever, rack up huge API bills, or get stuck in failure loops.\n\n**Solution**: SafetyMonitor enforces hard limits on cost, time, and failures with automatic session abort.\n\n**Limits** (configurable):\n- max_cost: $5.00 (prevent runaway costs)\n- max_time_hours: 4.0 (prevent infinite loops)\n- max_consecutive_failures: 3 (prevent retry storms)\n\n**Workflow**:\n1. Initialize monitor at session start\n2. Before each task: check_limits() → (allowed: bool, reason: str)\n3. After task completion: record_success(cost) or record_failure()\n4. If limits exceeded: abort session immediately\n\n**State Tracking**: total_cost, consecutive_failures, elapsed_hours (calculated from start_time).\n\n**Budget Reporting**: get_remaining_budget() shows remaining cost, time, failures.\n\n**Use Case**: Wrap SCAFFOLD orchestrator sessions to prevent disasters.\n\n**Files**: agent_factory/scaffold/safety_monitor.py (230 lines)",
      "atom_type": "pattern",
      "vendor": "scaffold",
      "equipment_type": "safety-monitoring",
      "source_document": "agent_factory/scaffold/safety_monitor.py",
      "keywords": [
        "safety",
        "cost-limits",
        "circuit-breaker",
        "monitoring"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:safety-limits",
        "concept:monitoring"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.safety_monitor import SafetyMonitor\n\nmonitor = SafetyMonitor(\n    max_cost=5.0,\n    max_time_hours=4.0,\n    max_consecutive_failures=3\n)\n\n# Before each task\nallowed, reason = monitor.check_limits()\nif not allowed:\n    print(f\"❌ Session aborted: {reason}\")\n    exit(1)\n\n# Execute task\nresult = execute_task()\n\n# Record outcome\nif result.success:\n    monitor.record_success(cost=result.cost_usd)\nelse:\n    monitor.record_failure()\n\n# Check budget\nbudget = monitor.get_remaining_budget()\nprint(f\"Remaining: ${budget['remaining_cost']:.2f}, \"\n      f\"{budget['remaining_hours']:.1f}h, \"\n      f\"{budget['remaining_failures']} failures\")\n```"
    },
    {
      "atom_id": "pattern:rivet-confidence-scoring",
      "type": "pattern",
      "title": "Multi-Factor Answer Confidence Scoring",
      "summary": "Weighted scoring (semantic similarity 40%, atom count 20%, atom quality 25%, coverage 15%) determines auto-respond vs upsell vs escalate",
      "content": "**Problem**: Need to assess answer quality and decide when to auto-respond vs escalate to human expert.\n\n**Solution**: ConfidenceScorer combines 4 factors into weighted confidence score (0.0-1.0) with action thresholds.\n\n**Factors**:\n1. Semantic Similarity (40%): Top-3 atoms' vector search scores (weighted: 50%, 30%, 20%)\n2. Atom Count (20%): More matches = better coverage (1 atom=0.5, 2=0.7, 3-5=0.9, 6+=1.0)\n3. Atom Quality (25%): Human-verified +0.2, citations +0.15, OEM source +0.15\n4. Coverage (15%): Equipment type match, fault codes match, symptoms match\n\n**Confidence Thresholds**:\n- ≥0.75: AUTO_RESPOND (high confidence)\n- 0.50-0.74: SUGGEST_UPGRADE (medium confidence, answer + upsell)\n- <0.50: REQUIRE_EXPERT (low confidence, block auto-response)\n\n**Upsell Triggers**:\n- Question limit reached → Upgrade to Pro\n- Confidence <0.60 → Suggest Pro or Expert Call\n- Urgency ≥8 or 3+ fault codes → Expert Call recommended\n\n**Files**: agent_factory/rivet_pro/confidence_scorer.py (550 lines)",
      "atom_type": "pattern",
      "vendor": "rivet-pro",
      "equipment_type": "ai-quality",
      "source_document": "agent_factory/rivet_pro/confidence_scorer.py",
      "keywords": [
        "confidence-scoring",
        "quality-assessment",
        "upsell",
        "escalation"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:semantic-similarity",
        "concept:weighted-scoring"
      ],
      "code_example": "```python\nfrom agent_factory.rivet_pro.confidence_scorer import ConfidenceScorer\n\nscorer = ConfidenceScorer()\n\nmatched_atoms = [\n    {\n        \"similarity\": 0.92,\n        \"equipment_type\": \"motor\",\n        \"human_verified\": True,\n        \"citations\": [\"https://oem-manual.com\"],\n        \"symptoms\": [\"overheating\"]\n    },\n    {\n        \"similarity\": 0.85,\n        \"equipment_type\": \"motor\",\n        \"symptoms\": [\"tripping\"]\n    }\n]\n\nquality = scorer.score_answer(\n    question=\"Motor running hot and tripping\",\n    matched_atoms=matched_atoms,\n    user_tier=\"free\",\n    questions_today=2,\n    daily_limit=5\n)\n\nprint(f\"Confidence: {quality.overall_confidence:.2f}\")\nprint(f\"Action: {quality.answer_action.value}\")\n\nif quality.should_upsell:\n    print(f\"Upsell Trigger: {quality.upsell_trigger}\")\n    print(f\"Message: {quality.upsell_message}\")\n\nif quality.is_safe_to_auto_respond:\n    # Auto-send answer to user\n    send_answer(quality.answer_text)\nelse:\n    # Escalate to human\n    escalate_to_expert(quality)\n```"
    },
    {
      "atom_id": "pattern:rivet-vps-kb-client",
      "type": "pattern",
      "title": "VPS Knowledge Base Client with Connection Pooling",
      "summary": "PostgreSQL connection pool (min=1, max=5) to VPS KB Factory (72.60.175.144) with keyword + semantic search and health monitoring",
      "content": "**Problem**: Querying remote VPS knowledge base requires connection management, caching, and failover.\n\n**Solution**: VPSKBClient provides unified interface with connection pooling, semantic search (pgvector + Ollama), and health checks.\n\n**Features**:\n- Connection pool (min=1, max=5) with 5-second timeout\n- Keyword search across title, summary, content, keywords\n- Semantic search with Ollama embeddings (nomic-embed-text) + pgvector similarity\n- Health monitoring with 1-minute cache (database connected, atom count, last ingestion, Ollama availability)\n- Automatic fallback to keyword search if embeddings fail\n\n**VPS Config** (.env):\n```\nVPS_KB_HOST=72.60.175.144\nVPS_KB_PORT=5432\nVPS_KB_USER=rivet\nVPS_KB_PASSWORD=rivet_factory_2025!\nVPS_KB_DATABASE=rivet\nVPS_OLLAMA_URL=http://72.60.175.144:11434\n```\n\n**Semantic Search**: Uses pgvector <=> operator (cosine distance) with similarity threshold (default: 0.7).\n\n**Files**: agent_factory/rivet_pro/vps_kb_client.py (460 lines)",
      "atom_type": "pattern",
      "vendor": "rivet-pro",
      "equipment_type": "kb-integration",
      "source_document": "agent_factory/rivet_pro/vps_kb_client.py",
      "keywords": [
        "vps",
        "knowledge-base",
        "connection-pool",
        "semantic-search"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:connection-pooling",
        "concept:pgvector"
      ],
      "code_example": "```python\nfrom agent_factory.rivet_pro.vps_kb_client import VPSKBClient\n\nclient = VPSKBClient()\n\n# Health check\nhealth = client.health_check()\nif health[\"status\"] == \"healthy\":\n    print(f\"✓ VPS KB: {health['atom_count']} atoms available\")\n\n# Keyword search\natoms = client.query_atoms(\"ControlLogix\", limit=5)\nfor atom in atoms:\n    print(f\"  - {atom['title']}\")\n\n# Semantic search (with Ollama embeddings)\natoms = client.query_atoms_semantic(\n    \"How to troubleshoot motor overheating\",\n    limit=5,\n    similarity_threshold=0.7\n)\nfor atom in atoms:\n    print(f\"  [{atom['similarity']:.2f}] {atom['title']}\")\n\n# Equipment-specific search\natoms = client.search_by_equipment(\n    equipment_type=\"plc\",\n    manufacturer=\"allen_bradley\",\n    limit=5\n)\n\nclient.close()\n```"
    },
    {
      "atom_id": "pattern:llm-response-cache",
      "type": "pattern",
      "title": "LRU Cache with TTL for LLM Responses",
      "summary": "OrderedDict-based LRU cache (max 1000 entries, 1 hour TTL) with SHA256 key generation from messages + config, tracks hit/miss rate",
      "content": "**Problem**: Identical LLM prompts waste API costs and add latency.\n\n**Solution**: ResponseCache implements LRU eviction with TTL expiration using Python OrderedDict.\n\n**Features**:\n- LRU eviction when max_size reached (default: 1000 entries)\n- TTL expiration (default: 3600s = 1 hour)\n- Deterministic cache keys (SHA256 of messages + model + temperature)\n- Hit/miss rate tracking\n- Thread-safe operations\n\n**Key Generation**: MD5/SHA256 hash of JSON-serialized messages + config for deterministic keys.\n\n**LRU Mechanism**: OrderedDict.move_to_end() marks entries as recently used, popitem(last=False) removes oldest.\n\n**Expected Performance**: 30-40% cache hit rate in production (saves $100-200/month with 50+ agents).\n\n**Use Case**: Wrap all LLM completion calls to avoid redundant API requests.\n\n**Files**: agent_factory/llm/cache.py (180 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "llm-optimization",
      "source_document": "agent_factory/llm/cache.py",
      "keywords": [
        "caching",
        "lru",
        "ttl",
        "cost-optimization"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:caching",
        "concept:lru"
      ],
      "code_example": "```python\nfrom agent_factory.llm.cache import ResponseCache\n\ncache = ResponseCache(max_size=1000, ttl_seconds=3600)\n\nmessages = [{\"role\": \"user\", \"content\": \"Hello\"}]\nconfig = type('Config', (), {'model': 'gpt-4o-mini', 'temperature': 0.7})()\n\n# Check cache\ncached = cache.get(messages, config)\nif cached:\n    print(\"Cache hit!\")\n    return cached\n\n# Cache miss - call API\nresponse = llm_api.complete(messages, config)\n\n# Store in cache\ncache.set(messages, config, response)\n\n# Stats\nstats = cache.get_stats()\nprint(f\"Hit rate: {stats['hit_rate']:.1%}\")\nprint(f\"Size: {stats['size']}/{stats['max_size']}\")\n```"
    },
    {
      "atom_id": "pattern:llm-streaming",
      "type": "pattern",
      "title": "Token-by-Token LLM Response Streaming",
      "summary": "Iterator-based streaming with StreamChunk (text, is_final, metadata) for real-time token delivery from LiteLLM raw streams",
      "content": "**Problem**: Users wait for full LLM response before seeing any output (poor UX).\n\n**Solution**: stream_complete() wraps LiteLLM raw streams into iterator of StreamChunk objects for token-by-token delivery.\n\n**StreamChunk Schema**:\n```python\n@dataclass\nclass StreamChunk:\n    text: str  # Token content\n    is_final: bool  # Last chunk indicator\n    metadata: Dict  # provider, model, chunk_index, finish_reason\n```\n\n**Usage Pattern**:\n1. Call litellm.completion(stream=True)\n2. Pass raw stream to stream_complete()\n3. Iterate StreamChunk objects\n4. Stop when is_final=True\n\n**Utility**: collect_stream() accumulates all chunks into full text (for testing or fallback).\n\n**Error Handling**: Yields error chunk with metadata on exception (graceful degradation).\n\n**Supported Providers**: OpenAI, Anthropic, Google Gemini, Ollama (all via LiteLLM).\n\n**Files**: agent_factory/llm/streaming.py (160 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "llm-streaming",
      "source_document": "agent_factory/llm/streaming.py",
      "keywords": [
        "streaming",
        "real-time",
        "tokens",
        "ux"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:iterators",
        "concept:streaming"
      ],
      "code_example": "```python\nfrom agent_factory.llm.streaming import stream_complete, collect_stream\nimport litellm\n\n# Get streaming response\nraw_stream = litellm.completion(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a story\"}],\n    stream=True\n)\n\n# Stream tokens\nfor chunk in stream_complete(raw_stream, \"openai\", \"gpt-4o-mini\"):\n    print(chunk.text, end=\"\", flush=True)\n\n    if chunk.is_final:\n        print(f\"\\n\\nDone! Finish reason: {chunk.metadata['finish_reason']}\")\n\n# Alternative: collect all chunks\nchunks = stream_complete(raw_stream, \"openai\", \"gpt-4o-mini\")\nfull_text = collect_stream(chunks)\nprint(full_text)\n```"
    },
    {
      "atom_id": "pattern:sme-agent-template",
      "type": "pattern",
      "title": "SME Agent Template for Domain Experts",
      "summary": "Abstract base class for subject matter experts with 4 abstract methods (analyze_query, search_kb, generate_answer, score_confidence) and built-in orchestration",
      "content": "**Problem**: Each SME agent reimplemented same boilerplate (query parsing, KB search, answer generation, escalation).\n\n**Solution**: SMEAgentTemplate provides abstract base class with built-in orchestration and 4 customization hooks.\n\n**Abstract Methods** (must implement):\n1. analyze_query(query) → QueryAnalysis (domain, question_type, entities, keywords, complexity)\n2. search_kb(analysis) → List[Dict] (retrieve top-k relevant documents)\n3. generate_answer(query, docs) → str (generate domain-specific answer)\n4. score_confidence(query, answer, docs) → float (0.0-1.0 confidence score)\n\n**Built-In Methods** (template provides):\n- answer(query) → SMEAnswer (main entry point, orchestrates pipeline)\n- _generate_follow_ups(query, analysis) → List[str] (default implementation)\n\n**Data Classes**:\n- QueryAnalysis: Structured query parsing output\n- SMEAnswer: Complete answer with confidence, sources, escalation flag, follow-ups\n\n**Escalation Logic**: If confidence < min_confidence (default: 0.7), sets escalate=True and flags for human.\n\n**Use Case**: Build new SME agents in 50 lines instead of 200 lines.\n\n**Files**: docs/patterns/SME_AGENT_PATTERN.md (365 lines)",
      "atom_type": "pattern",
      "vendor": "agent-factory",
      "equipment_type": "agent-template",
      "source_document": "docs/patterns/SME_AGENT_PATTERN.md",
      "keywords": [
        "template",
        "sme-agent",
        "abstraction",
        "reusability"
      ],
      "difficulty": "advanced",
      "prereqs": [
        "concept:abstract-base-class",
        "concept:template-method"
      ],
      "code_example": "```python\nfrom agent_factory.templates import SMEAgentTemplate\nfrom agent_factory.templates.sme_agent_template import QueryAnalysis, SMEAnswer\n\nclass MotorControlSME(SMEAgentTemplate):\n    def __init__(self):\n        super().__init__(\n            name=\"Motor Control SME\",\n            domain=\"motor_control\",\n            min_confidence=0.7,\n            max_docs=10\n        )\n\n    def analyze_query(self, query: str) -> QueryAnalysis:\n        # Extract motor-specific entities\n        keywords = extract_keywords(query)\n        return QueryAnalysis(\n            domain=\"motor_control\",\n            question_type=\"troubleshooting\" if \"why\" in query else \"how_to\",\n            key_entities=[\"motor\"],\n            search_keywords=keywords,\n            complexity=\"moderate\"\n        )\n\n    def search_kb(self, analysis: QueryAnalysis) -> List[Dict]:\n        # Search motor KB with reranking\n        from agent_factory.rivet_pro.rag import search_docs\n        return search_docs(analysis.search_keywords)\n\n    def generate_answer(self, query: str, docs: List[Dict]) -> str:\n        # Generate answer using LLM\n        context = \"\\n\".join(d[\"content\"] for d in docs[:3])\n        return llm.complete(f\"Answer: {query}\\n\\nContext: {context}\")\n\n    def score_confidence(self, query: str, answer: str, docs: List[Dict]) -> float:\n        # Multi-factor scoring\n        similarity = sum(d[\"similarity\"] for d in docs) / len(docs)\n        length_score = min(len(answer) / 200, 1.0)\n        return (similarity * 0.6 + length_score * 0.4)\n\n# Use agent\nsme = MotorControlSME()\nresult = sme.answer(\"Why is my motor overheating?\")\n\nif not result.escalate:\n    print(result.answer_text)\nelse:\n    print(\"Escalating to human expert\")\n```"
    },
    {
      "atom_id": "pattern:scaffold-context-assembly",
      "type": "pattern",
      "title": "Rich Context Assembly for Claude Code CLI",
      "summary": "Assemble execution context from CLAUDE.md system prompts, file tree (max depth 3), recent commits (last 10), and structured task spec",
      "content": "**Problem**: Claude Code CLI needs comprehensive context to execute tasks correctly.\n\n**Solution**: ContextAssembler combines system prompts, repo snapshot, and task spec into unified execution context.\n\n**Components**:\n1. **System Prompt**: First 200 lines from CLAUDE.md (core instructions)\n2. **File Tree**: Directory structure (max depth 3, excludes node_modules, __pycache__, .git)\n3. **Git History**: Last 10 commits (git log --oneline --decorate)\n4. **Task Spec**: Formatted task (title, description, acceptance criteria)\n\n**Template Format**:\n```markdown\n# SCAFFOLD Task Execution Context\n\n## System Prompt\n{CLAUDE.md content}\n\n## Repository Snapshot\n### File Tree\n{tree output}\n\n### Recent Commits\n{git log output}\n\n## Task Specification\n{formatted task}\n\n## Execution Environment\n- Worktree Path: {path}\n- Task ID: {id}\n\n## Instructions\nExecute the task according to the acceptance criteria...\n```\n\n**Fallback**: If assembly fails, returns minimal prompt (prevents total failure).\n\n**Files**: agent_factory/scaffold/context_assembler.py (280 lines)",
      "atom_type": "pattern",
      "vendor": "scaffold",
      "equipment_type": "context-management",
      "source_document": "agent_factory/scaffold/context_assembler.py",
      "keywords": [
        "context",
        "prompt-assembly",
        "claude-code",
        "execution"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:context-engineering",
        "concept:prompt-templates"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.context_assembler import ContextAssembler\nfrom pathlib import Path\n\nassembler = ContextAssembler(\n    repo_root=Path.cwd(),\n    max_tree_depth=3,\n    max_commits=10\n)\n\ntask = {\n    \"id\": \"task-42\",\n    \"title\": \"Add caching\",\n    \"description\": \"Implement LRU cache with TTL\",\n    \"acceptance_criteria\": [\n        \"Cache class implemented\",\n        \"Tests pass\"\n    ]\n}\n\n# Assemble full context\ncontext = assembler.assemble_context(task, \"/path/to/worktree\")\n\n# Context is ready for Claude Code CLI\nprint(f\"Context size: {len(context)} chars\")\n\n# Pass to executor\nexecutor.execute(context, worktree_path)\n```"
    },
    {
      "atom_id": "pattern:cross-repo-config-management",
      "type": "pattern",
      "title": "Multi-Repository Configuration Patterns",
      "summary": "Agent-Factory (database-backed), Backlog.md (YAML), pai-config (JSON) all use environment fallback + structured config files",
      "content": "**Problem**: All 3 core repos (Agent-Factory, Backlog.md, pai-config-windows) need runtime configuration without code changes.\n\n**Shared Principle**: Environment variables fallback + structured config files.\n\n**Agent-Factory** (database-backed):\n```python\nfrom agent_factory.core.settings_service import settings\nmodel = settings.get(\"DEFAULT_MODEL\", category=\"llm\")\n# Falls back to .env if database unavailable\n```\n\n**Backlog.md** (YAML):\n```yaml\n# backlog/config.yaml\nbacklog_dir: \"./backlog\"\nmilestones:\n  - name: \"Week 2\"\n    description: \"LLM enhancements\"\n```\n\n**pai-config-windows** (JSON):\n```json\n{\n  \"hooks\": {\n    \"onToolUse\": \"hooks/capture-all-events.ts\"\n  },\n  \"context\": {\n    \"checkpointInterval\": 300\n  }\n}\n```\n\n**Why Different Formats**: Different ecosystems (Python, Node, TypeScript) favor different formats, but all share environment fallback pattern.\n\n**Files**: docs/patterns/CROSS_REPO_INTEGRATION.md (lines 71-108)",
      "atom_type": "pattern",
      "vendor": "cross-repo",
      "equipment_type": "configuration",
      "source_document": "docs/patterns/CROSS_REPO_INTEGRATION.md",
      "keywords": [
        "configuration",
        "multi-repo",
        "environment-variables",
        "fallback"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:environment-variables",
        "concept:config-files"
      ],
      "code_example": "```python\n# Agent-Factory pattern\nfrom agent_factory.core.settings_service import settings\nimport os\n\n# Try database first, fallback to environment\nmodel = settings.get(\"DEFAULT_MODEL\", category=\"llm\")\nif not model:\n    model = os.getenv(\"DEFAULT_MODEL\", \"gpt-4o-mini\")\n\n# Backlog.md pattern (Node)\nimport yaml\nwith open(\"backlog/config.yaml\") as f:\n    config = yaml.safe_load(f)\n    backlog_dir = config.get(\"backlog_dir\", process.env.BACKLOG_DIR || \"./backlog\")\n\n# pai-config pattern (TypeScript)\nimport settings from \"./settings.json\"\nconst checkpointInterval = settings.context?.checkpointInterval ??\n                          parseInt(process.env.CHECKPOINT_INTERVAL || \"300\")\n```"
    },
    {
      "atom_id": "pattern:cross-repo-event-driven",
      "type": "pattern",
      "title": "Event-Driven Architecture Across Repos",
      "summary": "Agent-Factory (callbacks), Backlog.md (MCP events), pai-config (hooks) all use typed event payloads with registration pattern",
      "content": "**Problem**: Need to react to lifecycle events (session start, task complete, error) across different systems.\n\n**Shared Principle**: Hook/callback registration with typed event payloads.\n\n**Agent-Factory** (callbacks):\n```python\nfrom agent_factory.core.callbacks import on_agent_complete\n\n@on_agent_complete\ndef handle_completion(agent_name, result):\n    print(f\"{agent_name} completed: {result}\")\n```\n\n**Backlog.md** (MCP protocol events):\n```javascript\ntask.on('status_change', (task_id, old_status, new_status) => {\n  console.log(`Task ${task_id}: ${old_status} → ${new_status}`)\n})\n```\n\n**pai-config-windows** (hook system):\n```typescript\nexport async function onTaskComplete(context: HookContext) {\n  await saveCheckpoint(context)\n  await notifyUser(context.task_id)\n}\n```\n\n**Common Pattern**: Register handler functions that receive typed context/payload objects.\n\n**Use Case**: Cross-system observability, checkpoint synchronization, user notifications.\n\n**Files**: docs/patterns/CROSS_REPO_INTEGRATION.md (lines 111-145)",
      "atom_type": "pattern",
      "vendor": "cross-repo",
      "equipment_type": "event-driven",
      "source_document": "docs/patterns/CROSS_REPO_INTEGRATION.md",
      "keywords": [
        "events",
        "callbacks",
        "hooks",
        "observability"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:event-driven",
        "concept:callbacks"
      ],
      "code_example": "```python\n# Agent-Factory callback pattern\nfrom typing import Callable, Dict, Any\n\ncallbacks = {}\n\ndef register_callback(event: str, func: Callable):\n    if event not in callbacks:\n        callbacks[event] = []\n    callbacks[event].append(func)\n\ndef trigger_event(event: str, payload: Dict[str, Any]):\n    for func in callbacks.get(event, []):\n        func(payload)\n\n# Register handlers\n@register_callback(\"task_complete\")\ndef log_completion(payload):\n    print(f\"Task {payload['task_id']} completed\")\n\n@register_callback(\"task_complete\")\ndef save_metrics(payload):\n    metrics_db.save(payload)\n\n# Trigger event\ntrigger_event(\"task_complete\", {\n    \"task_id\": \"task-42\",\n    \"duration_sec\": 120.5,\n    \"cost_usd\": 0.023\n})\n```"
    },
    {
      "atom_id": "best-practice:scaffold-task-routing",
      "type": "best-practice",
      "title": "Label-Based Task Routing",
      "summary": "Route tasks by labels: 'user-action' → ManualActionHandler, default → ClaudeCodeHandler for flexible handler registration",
      "content": "**Practice**: Use task labels to route to appropriate execution handlers instead of hardcoding logic.\n\n**Routing Logic**:\n- Label \"user-action\" → ManualActionHandler (flags for human intervention)\n- Default → ClaudeCodeHandler (autonomous execution)\n\n**Benefits**:\n- Declarative routing (labels define behavior)\n- Easy to add new handlers (register in handler dict)\n- Tasks self-describe execution requirements\n\n**Handler Interface**:\n```python\nclass Handler:\n    def execute(self, task: Dict, worktree_path: str, timeout_sec: int) -> Dict:\n        # Returns: {success, output, cost, duration_sec, files_changed}\n        pass\n```\n\n**Example Labels**:\n- \"user-action\": Requires manual execution (API keys, cloud signup)\n- \"autonomous\": Standard ClaudeCode execution\n- \"review-required\": Execute but flag for human review\n\n**Files**: agent_factory/scaffold/task_router.py (300 lines)",
      "atom_type": "best-practice",
      "vendor": "scaffold",
      "equipment_type": "task-routing",
      "source_document": "agent_factory/scaffold/task_router.py",
      "keywords": [
        "routing",
        "labels",
        "handlers",
        "extensibility"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:routing",
        "concept:labels"
      ],
      "code_example": "```python\nfrom agent_factory.scaffold.task_router import TaskRouter\n\nrouter = TaskRouter()\n\n# Add custom handler\nclass CustomHandler:\n    def execute(self, task, worktree_path, timeout_sec):\n        # Custom execution logic\n        return {\"success\": True, \"output\": \"Custom handled\"}\n\nrouter.handlers[\"custom\"] = CustomHandler()\n\n# Route task\ntask = {\n    \"id\": \"task-42\",\n    \"labels\": [\"user-action\"]  # Routes to ManualActionHandler\n}\n\nhandler_name = router.route(task)  # \"manual\"\nhandler = router.get_handler(handler_name)\nresult = handler.execute(task, \"/path/to/worktree\")\n```"
    },
    {
      "atom_id": "best-practice:rivet-upsell-triggers",
      "type": "best-practice",
      "title": "Non-Intrusive Upsell Trigger Logic",
      "summary": "Trigger upsells based on context: question limit (hard block), low confidence (suggest upgrade), urgency ≥8 (expert call)",
      "content": "**Practice**: Only trigger upsells when genuinely beneficial to user, not annoyingly frequent.\n\n**Trigger Categories**:\n1. **Question Limit** (FREE tier, 5/5 used): Hard block with upgrade prompt\n2. **Low Confidence** (<0.60): Suggest Pro or Expert Call (still provide answer)\n3. **Complex Issue** (urgency ≥8 or 3+ fault codes): Recommend Expert Call\n4. **Near Limit** (FREE tier, 4/5 used): Soft reminder about unlimited questions\n\n**Upsell Messages**:\n- **Question Limit**: \"🚫 Daily limit reached. Upgrade to Pro for unlimited questions ($29/month)\"\n- **Low Confidence**: \"💡 Partial match found. Consider Pro ($29/mo) or Expert Call ($75/hr)\"\n- **Complex Issue**: \"🚨 Critical issue detected. Expert Call recommended ($75/hr)\"\n\n**Revenue Optimization**: Maximize conversions without annoying users (trigger only when helpful).\n\n**Files**: agent_factory/rivet_pro/confidence_scorer.py (upsell logic lines 275-360)",
      "atom_type": "best-practice",
      "vendor": "rivet-pro",
      "equipment_type": "monetization",
      "source_document": "agent_factory/rivet_pro/confidence_scorer.py",
      "keywords": [
        "upsell",
        "monetization",
        "user-experience",
        "revenue"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:subscription-tiers",
        "concept:revenue-optimization"
      ],
      "code_example": "```python\ndef _determine_upsell(self, overall_confidence, user_tier, questions_today, daily_limit, intent_data):\n    # Pro/Enterprise: Never upsell (already paying)\n    if user_tier in [\"pro\", \"enterprise\"]:\n        return {\"should_upsell\": False}\n\n    # Trigger 1: Question limit (hard block)\n    if user_tier == \"free\" and questions_today >= daily_limit:\n        return {\n            \"should_upsell\": True,\n            \"trigger\": \"question_limit\",\n            \"message\": \"🚫 Daily limit reached. Upgrade to Pro ($29/mo)\",\n            \"suggested_tier\": \"pro\"\n        }\n\n    # Trigger 2: Low confidence\n    if overall_confidence < 0.60:\n        if overall_confidence < 0.40:\n            # Very low: Expert call\n            return {\n                \"should_upsell\": True,\n                \"trigger\": \"very_low_confidence\",\n                \"message\": \"⚠️ Complex issue. Expert Call recommended ($75/hr)\",\n                \"suggested_tier\": \"premium_call\"\n            }\n        else:\n            # Medium-low: Pro upgrade\n            return {\n                \"should_upsell\": True,\n                \"trigger\": \"low_confidence\",\n                \"message\": \"💡 Partial match. Pro tier ($29/mo) for better answers\",\n                \"suggested_tier\": \"pro\"\n            }\n\n    # No upsell\n    return {\"should_upsell\": False}\n```"
    },
    {
      "atom_id": "best-practice:llm-semantic-fallback",
      "type": "best-practice",
      "title": "Semantic Search with Keyword Fallback",
      "summary": "Try semantic search (pgvector + Ollama) first, fallback to keyword search if embeddings fail - graceful degradation",
      "content": "**Practice**: Always provide fallback from fancy tech (embeddings) to simple tech (keyword search) for reliability.\n\n**Workflow**:\n1. Try semantic search (Ollama embeddings + pgvector similarity)\n2. If Ollama unavailable or embedding fails → Fallback to keyword search\n3. Log fallback event for monitoring\n\n**Why Fallback Matters**: Ollama might be down, embedding model might be loading, network issues - don't fail completely.\n\n**Performance Trade-Off**: Semantic search more accurate (85-90% relevance), keyword search acceptable (70-75% relevance).\n\n**Monitoring**: Track fallback rate (should be <5% in healthy system).\n\n**Example Failure Scenarios**:\n- Ollama server down (502 Bad Gateway)\n- Embedding model not loaded (404 Model Not Found)\n- Network timeout (10s timeout)\n\n**Files**: agent_factory/rivet_pro/vps_kb_client.py (semantic search lines 315-385)",
      "atom_type": "best-practice",
      "vendor": "rivet-pro",
      "equipment_type": "search-reliability",
      "source_document": "agent_factory/rivet_pro/vps_kb_client.py",
      "keywords": [
        "fallback",
        "reliability",
        "semantic-search",
        "degradation"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:fallback-pattern",
        "concept:graceful-degradation"
      ],
      "code_example": "```python\ndef query_atoms_semantic(self, query_text, limit=5, similarity_threshold=0.7):\n    try:\n        # Try semantic search\n        response = requests.post(\n            f\"{self.ollama_url}/api/embeddings\",\n            json={\"model\": \"nomic-embed-text\", \"prompt\": query_text},\n            timeout=10\n        )\n\n        if response.status_code != 200:\n            logger.warning(f\"Ollama embedding failed: {response.status_code}\")\n            return self.query_atoms(query_text, limit)  # FALLBACK\n\n        embedding = response.json()[\"embedding\"]\n\n        # pgvector similarity search\n        atoms = db.execute(\n            \"\"\"\n            SELECT *, 1 - (embedding <=> %s::vector) as similarity\n            FROM knowledge_atoms\n            WHERE 1 - (embedding <=> %s::vector) >= %s\n            ORDER BY similarity DESC\n            LIMIT %s\n            \"\"\",\n            (embedding, embedding, similarity_threshold, limit)\n        )\n\n        return atoms\n\n    except Exception as e:\n        logger.error(f\"Semantic search failed: {e}\")\n        logger.info(\"Falling back to keyword search\")\n        return self.query_atoms(query_text, limit)  # FALLBACK\n```"
    },
    {
      "atom_id": "best-practice:scaffold-success-detection",
      "type": "best-practice",
      "title": "Multi-Signal Task Success Detection",
      "summary": "Combine exit code, commit detection, test results, and output keywords to reliably determine task success",
      "content": "**Practice**: Use multiple success indicators instead of relying on single signal (exit code alone insufficient).\n\n**Success Signals** (any true → success):\n1. **Exit code 0** (basic indicator)\n2. **Commit created** (git log detects SHA)\n3. **Tests passed** (pytest output parsing)\n4. **Success keywords** (\"completed successfully\", \"implementation complete\", \"N files changed\")\n\n**Why Multiple Signals**: Claude Code might:\n- Exit 0 without committing (dry run)\n- Create commit but tests fail\n- Mention success in output but exit non-zero\n\n**Test Detection Patterns**:\n- \"5 passed in 2.5s\" (pytest)\n- \"All tests passed\"\n- \"OK (10 tests)\" (unittest)\n\n**Failure Patterns**:\n- \"1 failed\" or \"FAILED tests/...\"\n- \"ERROR:\" in output\n- Tests detected but no success keywords\n\n**Files**: agent_factory/scaffold/claude_executor.py (success detection lines 180-220)",
      "atom_type": "best-practice",
      "vendor": "scaffold",
      "equipment_type": "success-detection",
      "source_document": "agent_factory/scaffold/claude_executor.py",
      "keywords": [
        "success-detection",
        "testing",
        "reliability",
        "signals"
      ],
      "difficulty": "moderate",
      "prereqs": [
        "concept:exit-codes",
        "concept:pattern-matching"
      ],
      "code_example": "```python\ndef _is_successful(self, output, exit_code, commits, tests_passed):\n    # Exit code must be 0\n    if exit_code != 0:\n        return False\n\n    # If tests were run and failed, not successful\n    if tests_passed is False:\n        return False\n\n    # Strong indicator: commit created\n    if commits and len(commits) > 0:\n        return True\n\n    # Look for success keywords\n    success_patterns = [\n        r\"completed successfully\",\n        r\"all tests? passed\",\n        r\"implementation complete\",\n        r\"task complete\",\n        r\"\\d+ files? changed\"\n    ]\n\n    for pattern in success_patterns:\n        if re.search(pattern, output, re.IGNORECASE):\n            return True\n\n    # Exit code 0 but no other indicators → still success\n    return True\n```"
    },
    {
      "atom_id": "best-practice:rivet-health-monitoring",
      "type": "best-practice",
      "title": "Cached Health Checks with Auto-Refresh",
      "summary": "Cache health check results (1 minute TTL) to avoid hammering VPS, include database + Ollama + response time in status",
      "content": "**Practice**: Cache health check results to avoid excessive monitoring overhead while maintaining visibility.\n\n**Health Check Components**:\n1. **Database**: Connection test, atom count, last ingestion timestamp\n2. **Ollama**: API availability check (GET /api/tags)\n3. **Response Time**: Latency measurement for SLA monitoring\n\n**Cache Strategy**:\n- TTL: 60 seconds (1 minute)\n- Auto-refresh on cache miss\n- Return cached result if <60s old\n\n**Status Levels**:\n- \"healthy\": Database connected, atoms available, Ollama up\n- \"degraded\": Database connected but no atoms or Ollama down\n- \"down\": Database connection failed\n\n**Why Cache**: Health checks every request would add 50-100ms latency + hammer VPS.\n\n**Monitoring Output**:\n```json\n{\n  \"status\": \"healthy\",\n  \"database_connected\": true,\n  \"atom_count\": 1247,\n  \"last_ingestion\": \"2025-12-21T10:00:00Z\",\n  \"ollama_available\": true,\n  \"response_time_ms\": 45\n}\n```\n\n**Files**: agent_factory/rivet_pro/vps_kb_client.py (health check lines 80-165)",
      "atom_type": "best-practice",
      "vendor": "rivet-pro",
      "equipment_type": "monitoring",
      "source_document": "agent_factory/rivet_pro/vps_kb_client.py",
      "keywords": [
        "health-check",
        "caching",
        "monitoring",
        "performance"
      ],
      "difficulty": "beginner",
      "prereqs": [
        "concept:health-checks",
        "concept:caching"
      ],
      "code_example": "```python\nclass VPSKBClient:\n    def __init__(self):\n        self._last_health_check = None\n        self._health_status = None\n        self._health_cache_duration = timedelta(minutes=1)\n\n    def health_check(self):\n        # Return cached if fresh\n        if self._last_health_check and self._health_status:\n            elapsed = datetime.now() - self._last_health_check\n            if elapsed < self._health_cache_duration:\n                return self._health_status\n\n        # Fresh check\n        health = {\n            \"status\": \"unknown\",\n            \"database_connected\": False,\n            \"atom_count\": 0,\n            \"ollama_available\": False,\n            \"response_time_ms\": 0\n        }\n\n        start_time = datetime.now()\n\n        # Test database\n        try:\n            count = db.execute(\"SELECT COUNT(*) FROM knowledge_atoms\")[0]\n            health[\"atom_count\"] = count\n            health[\"database_connected\"] = True\n        except:\n            health[\"database_connected\"] = False\n\n        # Test Ollama\n        try:\n            response = requests.get(f\"{ollama_url}/api/tags\", timeout=5)\n            health[\"ollama_available\"] = response.status_code == 200\n        except:\n            health[\"ollama_available\"] = False\n\n        # Response time\n        health[\"response_time_ms\"] = int((datetime.now() - start_time).total_seconds() * 1000)\n\n        # Status\n        if health[\"database_connected\"] and health[\"atom_count\"] > 0:\n            health[\"status\"] = \"healthy\"\n        elif health[\"database_connected\"]:\n            health[\"status\"] = \"degraded\"\n        else:\n            health[\"status\"] = \"down\"\n\n        # Cache\n        self._last_health_check = datetime.now()\n        self._health_status = health\n\n        return health\n```"
    }
  ]
}