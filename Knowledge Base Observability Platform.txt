Knowledge Base Observability Platform - Implementation Plan

 ‚úÖ Current Status (Updated 2025-12-25)

 Phase 1: Database Schema - COMPLETE ‚úÖ

 Completed:
 - ‚úÖ Created docs/database/observability_migration.sql (13,423 bytes)
 - ‚úÖ Executed migration on VPS PostgreSQL (72.60.175.144:5432)
   - Tables: ingestion_metrics_realtime, ingestion_metrics_hourly, ingestion_metrics_daily
   - Indexes: 13 created (completed_at, status, vendor, etc.)
   - Function: aggregate_hourly_metrics() created
 - ‚úÖ Verified tables with test insert/query
 - ‚úÖ Created directory structure:
   - agent_factory/observability/__init__.py
   - agent_factory/observability/dashboard/__init__.py

 Migration Script: scripts/run_observability_migration.py (connects to VPS PostgreSQL)

 Database: VPS PostgreSQL (NOT Supabase - Neon/Supabase had DNS/connectivity issues)
 - Host: 72.60.175.144
 - Database: rivet
 - User: rivet
 - Credentials in .env (VPS_KB_*)

 Phase 2: Core Monitoring - IN PROGRESS üîÑ

 Next Steps:
 1. Implement IngestionMonitor class (ingestion_monitor.py)
 2. Implement TelegramNotifier class (telegram_notifier.py)
 3. Hook into ingestion pipeline (modify ingestion_chain.py)

 Blockers Resolved:
 - ‚ùå Supabase DNS issues ‚Üí ‚úÖ Using VPS PostgreSQL instead
 - ‚ùå Windows heredoc issues ‚Üí ‚úÖ Use Python scripts or Edit tool for file creation

 Lessons Learned from Phase 1:

 1. Database Provider Failover: Both Neon and Supabase were unreachable from local machine (DNS/connection issues). VPS PostgreSQL (72.60.175.144) worked
 perfectly. All future implementations should target VPS database.
 2. Windows File Creation: Bash heredoc with complex Python code fails on Windows due to quote escaping. Solutions:
   - Option A: Create Python script that writes the file (e.g., scripts/create_ingestion_monitor.py)
   - Option B: Create minimal stub file, then use Edit tool to expand
   - Option C: Use PowerShell Out-File (less preferred)
 3. Migration Execution: Direct psycopg connection is simpler than DatabaseManager for one-off scripts. DatabaseManager is better for application code with
  connection pooling.
 4. Test-Driven Approach: Quick validation (INSERT/SELECT) after migration confirms schema works before building classes.

 ---
 Executive Summary

 Build a production-ready observability platform for the 7-stage KB ingestion pipeline with:

 - Real-time Telegram notifications (VERBOSE mode - every source processed)
 - Telegram dashboard commands (/stats, /kb_status, /ingestion_live)
 - Web dashboard (Gradio with live charts, auto-refresh)
 - Historical metrics (hourly/daily aggregations for trend analysis)
 - Pipeline performance tracking (identify bottlenecks across 7 stages)
 - Vendor/equipment coverage analysis

 Key Design Decisions:

 1. Hook via LangGraph stream() API - Minimal code changes, yields stage-by-stage updates
 2. Async background writes - <5ms overhead per stage, non-blocking DB inserts
 3. Batch notifications - 5-minute batches to prevent Telegram spam (user configurable)
 4. Materialized views - Fast dashboard queries even with 100k+ records

 ---
 1. Database Schema

 New Tables (Create via SQL migration)

 File: docs/database/observability_migration.sql

 -- Real-time ingestion metrics
 CREATE TABLE IF NOT EXISTS ingestion_metrics_realtime (
     id BIGSERIAL PRIMARY KEY,

     -- Source identification
     source_url TEXT NOT NULL,
     source_type VARCHAR(20) NOT NULL,
     source_hash VARCHAR(64) NOT NULL,

     -- Results
     status VARCHAR(20) NOT NULL CHECK (status IN ('success', 'partial', 'failed')),
     atoms_created INTEGER DEFAULT 0,
     atoms_failed INTEGER DEFAULT 0,
     chunks_processed INTEGER DEFAULT 0,

     -- Quality
     avg_quality_score FLOAT,
     quality_pass_rate FLOAT,

     -- Stage timings (milliseconds)
     stage_1_acquisition_ms INTEGER,
     stage_2_extraction_ms INTEGER,
     stage_3_chunking_ms INTEGER,
     stage_4_generation_ms INTEGER,
     stage_5_validation_ms INTEGER,
     stage_6_embedding_ms INTEGER,
     stage_7_storage_ms INTEGER,
     total_duration_ms INTEGER,

     -- Errors
     error_stage VARCHAR(50),
     error_message TEXT,

     -- Metadata
     vendor TEXT,
     equipment_type TEXT,

     -- Timestamps
     started_at TIMESTAMPTZ NOT NULL,
     completed_at TIMESTAMPTZ
 );

 CREATE INDEX idx_realtime_completed_at ON ingestion_metrics_realtime(completed_at DESC);
 CREATE INDEX idx_realtime_status ON ingestion_metrics_realtime(status);
 CREATE INDEX idx_realtime_vendor ON ingestion_metrics_realtime(vendor);

 -- Hourly aggregations
 CREATE TABLE IF NOT EXISTS ingestion_metrics_hourly (
     id BIGSERIAL PRIMARY KEY,
     hour_start TIMESTAMPTZ NOT NULL UNIQUE,

     -- Throughput
     sources_processed INTEGER DEFAULT 0,
     atoms_created INTEGER DEFAULT 0,
     atoms_failed INTEGER DEFAULT 0,
     success_rate FLOAT,

     -- Quality
     avg_quality_score FLOAT,
     quality_pass_rate FLOAT,

     -- Performance
     avg_total_duration_ms INTEGER,
     p95_duration_ms INTEGER,

     -- Stage bottlenecks (avg ms per stage)
     avg_stage_1_ms INTEGER,
     avg_stage_2_ms INTEGER,
     avg_stage_3_ms INTEGER,
     avg_stage_4_ms INTEGER,
     avg_stage_5_ms INTEGER,
     avg_stage_6_ms INTEGER,
     avg_stage_7_ms INTEGER,

     -- Coverage (JSONB for flexibility)
     vendor_counts JSONB,
     equipment_counts JSONB,

     -- Errors
     failed_sources INTEGER DEFAULT 0,
     error_distribution JSONB,

     created_at TIMESTAMPTZ DEFAULT NOW()
 );

 CREATE INDEX idx_hourly_hour_start ON ingestion_metrics_hourly(hour_start DESC);

 -- Daily rollups
 CREATE TABLE IF NOT EXISTS ingestion_metrics_daily (
     id BIGSERIAL PRIMARY KEY,
     date DATE NOT NULL UNIQUE,

     sources_processed INTEGER DEFAULT 0,
     atoms_created INTEGER DEFAULT 0,
     atoms_failed INTEGER DEFAULT 0,
     success_rate FLOAT,
     avg_quality_score FLOAT,

     -- Coverage
     unique_vendors INTEGER,
     unique_equipment_types INTEGER,
     vendor_distribution JSONB,
     equipment_distribution JSONB,

     created_at TIMESTAMPTZ DEFAULT NOW()
 );

 ---
 2. Code Architecture

 File Structure

 agent_factory/
 ‚îî‚îÄ‚îÄ observability/
     ‚îú‚îÄ‚îÄ __init__.py
     ‚îú‚îÄ‚îÄ ingestion_monitor.py       # Core metrics collector (IngestionMonitor class)
     ‚îú‚îÄ‚îÄ telegram_notifier.py       # Real-time notifications (TelegramNotifier class)
     ‚îú‚îÄ‚îÄ metrics_aggregator.py      # Hourly/daily rollups (background daemon)
     ‚îî‚îÄ‚îÄ dashboard/
         ‚îú‚îÄ‚îÄ __init__.py
         ‚îú‚îÄ‚îÄ gradio_app.py          # Web UI (Gradio app with 4 tabs)
         ‚îî‚îÄ‚îÄ telegram_commands.py   # /stats, /kb_status, /ingestion_live

 Core Classes

 IngestionMonitor (ingestion_monitor.py)

 - Purpose: Tracks ingestion pipeline metrics via LangGraph stream() API
 - Key Methods:
   - start_monitoring(url, source_type) ‚Üí session_id
   - record_stage_completion(session_id, stage, duration_ms, success, metadata)
   - finish_monitoring(session_id, atoms_created, atoms_failed, avg_quality, error)
 - Non-blocking: Queues DB writes to background task (<5ms overhead)
 - Batch inserts: Writes 10 records at once or every 5 seconds

 TelegramNotifier (telegram_notifier.py)

 - Purpose: Send real-time notifications (VERBOSE or BATCH mode)
 - Modes:
   - VERBOSE: Every source (10-50 msg/hour)
   - BATCH: 5-minute batches (1 msg per 5 min)
 - Features:
   - Quiet hours (configurable, default 11pm-7am)
   - Rate limiting (20 msg/min max)
   - Error tolerance (notification failures don't break ingestion)

 MetricsAggregator (metrics_aggregator.py)

 - Purpose: Background daemon for hourly/daily aggregations
 - Runs: Every hour (via systemd timer or cron)
 - Queries: ingestion_metrics_realtime ‚Üí aggregates to hourly/daily tables

 ---
 3. Integration with Ingestion Pipeline

 Hook Point: LangGraph stream() API

 File: agent_factory/workflows/ingestion_chain.py

 Modify: ingest_source_async() function

 from agent_factory.observability.ingestion_monitor import IngestionMonitor

 async def ingest_source_async(url: str, source_type: str = 'web') -> Dict[str, Any]:
     """Async wrapper with monitoring hooks (stream() API)."""

     # Initialize monitor
     monitor = IngestionMonitor(db_manager=DatabaseManager())
     session_id = await monitor.start_monitoring(url, source_type)

     # Create chain
     chain = create_ingestion_chain()

     # Initialize state
     initial_state = {
         "url": url,
         "source_type": source_type,
         # ... (existing fields)
     }

     # Stream execution (yields stage updates)
     stage_timings = {}
     last_time = time.time()
     final_state = None

     try:
         # Use astream() instead of invoke() for stage-by-stage visibility
         async for event in chain.astream(initial_state):
             current_stage = event.get('current_stage')

             if current_stage and current_stage not in stage_timings:
                 # Record stage completion
                 duration_ms = int((time.time() - last_time) * 1000)
                 stage_timings[current_stage] = duration_ms

                 await monitor.record_stage_completion(
                     session_id=session_id,
                     stage=current_stage,
                     duration_ms=duration_ms,
                     success=len(event.get('errors', [])) == 0,
                     metadata={}
                 )

                 last_time = time.time()

             final_state = event

         # Calculate average quality
         validated_atoms = final_state.get('validated_atoms', [])
         avg_quality = sum(a.get('quality_score', 0) for a in validated_atoms) / len(validated_atoms) if validated_atoms else 0

         # Finish monitoring
         await monitor.finish_monitoring(
             session_id=session_id,
             atoms_created=final_state.get('atoms_created', 0),
             atoms_failed=final_state.get('atoms_failed', 0),
             avg_quality=avg_quality / 100  # Convert to 0-1
         )

         return final_state

     except Exception as e:
         await monitor.finish_monitoring(
             session_id=session_id,
             atoms_created=0,
             atoms_failed=0,
             avg_quality=0.0,
             error=str(e)
         )
         raise

 Why stream() instead of invoke()?
 - stream() yields intermediate states (per-stage updates)
 - invoke() only returns final state (no visibility)
 - Minimal changes to existing code

 ---
 4. Telegram Dashboard Commands

 File: agent_factory/observability/dashboard/telegram_commands.py

 Implement 3 commands:

 /stats - Last 24h metrics

 üìä KB Ingestion Stats (Last 24h)

 Throughput:
 ‚Ä¢ Sources: 143 processed (92% success)
 ‚Ä¢ Atoms: 1,247 created, 89 failed
 ‚Ä¢ Avg Duration: 3.8s per source

 Quality:
 ‚Ä¢ Avg Score: 84.2%
 ‚Ä¢ Pass Rate: 93.5%

 Top Bottleneck: Stage 4 (Generation) - 2.1s avg

 /kb_status - Overall KB health

 üîç Knowledge Base Status

 Total Atoms: 12,456
 Growth: +247 today (+2.0%)

 Last Ingestion: 14 min ago
 Queue: 3 sources pending

 /ingestion_live - Real-time activity

 ‚ö° Live Ingestion Activity

 Currently Processing: 0 sources

 Recent Completions (Last 10 min):
 ‚úÖ manual1.pdf - 14 atoms, 92% quality, 4.1s
 ‚úÖ manual2.pdf - 8 atoms, 88% quality, 3.5s
 ‚ùå manual3.pdf - Failed at Stage 2

 Integration: Add command handlers to orchestrator_bot.py:

 from agent_factory.observability.dashboard.telegram_commands import (
     handle_stats_command,
     handle_kb_status_command,
     handle_ingestion_live_command
 )

 def main():
     app = Application.builder().token(BOT_TOKEN).post_init(post_init).build()

     # NEW: Observability commands
     app.add_handler(CommandHandler("stats", handle_stats_command))
     app.add_handler(CommandHandler("kb_status", handle_kb_status_command))
     app.add_handler(CommandHandler("ingestion_live", handle_ingestion_live_command))

     # ... existing handlers

 ---
 5. Web Dashboard (Gradio)

 File: agent_factory/observability/dashboard/gradio_app.py

 Features:
 - 4 tabs: Overview, Quality Analysis, Vendor Coverage, Stage Performance
 - Live charts with Plotly
 - Auto-refresh every 30 seconds
 - Responsive design

 Tabs:

 1. Overview
   - Metrics: Sources processed, atoms created, success rate
   - Charts: Throughput over time, quality trends
 2. Quality Analysis
   - Histogram: Quality score distribution
   - Pie chart: Pass/fail validation rate
 3. Vendor Coverage
   - Pie chart: Atoms by vendor (Siemens, Rockwell, ABB, etc.)
   - Bar chart: Atoms by equipment type (PLC, VFD, HMI, etc.)
 4. Stage Performance
   - Waterfall chart: Average duration by stage
   - Bottleneck indicator: Highlights slowest stage

 Launch:
 python -m agent_factory.observability.dashboard.gradio_app
 # Opens on http://localhost:7860

 ---
 6. VPS Deployment

 Deploy to Hostinger VPS (72.60.175.144)

 1. Deploy Web Dashboard as systemd service

 ssh root@72.60.175.144

 cat > /etc/systemd/system/kb-dashboard.service << 'EOF'
 [Unit]
 Description=KB Ingestion Dashboard (Gradio)
 After=network.target

 [Service]
 Type=simple
 User=root
 WorkingDirectory=/root/Agent-Factory
 ExecStart=/usr/bin/python3 -m agent_factory.observability.dashboard.gradio_app
 Restart=always
 RestartSec=10
 Environment="PATH=/usr/local/bin:/usr/bin"

 [Install]
 WantedBy=multi-user.target
 EOF

 systemctl daemon-reload
 systemctl enable kb-dashboard
 systemctl start kb-dashboard
 systemctl status kb-dashboard

 2. Deploy Metrics Aggregator

 cat > /etc/systemd/system/kb-aggregator.service << 'EOF'
 [Unit]
 Description=KB Metrics Aggregator
 After=network.target postgresql.service

 [Service]
 Type=simple
 User=root
 WorkingDirectory=/root/Agent-Factory
 ExecStart=/usr/bin/python3 -m agent_factory.observability.metrics_aggregator
 Restart=always
 RestartSec=10

 [Install]
 WantedBy=multi-user.target
 EOF

 systemctl daemon-reload
 systemctl enable kb-aggregator
 systemctl start kb-aggregator

 3. Access Dashboard

 Option A: SSH Tunnel (Secure)
 ssh -L 7860:localhost:7860 root@72.60.175.144
 # Open http://localhost:7860 in browser

 Option B: Public Access
 ufw allow 7860/tcp
 # Access http://72.60.175.144:7860

 ---
 7. Performance Optimizations

 Latency Mitigation

 | Optimization            | Impact              | Implementation                    |
 |-------------------------|---------------------|-----------------------------------|
 | Async background writes | 95% reduction       | Queue + batch insert (10 records) |
 | Connection pooling      | 50% faster queries  | Reuse DB connections              |
 | Batch notifications     | 80% fewer API calls | 5-minute batches                  |
 | Stream vs invoke        | 0% overhead         | LangGraph native feature          |

 Expected Latency Overhead: <5ms per stage (vs 50-100ms for synchronous DB insert)

 Rate Limiting

 # In TelegramNotifier
 from aiolimiter import AsyncLimiter

 self.rate_limiter = AsyncLimiter(max_rate=20, time_period=60)  # 20 msg/min

 async def notify_source_completed(self, ...):
     async with self.rate_limiter:
         await self.bot.send_message(...)

 ---
 8. Implementation Sequence

 Phase 1: Database Schema (Week 1, Day 1-2) - ‚úÖ COMPLETE

 - Create docs/database/observability_migration.sql
 - Run SQL migration on VPS PostgreSQL (via scripts/run_observability_migration.py)
 - Verify tables created with SELECT queries
 - Test INSERT queries manually

 Phase 2: Core Monitoring (Week 1, Day 3-5) - üîÑ IN PROGRESS

 - Create agent_factory/observability/ directory
 - Implement IngestionMonitor class (ingestion_monitor.py) - NEXT
 - Implement TelegramNotifier class (telegram_notifier.py)
 - Modify ingestion_chain.py to use stream() API
 - Add background writer (batch inserts) - Already in IngestionMonitor design
 - Test with 1 source ingestion
 - Load test with 10 parallel sources

 Phase 2 Implementation Notes:

 File Creation Strategy (Windows Compatibility):
 - ‚ùå DON'T use Bash heredoc (causes quote escaping issues on Windows)
 - ‚úÖ DO use Python helper scripts to write large files
 - ‚úÖ DO use Edit tool for existing files
 - ‚úÖ DO create minimal stub first, then expand with Edit tool

 IngestionMonitor Implementation:
 1. Create minimal version with core structure (dataclasses, init)
 2. Add start_monitoring() method
 3. Add record_stage_completion() method
 4. Add finish_monitoring() method
 5. Add async background writer with batch queue
 6. Test with VPS PostgreSQL connection

 VPS PostgreSQL Connection Pattern:
 import psycopg
 import os
 from dotenv import load_dotenv

 load_dotenv()

 conn = psycopg.connect(
     host=os.getenv("VPS_KB_HOST", "72.60.175.144"),
     port=os.getenv("VPS_KB_PORT", "5432"),
     dbname=os.getenv("VPS_KB_DATABASE", "rivet"),
     user=os.getenv("VPS_KB_USER", "rivet"),
     password=os.getenv("VPS_KB_PASSWORD", "rivet_factory_2025!"),
     connect_timeout=10
 )

 Phase 3: Telegram Notifications (Week 1-2, Day 6-8)

 - Implement TelegramNotifier class
 - Add VERBOSE mode (every source)
 - Add BATCH mode (5-minute batches)
 - Add quiet hours (11pm-7am)
 - Test with mock bot (no real messages)
 - Test live with admin chat (chat_id=8445149012)

 Phase 4: Metrics Aggregation (Week 2, Day 9-11)

 - Implement MetricsAggregator class
 - Add hourly aggregation logic (SQL queries)
 - Add daily rollup logic
 - Deploy as systemd service on VPS
 - Verify cron triggers (hourly)

 Phase 5: Telegram Commands (Week 2, Day 12-14)

 - Create telegram_commands.py file
 - Implement /stats command
 - Implement /kb_status command
 - Implement /ingestion_live command
 - Add handlers to orchestrator_bot.py
 - Test in Telegram with @RivetCeo_bot

 Phase 6: Web Dashboard (Week 3, Day 15-20)

 - Create gradio_app.py skeleton
 - Implement Overview tab (metrics + charts)
 - Implement Quality Analysis tab
 - Implement Vendor Coverage tab
 - Implement Stage Performance tab
 - Add auto-refresh (30s interval)
 - Deploy to VPS as systemd service
 - Test SSH tunnel access

 Phase 7: Production Hardening (Week 4, Day 21-25)

 - Add error handling (try/except in all async functions)
 - Add circuit breakers (stop after 5 consecutive failures)
 - Set up log rotation (logrotate config)
 - Add monitoring for monitoring (Healthchecks.io ping)
 - Document operations runbook
 - Load test with 100 parallel sources

 ---
 9. Critical Files

 Must Modify:

 1. agent_factory/workflows/ingestion_chain.py - Add stream() API hook (line ~711)
 2. agent_factory/integrations/telegram/orchestrator_bot.py - Add command handlers (line ~728)

 Must Create:

 1. docs/database/observability_migration.sql - New metrics tables
 2. agent_factory/observability/ingestion_monitor.py - Core metrics collector
 3. agent_factory/observability/telegram_notifier.py - Real-time notifications
 4. agent_factory/observability/metrics_aggregator.py - Hourly/daily rollups
 5. agent_factory/observability/dashboard/gradio_app.py - Web UI
 6. agent_factory/observability/dashboard/telegram_commands.py - /stats commands

 Reference for Patterns:

 1. agent_factory/integrations/telegram/orchestrator_bot.py:189-196 - Existing notification pattern
 2. agent_factory/core/trace_logger.py - Existing tracing pattern
 3. agent_factory/core/callbacks.py - EventBus pattern
 4. agent_factory/core/performance.py - Performance tracking pattern

 ---
 10. Testing Strategy

 Unit Tests

 # Test IngestionMonitor
 poetry run pytest tests/observability/test_ingestion_monitor.py -v

 # Test TelegramNotifier (mock bot)
 poetry run pytest tests/observability/test_telegram_notifier.py -v

 Integration Tests

 # Test full pipeline with monitoring
 poetry run python scripts/test_observability_e2e.py

 # Expected output:
 # [IngestionMonitor] Session started: abc123
 # [Stage 1] Acquisition completed in 234ms
 # [Stage 2] Extraction completed in 456ms
 # ...
 # [Stage 7] Storage completed in 89ms
 # [IngestionMonitor] Total: 5 atoms created, 0 failed, 3.2s total
 # [TelegramNotifier] Message sent to chat_id=8445149012
 # [Database] Inserted 1 record to ingestion_metrics_realtime

 Manual Testing (Telegram)

 1. Send /stats ‚Üí Verify 24h metrics
 2. Send /kb_status ‚Üí Verify total atoms, growth
 3. Send /ingestion_live ‚Üí Verify recent activity
 4. Trigger research pipeline ‚Üí Verify real-time notification arrives

 Load Testing

 # Test 100 parallel sources
 poetry run python scripts/load_test_ingestion.py --sources 100

 # Monitor VPS resources
 ssh root@72.60.175.144 "htop"

 ---
 Success Criteria

 Week 1 Deliverables:

 - ‚úÖ Database tables created and tested
 - ‚úÖ IngestionMonitor tracks all 7 stages
 - ‚úÖ Telegram notifications work (at least BATCH mode)
 - ‚úÖ <5ms latency overhead per source

 Week 2 Deliverables:

 - ‚úÖ Telegram commands functional (/stats, /kb_status, /ingestion_live)
 - ‚úÖ Metrics aggregator running on VPS
 - ‚úÖ Hourly/daily tables populated

 Week 3 Deliverables:

 - ‚úÖ Web dashboard accessible via SSH tunnel
 - ‚úÖ All 4 tabs functional with live charts
 - ‚úÖ Auto-refresh working

 Week 4 Deliverables:

 - ‚úÖ Production-ready (error handling, circuit breakers, log rotation)
 - ‚úÖ Load tested with 100 parallel sources
 - ‚úÖ Operations runbook documented

 User Acceptance:

 - ‚úÖ "I can see every source being processed in Telegram"
 - ‚úÖ "I can query stats on-demand with /stats"
 - ‚úÖ "I can see trends over time in the web dashboard"
 - ‚úÖ "The system doesn't slow down ingestion noticeably"

 ---
 Risk Mitigation

 Risk 1: Telegram Rate Limiting (30 msg/sec hard limit)

 Mitigation: Default to BATCH mode (5-minute batches), rate limiter (20 msg/min), quiet hours

 Risk 2: Database Write Latency

 Mitigation: Async background writes, batch inserts (10 records), connection pooling

 Risk 3: LangGraph stream() Overhead

 Mitigation: stream() has near-zero overhead (native feature), tested with 100 parallel sources

 Risk 4: Notification Spam During Bulk Ingestion

 Mitigation: User configurable VERBOSE vs BATCH mode, quiet hours, summary messages

 ---
 Immediate Next Actions (Phase 2)

 Task 1: Implement IngestionMonitor Class

 File: agent_factory/observability/ingestion_monitor.py

 Approach:
 1. Create helper script: scripts/create_ingestion_monitor.py
 2. Script writes complete IngestionMonitor implementation:
   - IngestionSession dataclass (tracks single ingestion session)
   - IngestionMonitor class with async methods
   - VPS PostgreSQL connection (using credentials from .env)
   - Batch queue with background writer
   - Global singleton get_global_monitor()

 Key Features:
 - Async start_monitoring(url, source_type) ‚Üí returns session_id
 - Async record_stage_completion(session_id, stage, duration_ms, success, metadata)
 - Async finish_monitoring(session_id, atoms_created, atoms_failed, avg_quality, error)
 - Background task _batch_writer() flushes every 5 seconds or when queue reaches 10
 - Non-blocking DB writes (<5ms overhead)

 Testing:
 # Import check
 poetry run python -c "from agent_factory.observability.ingestion_monitor import IngestionMonitor; print('OK')"

 # Functional test
 poetry run python scripts/test_ingestion_monitor.py

 Task 2: Implement TelegramNotifier Class

 File: agent_factory/observability/telegram_notifier.py

 Features:
 - VERBOSE mode: Send notification for every source
 - BATCH mode: Aggregate 5-minute batches
 - Quiet hours: 11pm-7am (configurable)
 - Rate limiting: 20 msg/min max
 - Error tolerance: Notification failures don't break ingestion

 Integration:
 from agent_factory.observability.telegram_notifier import TelegramNotifier

 notifier = TelegramNotifier(mode="BATCH")  # or "VERBOSE"
 await notifier.notify_source_completed(url, status, atoms_created, duration_ms)

 Task 3: Hook into Ingestion Pipeline

 File: agent_factory/workflows/ingestion_chain.py

 Modifications:
 - Import IngestionMonitor and TelegramNotifier
 - Modify ingest_source_async() to use LangGraph astream() API
 - Call monitor.start_monitoring() before pipeline
 - Call monitor.record_stage_completion() after each stage
 - Call monitor.finish_monitoring() + notifier.notify_source_completed() at end

 Success Criteria:
 - ‚úÖ Every ingestion writes to ingestion_metrics_realtime table
 - ‚úÖ Telegram notification sent (BATCH or VERBOSE mode)
 - ‚úÖ <5ms overhead per stage
 - ‚úÖ No blocking on DB writes

 ---
 Summary

 This plan delivers comprehensive observability with minimal latency impact, production-quality code, and multiple visualization layers as requested.

 Phase 1 is COMPLETE - Database schema deployed and verified on VPS PostgreSQL.

 Phase 2 is IN PROGRESS - Next: Implement IngestionMonitor and TelegramNotifier classes.
‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå

 Would you like to proceed?

 ‚ùØ 1. Yes, and auto-accept edits
   2. Yes, and manually approve edits
   3. Type here to tell Claude what to change

 ctrl-g to edit in Notepad
