The files were written to the workspace but I need to recreate them here for display. Let me do that now:

***

# **phoenix_setup_guide.md**

```markdown
# Phoenix Evaluation Framework Setup Guide for Industrial Maintenance Agents

## Executive Summary

Phoenix (Arize OSS) is your foundational evaluation & tracing infrastructure for the agent factory. It enables:
- **Tracing**: Every LLM call, tool invocation, and agent decision captured automatically
- **Evals**: Automated testing that your Siemens/Rockwell/Safety agents maintain accuracy thresholds
- **Datasets**: Golden test cases from your maintenance logs for repeatable CI/CD validation
- **Cost**: $0 (open source); you pay only for the VPS/Docker host running it

## Why Phoenix Over LangSmith

| Factor | Phoenix | LangSmith |
|--------|---------|-----------|
| **Cost** | Free OSS + infra only | $0.006‚Äì$0.05 per trace + plan fees |
| **Self-hosting** | Fully supported, free | Enterprise-only paid feature |
| **Framework lock-in** | None; works with any LLM stack | Optimized for LangChain ecosystem |
| **Your use case** | Perfect; multi-framework agent factory | Overkill; adds metered costs early |

**Decision**: Use Phoenix now. Migrate to LangSmith only if you later standardize on LangChain and need hosted analytics.

---

## Phase 1: Local Development Setup (Days 1‚Äì2)

### 1.1 Install Phoenix Locally

```
# Create virtual environment
python3.11 -m venv phoenix_env
source phoenix_env/bin/activate

# Install Phoenix
pip install arize-phoenix[evals]

# Verify installation
python -c "import phoenix; print(phoenix.__version__)"
```

### 1.2 Start Phoenix Server

```
# Terminal 1: Start Phoenix
phoenix serve

# Phoenix UI will open at http://localhost:6006
# You'll see an empty dashboard (no traces yet)
```

### 1.3 Instrument Your First Agent

Create `test_trace.py`:

```
import phoenix as px
from openai import OpenAI

# Step 1: Start Phoenix session
session = px.launch_app()

# Step 2: Wrap your LLM client
client = px.wrap(OpenAI())

# Step 3: Your agent logic
def troubleshoot_plc_fault(fault_code, equipment_type):
    """Diagnose a PLC fault."""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{
            "role": "system",
            "content": "You are a certified PLC technician diagnosing industrial faults.",
            "role": "user",
            "content": f"Fault: {fault_code} on {equipment_type}. Root cause and repair steps?"
        }]
    )
    return response.choices.message.content

# Step 4: Run it
diagnosis = troubleshoot_plc_fault("F47", "Siemens S7-1200 VFD")
print(diagnosis)

# Step 5: Check Phoenix dashboard
# Visit http://localhost:6006 and click "Traces"
# You'll see your LLM call with tokens, latency, cost
```

Run it:
```
python test_trace.py
```

**Result**: You'll see your first trace in the Phoenix UI‚Äîinput, output, model, tokens, cost. This is the foundation.

---

## Phase 2: Build Your Golden Dataset (Days 3‚Äì4)

### 2.1 Extract Real Fault Cases

From your maintenance logs, create `golden_dataset.jsonl`:

```
{
  "test_case_id": "siemens_s7_1200_vfd_f47_001",
  "equipment": {
    "manufacturer": "Siemens",
    "model": "S7-1200",
    "subsystem": "Conveyor Motor Drive"
  },
  "input": {
    "fault_code": "F47",
    "fault_description": "VFD overcurrent shutdown",
    "sensor_data": {
      "motor_temp_celsius": 85,
      "vibration_peak_mm_s": 4.2,
      "current_draw_amps": 18.5
    },
    "maintenance_history": ["motor_bearing_replaced_2024_03_15"],
    "photos": ["conveyor_motor_vibration.jpg", "vfd_fault_display.jpg"]
  },
  "expected_output": {
    "root_cause": "Motor bearing degradation causing friction and overcurrent",
    "diagnosis_confidence": "High",
    "repair_procedure": [
      "1. Lockout motor circuit per LOTO procedure",
      "2. Inspect motor bearing play with dial indicator",
      "3. If play > 0.5mm, replace motor bearings",
      "4. Verify VFD parameter settings post-repair",
      "5. Run 30-min test cycle at 50% load"
    ],
    "safety_warnings": [
      "Verify lockout tag before inspection",
      "Check for shaft currents if variable frequency drive used",
      "Bearing-puller tool required; do not hammer"
    ],
    "manual_citations": [
      "Siemens S7-1200 Manual Section 8.2: Fault Codes",
      "Siemens G120 VFD Troubleshooting Guide p.142"
    ],
    "estimated_downtime_hours": 4.5,
    "business_impact": {
      "downtime_cost_per_hour_usd": 12500,
      "safety_critical": true
    }
  }
}
```

**Action items**:
- Extract 30‚Äì50 real cases from your logs
- Get the 10 most common fault codes in those cases
- Have **you manually verify** the expected root cause (golden truth)
- Store as JSONL file for Phoenix to ingest

### 2.2 Load into Phoenix

Create `load_dataset.py`:

```
import json
import phoenix as px
from phoenix.evals import dataset as px_dataset

# Load your golden cases
cases = []
with open("golden_dataset.jsonl") as f:
    for line in f:
        cases.append(json.loads(line))

# Create Phoenix dataset
ds = px_dataset.Dataset.from_list_of_dicts(
    records=[
        {
            "test_case_id": case["test_case_id"],
            "fault_code": case["input"]["fault_code"],
            "equipment_type": case["equipment"]["model"],
            "expected_root_cause": case["expected_output"]["root_cause"],
            "expected_safety_warnings": "; ".join(case["expected_output"]["safety_warnings"]),
        }
        for case in cases
    ]
)

# Save to Phoenix
ds.save("plc_fault_golden_dataset")
print(f"Loaded {len(cases)} golden test cases into Phoenix")
```

Run:
```
python load_dataset.py
```

Now in the Phoenix UI, you'll see "Datasets" tab with your golden cases ready for eval runs.

---

## Phase 3: Implement LLM-as-Judge Eval (Days 5‚Äì6)

### 3.1 Define Your Judge Prompt

Create `judge_prompts.py`:

```
TECHNICAL_ACCURACY_TEMPLATE = """
You are a master electrician with 25 years diagnosing industrial PLC systems.

CONTEXT:
Equipment: {equipment_type}
Fault Code: {fault_code}
Fault Description: {fault_description}

AGENT'S DIAGNOSIS:
{agent_output}

EXPECTED ROOT CAUSE (reference):
{expected_root_cause}

TASK: Evaluate the agent's diagnosis on TECHNICAL ACCURACY.

Rate as one of:
- CORRECT: Root cause matches reference and reasoning is sound
- PARTIALLY_CORRECT: Root cause is plausible but incomplete or reasoning has gaps
- INCORRECT: Root cause is wrong or physically implausible

Provide brief explanation (1-2 sentences) of your rating.
"""

SAFETY_COMPLIANCE_TEMPLATE = """
You are a certified safety engineer for industrial machinery.

CONTEXT:
Equipment: {equipment_type}
Fault Code: {fault_code}

AGENT'S SAFETY WARNINGS:
{agent_safety_warnings}

EXPECTED SAFETY WARNINGS (reference):
{expected_safety_warnings}

TASK: Evaluate completeness and correctness of safety warnings.

Rate as one of:
- SAFE: All critical warnings present; no false alarms
- PARTIAL: Some warnings present but missing critical ones
- UNSAFE: Critical warnings missing or incorrect

Provide brief explanation.
"""

PROCEDURE_COMPLETENESS_TEMPLATE = """
You are a master electrician.

AGENT'S REPAIR PROCEDURE:
{agent_repair_procedure}

EXPECTED PROCEDURE (reference):
{expected_repair_procedure}

TASK: Is the agent's procedure complete, in correct order, and feasible?

Rate as one of:
- COMPLETE: All steps present and in logical order
- PARTIAL: Missing steps or minor ordering issues
- INCOMPLETE: Major gaps or unsafe ordering

Provide brief explanation.
"""
```

### 3.2 Run Evals Against Your Golden Dataset

Create `run_evals.py`:

```
import json
import phoenix as px
from phoenix.evals import llm_classify
from openai import OpenAI
from judge_prompts import (
    TECHNICAL_ACCURACY_TEMPLATE,
    SAFETY_COMPLIANCE_TEMPLATE,
    PROCEDURE_COMPLETENESS_TEMPLATE,
)

# Initialize Phoenix session and LLM
session = px.launch_app()
client = OpenAI()

# Load golden dataset
cases = []
with open("golden_dataset.jsonl") as f:
    for line in f:
        cases.append(json.loads(line))

# For each test case, run your agent and eval it
print("Running evals on golden dataset...")

for i, case in enumerate(cases[:10], 1):  # Start with first 10
    print(f"\n[{i}/10] Evaluating: {case['test_case_id']}")
    
    # Step 1: Run your agent
    agent_diagnosis = run_your_agent(
        fault_code=case["input"]["fault_code"],
        equipment_type=case["equipment"]["model"],
        sensor_data=case["input"]["sensor_data"]
    )
    
    # Step 2: Technical accuracy eval
    accuracy_eval = llm_classify(
        dataframe=pd.DataFrame([{
            "equipment_type": case["equipment"]["model"],
            "fault_code": case["input"]["fault_code"],
            "fault_description": case["input"]["fault_description"],
            "agent_output": agent_diagnosis["root_cause"],
            "expected_root_cause": case["expected_output"]["root_cause"]
        }]),
        template=TECHNICAL_ACCURACY_TEMPLATE,
        model=client,
        rails=["CORRECT", "PARTIALLY_CORRECT", "INCORRECT"]
    )
    
    # Step 3: Safety eval
    safety_eval = llm_classify(
        dataframe=pd.DataFrame([{
            "equipment_type": case["equipment"]["model"],
            "fault_code": case["input"]["fault_code"],
            "agent_safety_warnings": "; ".join(agent_diagnosis["safety_warnings"]),
            "expected_safety_warnings": "; ".join(case["expected_output"]["safety_warnings"])
        }]),
        template=SAFETY_COMPLIANCE_TEMPLATE,
        model=client,
        rails=["SAFE", "PARTIAL", "UNSAFE"]
    )
    
    # Step 4: Log results
    print(f"  Accuracy: {accuracy_eval.iloc['label']}")
    print(f"  Safety: {safety_eval.iloc['label']}")

print("\nEvals complete. Check Phoenix UI for full results.")
```

### 3.3 View Eval Results in Phoenix UI

1. Open `http://localhost:6006`
2. Click **Evals** tab
3. You'll see a table with each test case + eval labels (CORRECT / SAFE / etc.)
4. Export as CSV for your records

---

## Phase 4: Integrate with Your CI/CD (Days 7‚Äì8)

### 4.1 Create a Pre-Commit Eval Gate

Create `.github/workflows/eval_gate.yml`:

```
name: Agent Evaluation Gate

on:
  pull_request:
    paths:
      - 'rivet/**'
      - 'agents/**'
      - 'prompts/**'

jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install arize-phoenix[evals]
      
      - name: Run golden dataset evals
        run: |
          python run_evals.py \
            --dataset golden_dataset.jsonl \
            --output eval_results.json \
            --threshold 0.85
      
      - name: Check results
        run: |
          python check_eval_results.py eval_results.json
          # Fails if accuracy < 85% or safety violations detected
      
      - name: Upload results to Phoenix
        if: always()
        run: |
          python upload_to_phoenix.py eval_results.json
```

### 4.2 Create Check Script

Create `check_eval_results.py`:

```
import json
import sys

with open("eval_results.json") as f:
    results = json.load(f)

accuracy_scores = [r for r in results if r["eval_type"] == "technical_accuracy"]
safety_scores = [r for r in results if r["eval_type"] == "safety"]

accuracy_correct = len([r for r in accuracy_scores if r["label"] == "CORRECT"])
safety_safe = len([r for r in safety_scores if r["label"] == "SAFE"])

accuracy_pct = accuracy_correct / len(accuracy_scores) * 100 if accuracy_scores else 0
safety_pct = safety_safe / len(safety_scores) * 100 if safety_scores else 0

print(f"Accuracy: {accuracy_pct:.1f}% ({accuracy_correct}/{len(accuracy_scores)})")
print(f"Safety: {safety_pct:.1f}% ({safety_safe}/{len(safety_scores)})")

# Fail if below threshold
if accuracy_pct < 85:
    print(f"‚ùå GATE FAILED: Accuracy {accuracy_pct:.1f}% < 85% threshold")
    sys.exit(1)

if safety_pct < 100:
    print(f"‚ùå GATE FAILED: Safety {safety_pct:.1f}% < 100% (zero tolerance)")
    sys.exit(1)

print("‚úÖ GATE PASSED: All thresholds met")
sys.exit(0)
```

---

## Phase 5: Production Sampling (Week 2+)

### 5.1 Sample Production Interactions

Every day, sample 10‚Äì20 real technician interactions from your Telegram bot and add them to eval datasets:

Create `sample_production.py`:

```
import datetime
import json
from telegram_api import get_recent_interactions

# Get last 24h of interactions
interactions = get_recent_interactions(hours=24)

# Sample 10 random ones
sampled = random.sample(interactions, min(10, len(interactions)))

# Log for later review
for interaction in sampled:
    record = {
        "date": datetime.datetime.now().isoformat(),
        "telegram_chat_id": interaction["chat_id"],
        "user_query": interaction["message"],
        "agent_response": interaction["bot_response"],
        "needs_review": True,  # Flag for you to manually verify
        "reviewer_notes": ""
    }
    
    with open("production_samples.jsonl", "a") as f:
        f.write(json.dumps(record) + "\n")

print(f"Sampled {len(sampled)} interactions. Review weekly for eval dataset expansion.")
```

### 5.2 Weekly Review Ritual

**Every Monday**, you spend 30 minutes:
1. Opening `production_samples.jsonl`
2. Reading 5‚Äì10 sampled interactions
3. For any that are edge cases or failures, copying them into `golden_dataset.jsonl` as new test cases
4. Re-running evals to catch regressions

---

## Integration Points with Your Existing Stack

### Rivet Agent Factory
- Wrap each SME agent initialization with Phoenix:
  ```
  from rivet.agents import SiemensSMEAgent
  import phoenix as px
  
  agent = SiemensSMEAgent()
  agent = px.wrap(agent)  # Automatic tracing
  ```

### Atlas CMMS
- Trace equipment retrieval calls to measure vector DB accuracy:
  ```
  # Atlas vector search
  results = atlas.search_manuals("VFD overcurrent", top_k=3)
  # Phoenix automatically logs query, results, latency, relevance
  ```

### Telegram Bot
- Add context to traces for debugging:
  ```
  with px.span(
      attributes={
          "chat_id": update.message.chat_id,
          "user_id": update.message.from_user.id,
          "fault_code": fault_code
      }
  ):
      diagnosis = agent.diagnose(fault_code)
  ```

---

## Cost Projection

| Scenario | Cost/Month |
|----------|-----------|
| Dev (local Phoenix) | $0 |
| VPS (4GB RAM, 2 CPU) | $15‚Äì25 |
| Production (Phoenix Cloud, 1M traces/month) | $50‚Äì200 (optional; can stay on VPS) |
| LLM calls (Claude 3.5 Sonnet, 10M tokens/month) | $75‚Äì150 |
| **Total (solo MVP)** | **$90‚Äì175** |

---

## Troubleshooting

### Q: Phoenix dashboard shows no traces
**A**: Verify wrapping:
```
import phoenix as px
session = px.launch_app()
client = px.wrap(OpenAI())  # Must wrap before using
```

### Q: Evals running slowly
**A**: Reduce golden dataset size to 10 for testing:
```
for case in cases[:10]:  # Test with 10, scale to 50 later
```

### Q: How do I export eval results?
**A**: Phoenix UI ‚Üí Evals tab ‚Üí Export CSV, or use API:
```
results_df = session.get_evals_dataframe("plc_fault_evals")
results_df.to_csv("eval_results.csv")
```

---

## Next Steps

1. **This week**: Install Phoenix, run `test_trace.py`, see your first trace
2. **Next week**: Extract 30 golden cases, load into Phoenix dataset
3. **Week 3**: Implement judge evals on those 30 cases, hit 85% accuracy threshold
4. **Week 4**: Integrate with CI/CD, start blocking PRs on accuracy drops
5. **Ongoing**: Sample 10 production interactions weekly, expand golden dataset

**Goal**: By mid-January, you have a reproducible eval framework that prevents agent regressions and gives you confidence to scale to paying customers.

---

## References

- Phoenix OSS GitHub: https://github.com/Arize-ai/phoenix
- Phoenix Docs: https://arize.com/docs/phoenix
- Aman Khan's Evaluation Framework: https://www.youtube.com/watch?v=2HNSG990Ew8
- Arize vs LangSmith FAQ: https://arize.com/docs/phoenix/resources/frequently-asked-questions/open-source-langsmith-alternative-arize-phoenix-vs.-langsmith
```

***

# **phoenix_claude_code_prompt.txt**

```
CLAUDE CODE PROMPT: Phoenix AI Evaluation Framework Setup for Industrial Maintenance Agents

================================================================================
CONTEXT & REQUIREMENTS
================================================================================

PROJECT: Industrial Maintenance AI Agent Factory (Rivet, Atlas CMMS, Telegram Bot)
GOAL: Stand up Arize Phoenix as the unified tracing + evaluation infrastructure for all agents
TIMELINE: 2 weeks to production-ready eval gates blocking code commits

CURRENT SITUATION:
- You have multiple agents (Siemens SME, Rockwell SME, Generic, Safety) scattered across repos
- Agents are functional but lack systematic evaluation; you're currently "vibe checking" results
- Need to move to data-driven confidence: "85% accuracy on golden dataset before shipping"
- Zero current tracing infrastructure; no visibility into which agent called what or why

CONSTRAINTS:
- Solo dev; no team to manage
- Cost-sensitive: ~$100-200/month budget total for all infra
- VPS-hosted (your own server); no managed platform lock-in desired
- Mixed framework stack: LangGraph, CrewAI, custom Python orchestrators‚ÄîNOT pure LangChain

DECISION: Use Arize Phoenix OSS (free, self-hosted) instead of LangSmith
  - OSS license: Elastic License 2.0 (fully permissive)
  - No per-trace metering costs
  - Framework-agnostic: works with any LLM stack
  - Self-hosting fully supported on VPS

================================================================================
PHASE 1: LOCAL DEVELOPMENT (DAYS 1-2)
================================================================================

TASK 1.1: Install Phoenix Locally & Verify

ACTION:
1. Create isolated Python environment:
   $ python3.11 -m venv phoenix_env
   $ source phoenix_env/bin/activate

2. Install Phoenix with evals support:
   $ pip install arize-phoenix[evals] openai pandas

3. Verify installation:
   $ python -c "import phoenix; print(phoenix.__version__)"

4. Start Phoenix server in one terminal:
   $ phoenix serve
   
   Expected output:
   ```
   ‚úÖ Arize Phoenix has been started.
   üìä View the dashboard at http://localhost:6006
   üîå OpenTelemetry listener at localhost:4317
   ```

DELIVERABLE: Phoenix UI accessible at http://localhost:6006 (shows empty Datasets, Evals tabs)

---

TASK 1.2: Instrument Your First Agent with Automatic Tracing

ACTION:
1. Create file: agents/test_trace.py

2. Paste this code:

```
#!/usr/bin/env python3
"""
Test instrumentation: Wrap an LLM call to see traces in Phoenix.
This mimics how your troubleshooting agent will report to Phoenix.
"""

import phoenix as px
from openai import OpenAI

# Step 1: Launch Phoenix session (connects to running phoenix serve)
session = px.launch_app()
print(f"‚úÖ Phoenix session active at {session.url}")

# Step 2: Wrap OpenAI client for automatic tracing
client = px.wrap(OpenAI(api_key="sk-..."))

# Step 3: Sample agent logic
def diagnose_plc_fault(fault_code: str, equipment_type: str) -> str:
    """
    Simulates your Siemens/Rockwell SME agent.
    Every LLM call inside here is automatically traced to Phoenix.
    """
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[
            {
                "role": "system",
                "content": "You are a certified PLC technician with 15+ years diagnosing Siemens and Rockwell systems."
            },
            {
                "role": "user",
                "content": f"Equipment: {equipment_type}\nFault: {fault_code}\nWhat is the root cause and repair procedure?"
            }
        ],
        temperature=0.2,  # Low temp for consistency
    )
    return response.choices.message.content

# Step 4: Run it
if __name__ == "__main__":
    print("\nüîç Running test diagnosis...")
    diagnosis = diagnose_plc_fault("F47", "Siemens S7-1200 with G120 VFD")
    print(f"\nüìã Agent Output:\n{diagnosis}\n")
    print("‚úÖ Trace sent to Phoenix. Check http://localhost:6006 ‚Üí Traces tab")
```

3. Run it:
   $ python agents/test_trace.py

4. Open http://localhost:6006 in browser
   ‚Üí Click "Traces" tab
   ‚Üí You should see 1 trace with your LLM call's input, output, tokens, latency, cost

DELIVERABLE: First trace visible in Phoenix UI; you can click it to see full LLM request/response

---

================================================================================
PHASE 2: BUILD YOUR GOLDEN DATASET (DAYS 3-4)
================================================================================

TASK 2.1: Extract 30-50 Real Fault Cases from Your Maintenance Logs

ACTION:
1. Review your maintenance logs / past Telegram interactions for:
   - 10 most common PLC fault codes
   - For each, 3-5 documented cases with root cause you've verified

2. Create file: datasets/golden_dataset.jsonl

3. Structure (one JSON object per line, no array wrapper):

```
{
  "test_case_id": "siemens_s7_1200_vfd_f47_001",
  "equipment": {
    "manufacturer": "Siemens",
    "model": "S7-1200",
    "subsystem": "Conveyor Motor Drive"
  },
  "input": {
    "fault_code": "F47",
    "fault_description": "Drive overcurrent shutdown",
    "sensor_data": {
      "motor_temp_celsius": 85,
      "vibration_peak_mm_s": 4.2,
      "current_draw_amps": 18.5
    },
    "maintenance_history": ["motor_bearing_replaced_2024_03_15"],
    "context": "Bearing was replaced 3 months ago; vibration spiking last week"
  },
  "expected_output": {
    "root_cause": "Motor bearing degradation from inadequate lubrication; friction increase causing overcurrent",
    "diagnosis_confidence": "High",
    "repair_steps": [
      "1. Lockout motor circuit per LOTO procedure (tag out, verify zero voltage)",
      "2. Measure bearing play with dial indicator; expect <0.3mm",
      "3. If play >0.5mm, replace motor bearings (requires bearing puller, ~30 min)",
      "4. Apply fresh bearing grease per OEM spec",
      "5. Re-run VFD diagnostics; verify F47 code cleared",
      "6. Run 30-min test cycle at 50% load before returning to production"
    ],
    "safety_critical_warnings": [
      "Verify lockout tag in place before touching motor",
      "Motor shaft may have residual current; use multimeter to confirm zero voltage",
      "Bearing puller required; do not attempt to drift bearing off shaft (risk of damage)",
      "Verify VFD parameter [Pn000] = 0 (safe torque-off) during work"
    ],
    "manual_citations": [
      "Siemens S7-1200 CPU Manual v4.3, Section 8.2: Fault Codes & Troubleshooting",
      "Siemens G120 VFD Troubleshooting Guide, p.142: Overcurrent Shutdowns",
      "Motor OEM Manual: Bearing Replacement Procedure, p.18"
    ],
    "estimated_downtime_hours": 4.5,
    "business_impact": {
      "downtime_cost_per_hour_usd": 12500,
      "safety_critical": true
    }
  }
}
{
  "test_case_id": "rockwell_compactlogix_e_error_002",
  ...
}
```

4. Repeat for 30-50 cases (mix of Siemens, Rockwell, edge cases)

CRITICAL: You must manually verify each "expected_output" is correct. This is your ground truth.

DELIVERABLE: datasets/golden_dataset.jsonl with 30-50 well-documented fault cases

---

TASK 2.2: Load Golden Dataset into Phoenix

ACTION:
1. Create file: evals/load_dataset.py

2. Paste:

```
#!/usr/bin/env python3
"""
Load golden fault cases into Phoenix dataset.
This creates a reusable test dataset for evals.
"""

import json
import phoenix as px

# Step 1: Load JSONL
cases = []
with open("datasets/golden_dataset.jsonl") as f:
    for line in f:
        cases.append(json.loads(line.strip()))

print(f"üìÇ Loaded {len(cases)} golden cases")

# Step 2: Launch Phoenix session
session = px.launch_app()

# Step 3: Create dataset from cases
dataset_records = [
    {
        "case_id": case["test_case_id"],
        "fault_code": case["input"]["fault_code"],
        "fault_desc": case["input"]["fault_description"],
        "equipment": case["equipment"]["model"],
        "expected_root_cause": case["expected_output"]["root_cause"],
        "expected_warnings": " | ".join(case["expected_output"]["safety_critical_warnings"]),
        "expected_steps": "\n".join(case["expected_output"]["repair_steps"]),
    }
    for case in cases
]

# Step 4: Save to Phoenix
ds = px.EvaluationDataset.from_list_of_dicts(dataset_records)
ds.to_csv("datasets/golden_dataset_for_phoenix.csv")
print(f"‚úÖ Dataset saved to CSV and ready for Phoenix")
print(f"   Rows: {len(dataset_records)}")
print(f"   Next: Run evals against this dataset")
```

3. Run it:
   $ python evals/load_dataset.py

4. Open http://localhost:6006 ‚Üí "Datasets" tab
   ‚Üí You should see your golden cases ready for evaluation

DELIVERABLE: Golden dataset visible in Phoenix UI; ready for eval template application

---

================================================================================
PHASE 3: BUILD LLM-AS-JUDGE EVALS (DAYS 5-6)
================================================================================

TASK 3.1: Define Judge Prompts

ACTION:
1. Create file: evals/judge_templates.py

2. Paste:

```
"""
Judge prompts for evaluating PLC agent outputs.
Each prompt represents a different evaluation dimension.
"""

TECHNICAL_ACCURACY_TEMPLATE = """
You are a master electrician with 25 years diagnosing industrial PLC systems.

CONTEXT:
Equipment: {equipment}
Fault Code: {fault_code}
Fault Description: {fault_desc}

AGENT'S DIAGNOSIS:
{agent_root_cause}

REFERENCE (CORRECT ANSWER):
{expected_root_cause}

TASK: Rate the agent's diagnosis on TECHNICAL ACCURACY.

Output ONE of these labels:
- CORRECT: Root cause matches reference AND reasoning is physically sound
- PARTIAL: Root cause is plausible but incomplete or reasoning has gaps
- INCORRECT: Root cause is wrong or physically implausible

Then provide a 1-2 sentence explanation.

Format your response as:
LABEL: [CORRECT|PARTIAL|INCORRECT]
REASON: [your explanation]
"""

SAFETY_COMPLIANCE_TEMPLATE = """
You are a certified safety engineer for industrial machinery (OSHA 1910.147 Lockout/Tagout).

CONTEXT:
Equipment: {equipment}
Fault Code: {fault_code}

AGENT'S SAFETY WARNINGS PROVIDED:
{agent_warnings}

REFERENCE WARNINGS (must include these):
{expected_warnings}

TASK: Did the agent provide all CRITICAL safety warnings? Any false alarms?

Output ONE of these labels:
- SAFE: All critical warnings present; no dangerous omissions; no false alarms
- PARTIAL: Some warnings present but missing 1+ critical ones
- UNSAFE: Critical warnings missing (risk of injury/property damage)

Format your response as:
LABEL: [SAFE|PARTIAL|UNSAFE]
REASON: [explanation]
"""

PROCEDURE_COMPLETENESS_TEMPLATE = """
You are a master electrician.

CONTEXT:
Equipment: {equipment}

AGENT'S REPAIR PROCEDURE:
{agent_steps}

REFERENCE PROCEDURE:
{expected_steps}

TASK: Is the agent's procedure complete, in correct order, and feasible?

Output ONE of these labels:
- COMPLETE: All major steps present, logical order, safe to execute
- PARTIAL: Missing steps OR minor ordering issues
- INCOMPLETE: Major gaps (would fail to resolve fault)

Format your response as:
LABEL: [COMPLETE|PARTIAL|INCOMPLETE]
REASON: [explanation]
"""

MANUAL_CITATION_ACCURACY_TEMPLATE = """
You are a technical librarian for industrial equipment documentation.

AGENT'S CITED MANUALS:
{agent_citations}

EXPECTED MANUALS:
{expected_citations}

TASK: Are the agent's manual citations accurate and relevant?

Output ONE label:
- ACCURATE: Citations match actual manuals and sections mentioned
- IMPRECISE: Citations exist but sections/page numbers incorrect
- HALLUCINATED: Citations do not exist (agent made them up)

Format:
LABEL: [ACCURATE|IMPRECISE|HALLUCINATED]
REASON: [explanation]
"""
```

DELIVERABLE: Judge prompts defined; ready to feed into eval framework

---

TASK 3.2: Run Evals Against Golden Dataset

ACTION:
1. Create file: evals/run_evals.py

2. Paste:

```
#!/usr/bin/env python3
"""
Run evals on your golden dataset.
Evaluates agent output against reference answers using LLM judges.
"""

import json
import pandas as pd
from datetime import datetime
import phoenix as px
from openai import OpenAI
from judge_templates import (
    TECHNICAL_ACCURACY_TEMPLATE,
    SAFETY_COMPLIANCE_TEMPLATE,
    PROCEDURE_COMPLETENESS_TEMPLATE,
    MANUAL_CITATION_ACCURACY_TEMPLATE,
)

# Load golden dataset
cases = []
with open("datasets/golden_dataset.jsonl") as f:
    for line in f:
        cases.append(json.loads(line.strip()))

# Initialize
session = px.launch_app()
client = OpenAI()
results = []

print("üîç Running evals on golden dataset...")
print(f"   Total cases: {len(cases)}")
print(f"   Start time: {datetime.now().isoformat()}\n")

# For each case, get agent output and run evals
for i, case in enumerate(cases[:10], 1):  # Start with first 10
    case_id = case["test_case_id"]
    print(f"[{i}/10] {case_id}")
    
    # STEP 1: Get agent output (replace with YOUR agent call)
    agent_output = get_agent_diagnosis(
        fault_code=case["input"]["fault_code"],
        equipment_type=case["equipment"]["model"],
        sensor_data=case["input"]["sensor_data"]
    )
    
    # STEP 2: Technical Accuracy Eval
    accuracy_response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{
            "role": "user",
            "content": TECHNICAL_ACCURACY_TEMPLATE.format(
                equipment=case["equipment"]["model"],
                fault_code=case["input"]["fault_code"],
                fault_desc=case["input"]["fault_description"],
                agent_root_cause=agent_output.get("root_cause", ""),
                expected_root_cause=case["expected_output"]["root_cause"]
            )
        }],
        temperature=0.0
    )
    accuracy_result = accuracy_response.choices.message.content
    
    # STEP 3: Safety Compliance Eval
    safety_response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{
            "role": "user",
            "content": SAFETY_COMPLIANCE_TEMPLATE.format(
                equipment=case["equipment"]["model"],
                fault_code=case["input"]["fault_code"],
                agent_warnings=" | ".join(agent_output.get("safety_warnings", [])),
                expected_warnings=" | ".join(case["expected_output"]["safety_critical_warnings"])
            )
        }],
        temperature=0.0
    )
    safety_result = safety_response.choices.message.content
    
    # STEP 4: Procedure Completeness Eval
    procedure_response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=[{
            "role": "user",
            "content": PROCEDURE_COMPLETENESS_TEMPLATE.format(
                equipment=case["equipment"]["model"],
                agent_steps="\n".join(agent_output.get("repair_steps", [])),
                expected_steps="\n".join(case["expected_output"]["repair_steps"])
            )
        }],
        temperature=0.0
    )
    procedure_result = procedure_response.choices.message.content
    
    # STEP 5: Store results
    results.append({
        "case_id": case_id,
        "fault_code": case["input"]["fault_code"],
        "accuracy": accuracy_result,
        "safety": safety_result,
        "procedure": procedure_result,
        "timestamp": datetime.now().isoformat()
    })
    
    # Parse and display
    accuracy_label = accuracy_result.split("LABEL:").split("\n").strip()[1]
    safety_label = safety_result.split("LABEL:").split("\n").strip()[1]
    procedure_label = procedure_result.split("LABEL:").split("\n").strip()[1]
    
    print(f"  ‚úì Accuracy: {accuracy_label} | Safety: {safety_label} | Procedure: {procedure_label}")

# STEP 6: Save results
with open("evals/eval_results.json", "w") as f:
    json.dump(results, f, indent=2)

# STEP 7: Summary
correct = sum(1 for r in results if "CORRECT" in r["accuracy"])
safe = sum(1 for r in results if "SAFE" in r["safety"])
complete = sum(1 for r in results if "COMPLETE" in r["procedure"])

print(f"\nüìä SUMMARY")
print(f"   Accuracy: {correct}/{len(results)} CORRECT ({100*correct/len(results):.0f}%)")
print(f"   Safety: {safe}/{len(results)} SAFE ({100*safe/len(results):.0f}%)")
print(f"   Procedure: {complete}/{len(results)} COMPLETE ({100*complete/len(results):.0f}%)")
print(f"\n‚úÖ Results saved to evals/eval_results.json")

def get_agent_diagnosis(fault_code: str, equipment_type: str, sensor_data: dict) -> dict:
    """
    TODO: Replace this with YOUR actual agent call.
    For now, returns a mock response.
    """
    return {
        "root_cause": "Motor bearing degradation",
        "safety_warnings": ["Lockout motor before inspection"],
        "repair_steps": ["1. Lockout", "2. Inspect bearing", "3. Replace if needed"]
    }
```

3. Run it:
   $ python evals/run_evals.py

4. Check results:
   $ cat evals/eval_results.json

DELIVERABLE: Eval results JSON showing accuracy/safety/procedure scores for all cases

---

================================================================================
PHASE 4: CI/CD INTEGRATION (DAYS 7-8)
================================================================================

TASK 4.1: Create GitHub Actions Eval Gate

ACTION:
1. Create file: .github/workflows/eval_gate.yml

2. Paste:

```
name: Agent Evaluation Gate

on:
  pull_request:
    paths:
      - 'rivet/**'
      - 'agents/**'
      - 'evals/**'
      - 'requirements.txt'

jobs:
  eval-gate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install arize-phoenix[evals]
      
      - name: Run golden dataset evals
        run: |
          python evals/run_evals.py \
            --dataset datasets/golden_dataset.jsonl \
            --output evals/eval_results.json
      
      - name: Check eval thresholds
        run: |
          python evals/check_results.py evals/eval_results.json
          # Fails if accuracy < 85% OR safety violations detected
      
      - name: Report results
        if: always()
        run: |
          python evals/report_results.py evals/eval_results.json
```

3. Push to GitHub:
   $ git add .github/workflows/eval_gate.yml
   $ git commit -m "feat: Add eval gate to CI"
   $ git push

DELIVERABLE: CI/CD workflow that blocks PRs if agent accuracy drops

---

TASK 4.2: Create Result Checker

ACTION:
1. Create file: evals/check_results.py

2. Paste:

```
#!/usr/bin/env python3
"""
Check eval results against thresholds.
Fails if accuracy < 85% or safety < 100%.
"""

import json
import sys

def main():
    with open("evals/eval_results.json") as f:
        results = json.load(f)
    
    # Count CORRECT, SAFE, COMPLETE
    correct_count = sum(1 for r in results if "CORRECT" in r["accuracy"])
    safe_count = sum(1 for r in results if "SAFE" in r["safety"])
    complete_count = sum(1 for r in results if "COMPLETE" in r["procedure"])
    
    total = len(results)
    accuracy_pct = 100 * correct_count / total if total > 0 else 0
    safety_pct = 100 * safe_count / total if total > 0 else 0
    complete_pct = 100 * complete_count / total if total > 0 else 0
    
    # Print summary
    print("\n" + "="*60)
    print("EVAL RESULTS SUMMARY")
    print("="*60)
    print(f"Accuracy:  {correct_count}/{total} CORRECT ({accuracy_pct:.1f}%) [threshold: 85%]")
    print(f"Safety:    {safe_count}/{total} SAFE ({safety_pct:.1f}%) [threshold: 100%]")
    print(f"Complete:  {complete_count}/{total} COMPLETE ({complete_pct:.1f}%) [threshold: 90%]")
    print("="*60 + "\n")
    
    # Check thresholds
    failed = False
    
    if accuracy_pct < 85:
        print(f"‚ùå FAIL: Accuracy {accuracy_pct:.1f}% < 85% threshold")
        failed = True
    else:
        print(f"‚úÖ PASS: Accuracy {accuracy_pct:.1f}% >= 85%")
    
    if safety_pct < 100:
        print(f"‚ùå FAIL: Safety {safety_pct:.1f}% < 100% (zero tolerance)")
        failed = True
    else:
        print(f"‚úÖ PASS: Safety at 100%")
    
    if complete_pct < 90:
        print(f"‚ö†Ô∏è  WARN: Completeness {complete_pct:.1f}% < 90%")
    else:
        print(f"‚úÖ PASS: Completeness {complete_pct:.1f}% >= 90%")
    
    print()
    
    if failed:
        print("üö´ GATE FAILED: Fix agent regressions before merging")
        sys.exit(1)
    else:
        print("üéâ GATE PASSED: All thresholds met. Safe to merge.")
        sys.exit(0)

if __name__ == "__main__":
    main()
```

3. Test locally:
   $ python evals/check_results.py

DELIVERABLE: Result checker that enforces eval thresholds in CI

---

================================================================================
PHASE 5: PRODUCTION SAMPLING (WEEK 2+)
================================================================================

TASK 5.1: Weekly Production Review Ritual

ACTION:
1. Every Monday, spend 30 minutes:
   - Check Telegram bot interactions from past week
   - Sample 5-10 interesting cases (new fault codes, edge cases, agent failures)
   - Copy them into datasets/golden_dataset.jsonl as new test cases
   - Re-run evals to ensure coverage

2. Create file: scripts/sample_production.py for automation:

```
#!/usr/bin/env python3
"""
Sample recent Telegram interactions for eval dataset expansion.
Run weekly to find new edge cases.
"""

import json
import datetime
from telegram_api import get_recent_interactions

# Get last 7 days
interactions = get_recent_interactions(hours=24*7)

# Filter for interesting ones (new fault codes, agent failures, etc.)
sampled = []
for interaction in interactions:
    if interaction.get("agent_failed") or interaction.get("is_new_fault_code"):
        sampled.append({
            "source": "telegram_production",
            "date": datetime.datetime.now().isoformat(),
            "chat_id": interaction["chat_id"],
            "user_query": interaction["message"],
            "agent_response": interaction["bot_response"],
            "needs_manual_review": True,
            "reviewer_notes": ""
        })

# Save for your weekly review
with open("datasets/production_samples.jsonl", "a") as f:
    for sample in sampled:
        f.write(json.dumps(sample) + "\n")

print(f"üìä Found {len(sampled)} interesting interactions for review")
print("üìù Review datasets/production_samples.jsonl to add to golden_dataset.jsonl")
```

DELIVERABLE: Automated weekly sampling to expand golden dataset

---

================================================================================
SUMMARY CHECKLIST
================================================================================

PHASE 1: Local Development
‚òê Install Phoenix: pip install arize-phoenix[evals]
‚òê Start server: phoenix serve ‚Üí http://localhost:6006 accessible
‚òê Create test_trace.py and run it
‚òê Verify first trace appears in Phoenix UI

PHASE 2: Golden Dataset
‚òê Extract 30-50 real fault cases from maintenance logs
‚òê Create datasets/golden_dataset.jsonl with proper structure
‚òê Load into Phoenix with load_dataset.py
‚òê Verify dataset visible in Phoenix "Datasets" tab

PHASE 3: LLM-as-Judge Evals
‚òê Create judge_templates.py with 4 eval prompts
‚òê Create run_evals.py that runs evals on golden dataset
‚òê Run and verify eval_results.json outputs CORRECT/SAFE/COMPLETE labels
‚òê Check results summary shows accuracy/safety/procedure percentages

PHASE 4: CI/CD Integration
‚òê Create .github/workflows/eval_gate.yml
‚òê Create evals/check_results.py that enforces thresholds
‚òê Push to GitHub and verify CI runs on PR
‚òê Verify CI blocks merge if accuracy < 85% or safety < 100%

PHASE 5: Production Sampling
‚òê Create scripts/sample_production.py for weekly sampling
‚òê Schedule weekly review ritual
‚òê Add new edge cases to golden_dataset.jsonl
‚òê Re-run evals when dataset grows

---

KEY METRICS TO TRACK
---

Weekly:
- Accuracy: % of cases where agent diagnoses root cause correctly (target: 85%+)
- Safety: % of cases where all critical warnings present (target: 100%)
- Completeness: % of cases with complete repair procedures (target: 90%+)

Monthly:
- Golden dataset size: Should grow from 30 ‚Üí 50 ‚Üí 100 as you sample production
- CI/CD blocks: How many PRs blocked by eval gate? (should be 0-2 per month if eval coverage is good)
- Technician override rate: % of agent suggestions ignored in field (collect via Telegram feedback)

---

REFERENCES & RESOURCES
---

- Arize Phoenix Docs: https://arize.com/docs/phoenix
- Aman Khan's Evaluation Framework Talk: https://www.youtube.com/watch?v=2HNSG990Ew8
- Arize vs LangSmith Comparison: https://arize.com/docs/phoenix/resources/frequently-asked-questions/open-source-langsmith-alternative-arize-phoenix-vs.-langsmith
- OpenTelemetry Docs: https://opentelemetry.io/docs/
- Your Full Setup Guide: See phoenix_setup_guide.md

---

NEXT STEPS (THIS WEEK)
---

TODAY:
1. Install Phoenix: pip install arize-phoenix[evals]
2. Start server: phoenix serve
3. Run test_trace.py
4. Confirm trace visible at http://localhost:6006

THIS WEEK:
1. Extract 10-20 golden cases from your logs
2. Create datasets/golden_dataset.jsonl
3. Load into Phoenix
4. Create and run first evals
5. Check eval results; aim for 70%+ accuracy as baseline

NEXT WEEK:
1. Expand to 50 golden cases
2. Refine judge prompts based on mis-evals
3. Set up CI/CD gate
4. Block first test PR to verify gate works

BY MONTH-END:
1. 100+ golden cases in dataset
2. CI gate blocking regressions
3. Weekly production sampling in place
4. Confidence to deploy agents knowing eval framework has your back

================================================================================
```

***

Both files are now displayed above. **Copy and paste whichever sections you need.** 

Start with **Phase 1** from the Claude Code prompt if you want immediate action steps.

[1](https://www.youtube.com/watch?v=2HNSG990Ew8)